{"pages":[{"url":"http://x-wei.github.io/andev_p2e2_pref.html","text":"Save settings and configurations. Data Persistance 5 different ways of data persistance: onSavedInstanceState() : store state of views in k-v pairs (Bundles), used when screen rotates / app killed by system, temperary. SharedPreferences : save k-v pairs to a file, can save primitive types . SQLite database: complicated data types Internal / External Storage: save large files to local phone, ex. podcast app / camera app. Server: ex. leaderboard for a game (Firebase) PreferenceFragments PreferenceFragment is a class to handle the storing of user preferences. Fragment Fragment is a class that represents a modular and reusable piece of an Activity . ex. a booklist-bookdetail app, for tablets want both screens (list/detail) on the same screen ⇒ put the 2 screens into Fragment s, and re-use them. PreferenceFragment populated with preferences defined in an XML, and (automatically) update k-v pairs in a sharedpref file. in the pref XML file: define what are in the preferences, keys(names) and default values, and summary of a pref item. Adding a settings screen from a pref-xml file Recipe 1. create a settings activity add a menu xml, in the xml, add a menu item show menu in mian activity when \"settings\" item is clicked: open settings activity 2. create a preferences xml setup gradle dependency (build.gradle): compile 'com.android.support:preference-v7:25.1.0 ' create a xml file in res/xml folder, add CheckBoxPreference. <?xml version=\"1.0\" encoding=\"utf-8\"?> 3. create a class that extends PreferenceFragment // TODO (2) Create a class called SettingsFragment that extends PreferenceFragmentCompat public class SettingsFragment extends PreferenceFragmentCompat{ // TODO (5) In SettingsFragment's onCreatePreferences method add the preference file using the // addPreferencesFromResource method @Override public void onCreatePreferences(Bundle savedInstanceState, String rootKey) { addPreferencesFromResource(R.xml.pref_visualizer); } } 4. change root element of settings activity to a fragment <?xml version=\"1.0\" encoding=\"utf-8\"?> <fragment xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:tools= \"http://schemas.android.com/tools\" android:id= \"@+id/activity_settings\" android:name= \"android.example.com.visualizerpreferences.SettingsFragment\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" > </fragment> 5. add a preferenceTheme in res/values/styles.xml : <item name=\"preferenceTheme\">@style/PreferenceThemeOverlay</item> 6. read from SharedPreferences and update in onStart or onResume of main activity: // TODO (2) Get a reference to the default shared preferences from the PreferenceManager class SharedPreferences pref = PreferenceManager.getDefaultSharedPreferences(this); // TODO (3) Get the value of the show_bass checkbox preference and use it to call setShowBass mVisualizerView.setShowBass(pref.getBoolean(\"show_bass\", true)); best practice: using resources instead of using constant iterals everywhere. i.e. extract strings/bools into strings.xml or bools.xml: <string name= \"pref_show_bass_title\" > Show Bass </string> <string name= \"pref_show_bass_key\" translatable= \"false\" > show_bass </string> <string name= \"pref_show_bass_summaryon\" > Shown </string> <string name= \"pref_show_bass_summaryoff\" > Hidden </string> <bool name= \"pref_show_bass_default\" > true </bool> in other xml files: <CheckBoxPreference android:defaultValue=\"@bool/pref_show_bass_default\" android:key=\"@string/pref_show_bass_key\" android:summaryOff=\"@string/pref_show_bass_summaryoff\" android:summaryOn=\"@string/pref_show_bass_summaryon\" android:title=\"@string/perf_show_bass_title\" /> in java files: mVisualizerView.setShowBass(perf.getBoolean(getString(R.string.pref_show_bass_key), getResources().getBoolean(R.bool.pref_show_bass_default))); PreferenceChangeListener instead of updating preferences in onStart/onResume of main activity, use onPreferenceChangeListener , this is called whenever a sharedpref is changed. Reciept step 1 : determine the activity who's UI needs to be changed when pref changes — in our case, is the main activity step 2 : let this activity implement this Listener ( onSharedPreferenceChange ) @Override public void onSharedPreferenceChanged(SharedPreferences sharedPreferences, String key) { if(key.equals(getString(R.string.pref_show_bass_key))){ mVisualizerView.setShowBass(sharedPreferences.getBoolean(key, true)); } } step 3 : link this Listener to the SharedPreference object that it listens to, using registerOnSharedPreferenceChangeListener in onCreate(): SharedPreferences sharedPreferences = PreferenceManager.getDefaultSharedPreferences(this); sharedPreferences.registerOnSharedPreferenceChangeListener(this); step 4 : unregister the Listener when activity is shut down in onDestroy(): @Override protected void onDestroy() { super.onDestroy(); PreferenceManager.getDefaultSharedPreferences(this) .unregisterOnSharedPreferenceChangeListener(this); } List Preference Compared to CheckBoxPreference , ListPreference offers an array of possible choices. ListPreference Recipe define options labels/values in res/values/strings.xml : <string name= \"pref_color_key\" > pref_color </string> <string name= \"pref_color_title\" > Pick a color </string> <string name= \"pref_color_red_label\" > Red </string> <string name= \"pref_color_blue_label\" > Blue </string> <string name= \"pref_color_green_label\" > Green </string> <string name= \"pref_color_red_value\" translatable= \"false\" > color_red </string> <string name= \"pref_color_blue_value\" translatable= \"false\" > color_blue </string> <string name= \"pref_color_green_value\" translatable= \"false\" > color_green </string> (note: xx_value is for internal usage, xx_lablel is what is shown on screen to users) create res/values/arrays.xml : <?xml version=\"1.0\" encoding=\"utf-8\"?> <!-- TODO (3) Add a res->values->arrays.xml file which contains two arrays, one for labels and one for values. The arrays should contain strings found in this file--> <resources> <array name= \"pref_color_option_labels\" > <item> @string/pref_color_red_label </item> <item> @string/pref_color_blue_label </item> <item> @string/pref_color_green_label </item> </array> <array name= \"pref_color_option_values\" > <item> @string/pref_color_red_value </item> <item> @string/pref_color_blue_value </item> <item> @string/pref_color_green_value </item> </array> </resources> in preference xml: add a ListPreference element <ListPreference android:defaultValue=\"@string/pref_color_red_value\" android:entries=\"@array/pref_color_option_labels\" android:entryValues=\"@array/pref_color_option_values\" android:key=\"@string/pref_color_key\" android:title=\"@string/pref_color_title\"/> finally in main activity: String default_color_value = getString(R.string.pref_color_red_value); mVisualizerView.setColor(sharedPreferences.getString(getString(R.string.pref_color_key), default_color_value) ); add summary for list preference we have to this programatically in the java code. Here is the recipe: step 1 : let the SettingsFragment class implement OnSharedPreferenceChangeListener: // TODO (1) Implement OnSharedPreferenceChangeListener public class SettingsFragment extends PreferenceFragmentCompat implements OnSharedPreferenceChangeListener step 2 :set preference's summary by setSummary() : // TODO (2) Create a setPreferenceSummary which takes a Preference and String value as parameters. // This method should check if the preference is a ListPreference and, if so, find the label // associated with the value. You can do this by using the findIndexOfValue and getEntries methods // of Preference. private void setPreferenceSummary(Preference sharedPref, String value) { if (sharedPref instanceof ListPreference) { ListPreference listPref = (ListPreference) sharedPref; int idx = listPref.findIndexOfValue(value); listPref.setSummary(listPref.getEntries()[idx]); } } step 3 : set pref summary in onCreatePreferences: @Override public void onCreatePreferences(Bundle bundle, String s) { // Add visualizer preferences, defined in the XML file in res->xml->pref_visualizer addPreferencesFromResource(R.xml.pref_visualizer); // TODO (3) Get the preference screen, get the number of preferences and iterate through // all of the preferences if it is not a checkbox preference, call the setSummary method // passing in a preference and the value of the preference PreferenceScreen prefScreen = getPreferenceScreen(); int nPrefs = prefScreen.getPreferenceCount(); for(int i=0; i<nPrefs; i++){ Preference p = prefScreen.getPreference(i); if(p instanceof CheckBoxPreference) continue; else { String value = prefScreen.getSharedPreferences().getString(p.getKey(), \"\"); setPreferenceSummary(p, value); } } } step 4 : implement onSharedPreferenceChanged listener: // TODO (4) Override onSharedPreferenceChanged and, if it is not a checkbox preference, // call setPreferenceSummary on the changed preference @Override public void onSharedPreferenceChanged(SharedPreferences sharedPreferences, String key) { if (sharedPreferences instanceof CheckBoxPreference) return; else{ Preference pref = findPreference(key); String value = sharedPreferences.getString(key, \"\"); setPreferenceSummary(pref, value); } } step 5 : finally register/unregister the listener in oncreate/ondestroy: // TODO (5) Register and unregister the OnSharedPreferenceChange listener (this class) in // onCreate and onDestroy respectively. @Override public void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); getPreferenceScreen().getSharedPreferences() .registerOnSharedPreferenceChangeListener(this); } @Override public void onDestroy() { super.onDestroy(); getPreferenceScreen().getSharedPreferences() .unregisterOnSharedPreferenceChangeListener(this); }","tags":"notes","title":"[Android Dev] 2.2 Preference"},{"url":"http://x-wei.github.io/andev_p2e1_lifecycle.html","text":"Android kills background apps !! → onCreate() → Created → onStart() → Visible( can be seen on screen) → onResume() → Active (get focus, can interact with) Active → onPause() → Paused (lose focus — same thing as Visible?) → onStop() → Stopped (disappeared) → onDestroy() → Destroyed (lifecycle ends) when rotate screen, the function calling is: onPause --> onStop --> onDestroy --> onCreate --> onStart --> onResume note: activity will be destroyed when device configuration is changed ! onSavedInstanceState() so that the app states can be saved when device configuration changes (ex. rotate). parameter: a Bundle to keep state into key-value pairs. to pass objects via Bundle: implement the Parcelble interface. Override this onSavedInstanceState() function → put useful objects into the bundle parameter. // TODO (3) Override onSaveInstanceState to persist data across Activity recreation // Do the following steps within onSaveInstanceState // TODO (4) Make sure super.onSaveInstanceState is called before doing anything else // TODO (5) Put the contents of the TextView that contains our URL into a variable // TODO (6) Using the key for the query URL, put the string in the outState Bundle // TODO (7) Put the contents of the TextView that contains our raw JSON search results into a variable // TODO (8) Using the key for the raw JSON search results, put the search results into the outState Bundle @Override public void onSaveInstanceState(Bundle outState) { super.onSaveInstanceState(outState); outState.putString(QUERYURL_KEY, mUrlDisplayTextView.getText().toString()); outState.putString(RAW_JSON_KEY, mSearchResultsTextView.getText().toString()); } in onCreate : try to extract objects form bundle savedInstanceState (if it's not null). @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mSearchBoxEditText = (EditText) findViewById(R.id.et_search_box); mUrlDisplayTextView = (TextView) findViewById(R.id.tv_url_display); mSearchResultsTextView = (TextView) findViewById(R.id.tv_github_search_results_json); mErrorMessageDisplay = (TextView) findViewById(R.id.tv_error_message_display); mLoadingIndicator = (ProgressBar) findViewById(R.id.pb_loading_indicator); // TODO (9) If the savedInstanceState bundle is not null, set the text of the URL and search results TextView respectively if(savedInstanceState!=null && savedInstanceState.containsKey(QUERYURL_KEY)) mUrlDisplayTextView.setText(savedInstanceState.getString(QUERYURL_KEY)); if(savedInstanceState!=null && savedInstanceState.containsKey(RAW_JSON_KEY)) mSearchResultsTextView.setText(savedInstanceState.getString(RAW_JSON_KEY)); } AsyncTasks and Loader edgecase: rotate when async task is performing → asynctask runs on separate thread → the zombie activities will not be killed !! ⇒ Loader Loader is identified by a loader ID, thus preventing several loaders running in the same time. AsyncTaskLoader lifecycle: it is bounded to an Activity instead of an async task ⇒ when device rotates, the loader will feed the results of tasks to the right activity. Using Loaders create loader id: private static final GITHUB_SEARCH_LOADER = 22; implement LoaderCallbacks interface functions in MainActivity init loader with a LoaderManager , and start the loader let MainActivty implement LoaderManager.LoaderCallbacks<String> implement onCreateLoader — onPreExecute and onDoInBackground for AsyncTask // TODO (3) Override onCreateLoader @Override public Loader<String> onCreateLoader(int id, final Bundle bundle) {// !make bundle final so that can access it below // TODO (4) Return a new AsyncTaskLoader<String> as an anonymous inner class with this as the constructor's parameter return new AsyncTaskLoader<String>(this) { @Override public String loadInBackground() { return null; } // TODO (5) Override onStartLoading @Override protected void onStartLoading() { super.onStartLoading(); // TODO (6) If bundle is null, return. if (bundle==null) return; // TODO (7) move the content of onPrecuter in the AsyncTask class mLoadingIndicator.setVisibility(View.VISIBLE); // TODO (8) Force a load forceLoad(); } // TODO (9) Override loadInBackground @Override protected String onLoadInBackground() { // TODO (10) Get the String for our URL from the bundle passed to onCreateLoader String searchUrlstr= bundle.getString(SEARCH_QUERY_URL_EXTRA); // TODO (11) If the URL is null or empty, return null if(searchUrlstr==null || TextUtils.isEmpty(searchUrlstr)) return null; // TODO (12) Copy the try / catch block from the AsyncTask's doInBackground method String githubSearchResults = null; try { githubSearchResults = NetworkUtils.getResponseFromHttpUrl(new URL(searchUrlstr)); } catch (IOException e) { e.printStackTrace(); } return githubSearchResults; } }; } implement onLoadFinished — onPostExecute for AsyncTask // TODO (13) Override onLoadFinished @Override public void onLoadFinished(Loader<String> loader, String githubSearchResults) { // TODO (15) Use the same logic used in onPostExecute to show the data or the error message mLoadingIndicator.setVisibility(View.INVISIBLE); if (githubSearchResults != null && !githubSearchResults.equals(\"\")) { showJsonDataView(); mSearchResultsTextView.setText(githubSearchResults); } else { showErrorMessage(); } } // TODO (16) Override onLoaderReset as it is part of the interface we implement, but don't do anything in this method @Override public void onLoaderReset(Loader<String> loader) { } in makeGithubSearchQuery(): create bundle, create a loder, start it instead of staring an AsyncTask , we get a Loader and re-run it via a loader manager. // TODO (19) Create a bundle called queryBundle Bundle queryBundle = new Bundle(); // TODO (20) Use putString with SEARCH_QUERY_URL_EXTRA as the key and the String value of the URL as the value queryBundle.putString(SEARCH_QUERY_URL_EXTRA, githubSearchUrl.toString()); // TODO (21) Call getSupportLoaderManager and store it in a LoaderManager variable LoaderManager loaderManager = getSupportLoaderManager(); // TODO (22) Get our Loader by calling getLoader and passing the ID we specified Loader loader = loaderManager.getLoader(GITHUB_SEARCH_LOADER); // TODO (23) If the Loader was null, initialize it. Else, restart it. if(loader==null) loader = loaderManager.initLoader(GITHUB_SEARCH_LOADER, queryBundle, this); else loaderManager.restartLoader(GITHUB_SEARCH_LOADER, queryBundle, this); Caching with loaders Loaders will reload if activity is destroyed and re-created. This overhead can be avoided by caching the results in the AsyncTaskLoader (created in onCreateLoader() ). // TODO (1) Create a String member variable called mGithubJson that will store the raw JSON String mGithubJson; // cached results @Override protected void onStartLoading() { /* If no arguments were passed, we don't have a query to perform. Simply return. */ if (args == null) { return; } /* * When we initially begin loading in the background, we want to display the * loading indicator to the user */ mLoadingIndicator.setVisibility(View.VISIBLE); // TODO (2) If mGithubJson is not null, deliver that result. Otherwise, force a load if(mGithubJson!=null) deliverResult(mGithubJson); else forceLoad(); } // TODO (3) Override deliverResult and store the data in mGithubJson @Override public void deliverResult(String data) { mGithubJson = data; // TODO (4) Call super.deliverResult after storing the data super.deliverResult(data); }","tags":"notes","title":"[Android Dev] 2.1 Lifecycles"},{"url":"http://x-wei.github.io/andev_p1e2_internet.html","text":"logging https://developer.android.com/reference/android/util/Log.html 5 log levels: error/warning/info/debug/verbose error/warning/info are preserved in the release of app use class name as tag: MyClass.class.getSimpleName() the Resources folder https://developer.android.com/guide/topics/resources/providing-resources.html working with res/values/strings.xml the strings defined in this xml file can be got from java activity using getString(R.string._the_id) method. example: in string.xml: <string name=\"today\">Today</string> in java: String myString = getString(R.string.today) in other xmls files: use @string/today to access it. ref: https://developer.android.com/guide/topics/resources/string-resource.html Menu and ActionBar a menu item in the xml file (res folder→ create new android resource directory → menu): // TODO (2) Create a menu in res/menu called main.xml // TODO (3) Add one menu item to your menu // TODO (4) Give the menu item an id of @+id/action_search // TODO (5) Set the orderInCategory to 1 // TODO (6) Show this item if there is room (use app:showAsAction, NOT android:showAsAction) // TODO (7) Set the title to the search string (\"Search\") from strings.xml <menu xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:app= \"http://schemas.android.com/apk/res-auto\" > <item android:id= \"@+id/action_search\" android:orderInCategory= \"1\" android:title= \"@string/search\" app:showAsAction= \"ifRoom\" /> </menu> shoule also set MainActivity as singleTop mode: <activity android:name=\".MainActivity\" android:launchMode=\"singleTop\"> To create menu: override onCreateMenu : // TODO ( 8 ) Override onCreateOptionsMenu // TODO ( 9 ) Within onCreateOptionsMenu , use getMenuInflater (). inflate to inflate the menu // TODO ( 10 ) Return true to display your menu @Override public boolean onCreateOptionsMenu ( Menu menu ) { getMenuInflater (). inflate ( R . menu . main , menu ); return true ; } To handle menu item clicks: onOptionsItemSelected // TODO (11) Override onOptionsItemSelected // TODO (12) Within onOptionsItemSelected, get the ID of the item that was selected // TODO (13) If the item's ID is R.id.action_search, show a Toast and return true to tell droid that you've handled this menu click // TODO (14) Don't forgot to call .show() on your Toast // TODO (15) If you do NOT handle the menu click, return super.onOptionsItemSelected to let droid handle the menu click @Override public boolean onOptionsItemSelected(MenuItem item) { int itemid = item.getItemId(); if(itemid==R.id.action_search) { // use MainActivity.this as context, instead of using just this Toast.makeText(MainActivity.this, \"search action selected!\", Toast.LENGTH_SHORT).show(); return true; } return super.onOptionsItemSelected(item); } Another way to set menuitem action: menuitem.setIntent @Override public boolean onCreateOptionsMenu(Menu menu) { getMenuInflater().inflate(R.menu.detail, menu); MenuItem menuitem = menu.findItem(R.id.action_share); Intent intent = ShareCompat.IntentBuilder.from(this) .setType(\"text/plain\") .setChooserTitle(\"choose which app to share\") .setText(mForecast) .getIntent(); menuitem.setIntent(intent); return true; } Build the query URL the request url for github repo search: https://api.github.com/search/repositories?q=android&sort=stars use Uri Builder: convert Uri to java URL: public static URL buildUrl(String githubSearchQuery) { Uri.Builder uribuilder = Uri.parse(GITHUB_BASE_URL).buildUpon(); uribuilder = uribuilder.appendQueryParameter(PARAM_QUERY, githubSearchQuery); uribuilder = uribuilder.appendQueryParameter(PARAM_SORT, sortBy); Uri uri = uribuilder.build(); URL url = null; try{ url = new URL(uri.toString()); } catch (MalformedURLException e) { e.printStackTrace(); } return url; } Fetching http request http://stackoverflow.com/questions/309424/read-convert-an-inputstream-to-a-string public static String getResponseFromHttpUrl(URL url) throws IOException { HttpURLConnection urlConnection = (HttpURLConnection) url.openConnection(); try { InputStream in = urlConnection.getInputStream(); Scanner scanner = new Scanner(in); scanner.useDelimiter(\"\\\\A\"); boolean hasInput = scanner.hasNext(); if (hasInput) { return scanner.next(); } else { return null; } } finally { urlConnection.disconnect(); } } or use external library: http://square.github.io/okhttp/ Permission declear permissions required in android manifest.xml https://developer.android.com/guide/topics/permissions/index.html in AndroidManifest.xml, under the manifest root: <uses-permission android:name=\"android.permission.INTERNET\"/> NetworkOnMainThread exception android has a single user interface thread responsible for user interaction: 60FPS — want to do as little as possible on this main thread . ⇒ run the network on a secondary execusion thread, when finished, modify the user interface on UI thread. ⇒ AsyncTask AsyncTask allows running a task on background thread while publishing results to the UI thread. The UI thread has a message queue and a handler : to process messages from other threads. — AsyncTask wraps this into a simpler interface. AsyncTask is generic, the type parameters include: AsyncTask<Params, Progress, Result> These 3 type parameters correspond to 4 function to override: doInBackgroud, onProgressUpdate, onPostExecute, onPreEcecute . Once this (sub)class is defined, on UI thread, just create an AysncTask task , and use task.execute(params..) to make it run on background thread. ref: https://developer.android.com/reference/android/os/AsyncTask.html AsyncTask Example code First create an inner class that extends AyncTask<> class, and add the job to do here: // TODO (1) Create a class called GithubQueryTask that extends AsyncTask<URL, Void, String> class GithubQueryTask extends AsyncTask<URL, Void, String> { @Override // TODO (2) Override the doInBackground method to perform the query. Return the results. protected String doInBackground(URL... params) { URL url = params[0]; try { String res = NetworkUtils.getResponseFromHttpUrl(url); return res; } catch (IOException e) { e.printStackTrace(); } return null; } @Override // TODO (3) Override onPostExecute to display the results in the TextView protected void onPostExecute(String s) { if (s != null && !s.equals(\"\")) mSearchResultsTextView.setText(s); else super.onPostExecute(s); } } Then in UI thread, create a such class object, call execute method on it: private void makeGithubSearchQuery() { String githubQuery = mSearchBoxEditText.getText().toString(); URL githubSearchUrl = NetworkUtils.buildUrl(githubQuery); mUrlDisplayTextView.setText(githubSearchUrl.toString()); // TODO (4) Create a new GithubQueryTask and call its execute method, passing in the url to query GithubQueryTask task = new GithubQueryTask(); task.execute(githubSearchUrl); } To show a progress bar when running in background ? ⇒ put textview and progressbar into the same FrameLayout , when running, make the progressbar visible, when finished, make it invisible. Parsing JSON json file content: { \"temp\": { \"min\":\"11.34\", \"max\":\"19.01\" } \"weather\": { \"id\":\"801\", \"condition\":\"Clouds\", \"description\":\"few clouds\" } \"pressure\":\"1023.51\", \"humidity\":\"87\" } the funtion to get the condition from the above json: String getCondition(String JSONString) { JSONObject forecast = new JSONObject(JSONString); JSONObject weather = forecast.getJSONObject(\"weather\"); return weather.getString(\"condition\"); }","tags":"notes","title":"[Android Dev] 1.2 Connect to the Internet"},{"url":"http://x-wei.github.io/andev_p1e3_recyclerview.html","text":"Recycler View, Adaper, ViewHolder rather than creating list items as we scroll, keep them in a queue (recycling bin) → when scrolling, the list items are recycled and re-bind to new content. RecyclerView is better version of ListView. Adaper : bind data from data source, and provide the RecyclerView with new views when needed. Adapter uses a ViewHolder to send views to RecyclerView. — each findViewById wil be only called once, and cached in the ViewHolder. And the RecyclerView uses a LayoutManager to manage the appearance of the items. RecyclerView code example: a list of numbers 1. add gradle dependency in build.gradle file, add in dependencies: compile 'com.android.support:recyclerview-v7:25.1.0 ' and re-sync project, now we can use android.support.v7.widget.RecyclerView in the layout files. 2. create item layout This is what each of the items in RecyclerView should like. <FrameLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" android:padding= \"16dp\" android:orientation= \"vertical\" > <!--// TODO (8) Align the TextView to the start of the parent--> <!--// TODO (9) Center the TextView vertically in the layout--> <TextView android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:id= \"@+id/tv_item\" style= \"@style/TextAppearance.AppCompat.Caption\" android:fontFamily= \"monospace\" android:textSize= \"42sp\" android:layout_gravity= \"center_vertical|start\" /> </FrameLayout> 3. subclass RecyclerView.ViewHolder Create an inner class ( inside the Adapter class ), define class variables, override the constructor (takes a View as parameter), and add functions. // TODO (12) Create a class called NumberViewHolder that extends RecyclerView.ViewHolder class NumberViewHolder extends RecyclerView.ViewHolder { // TODO (13) Within NumberViewHolder, create a TextView variable called listItemNumberView TextView listItemNumverView; // TODO (14) Create a constructor for NumberViewHolder that accepts a View called itemView as a parameter // TODO (15) Within the constructor, call super(itemView) and then find listItemNumberView by ID public NumberViewHolder(View itemView) { super(itemView); listItemNumverView = (TextView) itemView.findViewById(R.id.tv_item); // this view is cached in ViewHolder } // TODO (16) Within the NumberViewHolder class, create a void method called bind that accepts an int parameter called listIndex // TODO (17) Within bind, set the text of listItemNumberView to the listIndex // TODO (18) Be careful to get the String representation of listIndex, as using setText with an int does something different // calling setText() with an integer, android will use the int as a string resource id void bind(int listIndex) { listItemNumverView.setText(String.valueOf(listIndex)); } } 4. subclass RecyclerView.Adapter The Adapter will create a viewholder for each recycler view item ( onCreateViewHolder ) inflat each item that will be displayed ( onCreateViewHolder ) bind data from data source to each item ( getItemCount ) return the number of items in data source ( onBindViewHolder) ⇒ create a class that extends RecyclerView.Adapter<MyViewHolder> , and implement methods. public class GreenAdapter extends RecyclerView.Adapter<GreenAdapter.NumberViewHolder> { int mNumberItems; public GreenAdapter(int nitems){ this.mNumberItems = nitems; } // TODO (5) Override the onCreateViewHolder method // TODO (6) Create and return a new NumberViewHolder within this method @Override public NumberViewHolder onCreateViewHolder(ViewGroup parent, int viewType) { Context context = parent.getContext(); LayoutInflater inflater = LayoutInflater.from(context); boolean shouldAttachToParentImmediatelly = false; View view = inflater.inflate(R.layout.number_list_item, parent, shouldAttachToParentImmediatelly); return new NumberViewHolder(view); } // TODO (7) Override onBindViewHolder // TODO (8) Within onBindViewHolder, call holder.bind and pass in the position @Override public void onBindViewHolder(NumberViewHolder holder, int position) { holder.bind(position); } // TODO (9) Override getItemCount and return the number of items to display @Override public int getItemCount() { return mNumberItems; } /** * Cache of the children views for a list item. */ class NumberViewHolder extends RecyclerView.ViewHolder { // Will display the position in the list, ie 0 through getItemCount() - 1 TextView listItemNumberView; /** * Constructor for our ViewHolder. Within this constructor, we get a reference to our * TextViews and set an onClickListener to listen for clicks. Those will be handled in the * onClick method below. * @param itemView The View that you inflated in * {@link GreenAdapter#onCreateViewHolder(ViewGroup, int)} */ public NumberViewHolder(View itemView) { super(itemView); listItemNumberView = (TextView) itemView.findViewById(R.id.tv_item_number); } /** * A method we wrote for convenience. This method will take an integer as input and * use that integer to display the appropriate text within a list item. * @param listIndex Position of the item in the list */ void bind(int listIndex) { listItemNumberView.setText(String.valueOf(listIndex)); } } } https://www.youtube.com/watch?v=ns3WC8HFx90 https://github.com/udacity/ud851-Exercises/blob/student/Lesson03-Green-Recycler-View/T03.03-Solution-RecyclerViewAdapter/app/src/main/java/com/example/android/recyclerview/GreenAdapter.java 5. use LayoutManager, putting things together layoutmanager determines when an item is recycled. 3 implementation of LayoutManager: LinearLayoutManager, GridLayoutManager, StaggeredGridLayoutManager . In the MainActivity, create a LinearLayoutManager, and set the recycler view's manager to it. Then create an Adapter, and set recycler's adapter to it. In the onCreate method: mNumbersList = (RecyclerView) findViewById(R.id.rv_numbers); // TODO (5) Create a LinearLayoutManager variable called layoutManager // TODO (6) Use setLayoutManager on mNumbersList with the LinearLayoutManager we created above LinearLayoutManager layoutManager = new LinearLayoutManager(this); mNumbersList.setLayoutManager(layoutManager); // TODO (7) Use setHasFixedSize(true) to designate that the contents of the RecyclerView won't change an item's size mNumbersList.setHasFixedSize(true); // TODO (8) Store a new GreenAdapter in mAdapter and pass it NUM_LIST_ITEMS mAdapter = new GreenAdapter(NUM_LIST_ITEMS); // TODO (9) Set the GreenAdapter you created on mNumbersList mNumbersList.setAdapter(mAdapter); 6. handle item click reciept To handle the click on items: a. define a Listener interface // TODO (1) Add an interface called ListItemClickListener // TODO (2) Within that interface, define a void method called onListItemClick that takes an int as a parameter interface ListItemClickListener{ void onListItemClick(int index); } b. make this Listener a private meber of the Adapter class, passed as constructor parameter. c. make the Adapter 's inner ViewHolder to implement View.OnClickListener interface, and override the onClick method: // TODO (6) Override onClick, passing the clicked item's position (getAdapterPosition()) to mOnClickListener via its onListItemClick method @Override public void onClick(View v) { int pos = this.getAdapterPosition(); mOnClickListener.onListItemClick(pos); } d. in the inner ViewHolder class' constructor, set onclicklistner of the passed item to the viewholder itself: public NumberViewHolder(View itemView) { super(itemView); listItemNumberView = (TextView) itemView.findViewById(R.id.tv_item_number); viewHolderIndex = (TextView) itemView.findViewById(R.id.tv_view_holder_instance); // TODO (7) Call setOnClickListener on the View passed into the constructor (use 'this' as the OnClickListener) itemView.setOnClickListener(this); } e. make MainActivity implments the Adapter.ItemClickListener, and provide the onclick method: public class MainActivity extends AppCompatActivity implements GreenAdapter.ListItemClickListener { //... // TODO (10) Override ListItemClickListener's onListItemClick method // TODO (11) In the beginning of the method, cancel the Toast if it isn't null // TODO (12) Show a Toast when an item is clicked, displaying that item number that was clicked @Override public void onListItemClick(int index) { if(mToast!=null) mToast.cancel(); mToast.setText(String.format(\"item %d is clicked!\", index)); mToast.show(); } } f. when creating Adapter in MainActivity, pass this as second constructor parameter","tags":"notes","title":"[Android Dev] 1.3 RecyclerView"},{"url":"http://x-wei.github.io/andev_p1e4_intents.html","text":"So far: single screen with a single activity. start activity from another activity: use Intent s. Explicit intent: start a new activity toy app: type some text, press button, and a new activity will appear with the typed words. create activity in android studio: new → activity call startActivity with an intent as parameter. Explicit intent constructor: a context (every Acitivity is a Context ), and a destination activity class. Intent ineten = new Intent(MainActivity.this, ChildActivity.class); startActivity(intent); This is called explicit intent as we know exactly which activity to start. in AndroidManifest.xml file, configure child activity having a back buttom to return to parent activity: <activity android:name= \".MainActivity\" android:label= \"@string/app_name\" > <intent-filter> <action android:name= \"android.intent.action.MAIN\" /> <category android:name= \"android.intent.category.LAUNCHER\" /> </intent-filter> </activity> <!-- TODO (4) Configure DetailActivity's up button functionality --> <activity android:name= \".DetailActivity\" android:parentActivityName= \".MainActivity\" > <meta-data android:name= \"android.suport.PARENT_ACTIVITY\" android:value= \".MainActivity\" /> </activity> Passing data between activities in main activity: Intent.putExtra(String name, String value) put data ( k-v pairs ) into the intent (some constant string names exists in Intent class, like Intent.EXTRA_TEXT ) then start new activity with this intent. in child activity: use getIntent() to get the comming intent, then use intent.getStringExtra(String name) , before getting extra data: use intent.hasExtra(name) to check if the extra stuff exists. Intent commingIntent = getIntent(); if(commingIntent.hasExtra(Intent.EXTRA_TEXT)){ String txt = commingIntent.getStringExtra(Intent.EXTRA_TEXT); mDisplayText.setText(txt); } Implicit Intent: opening a link when don't know/care which activity to start. example: want to visit a link or dial a number. ref of common intents: https://developer.android.com/guide/components/intents-common.html most implicit intent contains 2 parameters: an action and the associated data ( Uri ). Or use intent.setData to add uri to intent. To test if there is any application that can handle the intent: use intent.resolveActivity(getPackageManager()) private void openWebPage(String url) { // TODO (2) Use Uri.parse to parse the String into a Uri Uri uri = Uri.parse(url); // TODO (3) Create an Intent with Intent.ACTION_VIEW and the webpage Uri as parameters Intent intent = new Intent(Intent.ACTION_VIEW, uri); // TODO (4) Verify that this Intent can be launched and then call startActivity if(intent.resolveActivity(getPackageManager()) != null){ startActivity(intent); } } URIs URI : uniform resource identifier, a string that identifies resources. full format of an URI: components: scheme: ex. geo, http ... host path query fragment example of URI: uri1 = https://archive.org/web uri2 = geo:0,0?q=Montreal,Canada construct a geo URI now: want to open a location on maps. https://developers.google.com/maps/documentation/android-api/intents Use Uri.Builder to create complex URIs. public void onClickOpenAddressButton(View v) { String address = \"Boulvard des Marechaux\"; // TODO (6) Use Uri.Builder with the appropriate scheme and query to form the Uri for the address Uri.Builder builder = new Uri.Builder(); builder.scheme(\"geo\").path(\"0,0\").query(address); Uri uri = builder.build(); showMap(uri); // constructs an intent and starts activity } Share Intent When mutilple apps that can handle this intent ⇒ a chooser will pop up. Example: open an image / share a piece of text / share link ... → ShareCompat.IntentBuilder gives helper functionality for sharing data between activities. Media type (MIME): format: toplevel_type_name/subtype_name[; parameters] example: text/html;charset=UTF-8, image/png , text/plain , text/rtf, video/mp4 ,... private void shareText(String txt) { String mimeType = \"text/plain\"; String title = \"title of chooser window\"; ShareCompat.IntentBuilder.from(this) .setType(mimeType) .setChooserTitle(title) .setText(txt).startChooser(); }","tags":"notes","title":"[Android Dev] 1.4 Intents"},{"url":"http://x-wei.github.io/andev_p1e1_basics.html","text":"1 create project sunshine Created 星期一 06 二月 2017 minSDK vs targetSDK The minSDK is the lowest SDK level that your app can run on. You can choose what level of devices to support. By comparison, the targetSDK is NOT a high pass filter -- it's used only to declare which platform version you've tested your app on. Enable VT-x and installing kvm this is necessary for running AVD devices. http://stackoverflow.com/questions/37087365/how-to-enable-vt-x-in-bios-and-kvm-modules-on-linux Components of an android app 4 main components of an app: Activity : responsible for most user interaction Service ContentProvider BroadcaseReceiver Activity Android keeps the activities in a stack, when press back button → stack pops. Define the launcher of the activity in manifests/AndroidManifest.xml : application → activity → intent-filter res folder contains layouts/images/values in res/layout/activity_main.xml : xml file that defines the layout view Activities and Layouts An activity is a single focused thing that the user can do. Activities are responsible for creating the window that your application uses to draw and receive events from the system. Activities are written in Java, extending from the Activity class. An activity creates views to show the user information, and to let the user interact with the activity. View s are a class in the Android UI framework. They occupy a rectangular area on the screen and are responsible for drawing and handling events. An activity determines what views to create (and where to put them), by reading an XML layout file. These XML files, are stored in the res folder inside the folder labeled layouts . 2 types of views type1: UI components (widgets) UI components that are often interactive elements. example: TextView, EditView, Buttom, ImageView, ... most of UI view classes: in android.widgets package https://developer.android.com/reference/android/widget/package-summary.html type2: container view The second are views called \"Layout\" or \"Container\" views. They extend from a class called ViewGroup . They are primarily responsible for containing a group of views and determining where they are on screen. Layout views can be nested in one another. example: LinearLayout, RelativeLayout, ConstraintLayout, ... relate XML layout to java activities use xml layout in java activity in onCreate method of an activity, using the setContentView(R.layout.name_of_layout) the setContentView method: Android reads your XML file and generates Java objects for each of the tags in your layout file. You can then edit these objects in the Java code by calling methods on the Java objects. refer to widgets defined in xml in java in the xml file, give an id to the view:","tags":"notes","title":"[Android Dev] 1.1 Create Project Sunshine"},{"url":"http://x-wei.github.io/scala-ocaml-impression.html","text":"2016年的最后几小时, 随便写写关于Scala和OCaml的一些入门体验好了. 今年对FP语言特别感兴趣, 上了两门Scala的公开课( here and here )和一门OCaml的公开课( here ), 在博客中写了一系列的笔记, 课后作业也都认真做完了. 斗胆说这两门语言都算入门了吧... 这里就随便写一下使用这两门语言的感受, 想到哪里写到哪里... FP语言和之前接触的语言确实不大一样, 比如之前我都有种错觉, 学什么语言只要知道循环/条件/基本类型运算怎么写, 就差不多可以上手了...... 然后遇到了FP, 发现循环语句其实是不必要的... 记得看到过一篇文章, 类比学FP就好像开了很多种车的老司机突然开始学开宇宙飞船, 肯定各种WTF不适应了~ 以前谈到FP我只能联想到一些Python里的FP特性: lambda表达式, 高阶函数之类的, 顶多还想到个闭包... 不过Python里面的FP特性和Scala/OCaml里的比起来还是差了不少: i.e. 现在非常希望Python里可以支持pattern matching... Scala Scala算是比较亲民的FP语言了(和Java有点像...), 也是我最早接触的FP语言. EPFL的那两门公开课质量很棒, 毕竟是Scala的作者亲自来上的... immutable types : 习惯了就好, 就像java里所有东西都是final的, 要修改什么东西的时候改成新建一个, immutable数据的优点就是并行方便啊... 一切皆为表达式 , specifically, if 语句也是表达式(expression)而不是语句(statement) 尽量不用 return , 返回值就是最后一句expression 类型推导: 就不用写麻烦的类型标记了(有时又必须写, 比如必须要告诉Scala递归函数的返回类型)... 不过有时不标记类型又容易出错(心里以为的类型和实际的类型不一样). 另外Scala的类型是写在冒号后面的, 习惯了就好... for expression : (和\"for循环\"不是一回事) 非常好用, 可以说是一大亮点, 比较复杂的 filter/map 组成的表达式, 用for expr写出来非常容易理解, 超棒... 后来学OCaml的时候非常期待OCaml里也有类似的东西(不过好像没有...) pattern matching: 用constructor来switch! 没什么好说的 非常好用的东西... lazy evaluation和Stream : 非常有用的东西, 学到这里时很受启发... intellij真好用, 还有Scala worksheet... 尾递归优化 : 把递归函数做到和iterative的一样快, tailrec算是工具之一吧. 一开始对tailrec有点不太会写, 其实后来大概明白了怎么回事: 尾递归要求 f(n) 最后的return语句里应该直接是递归调用 f(n-1) , 不要加别的什么带 f(n-1) 的表达式. 一般的递归函数大概是: f(n) = some_expr_of f(n-1) , 而tailrec的版本里, 加入了 acc 这样一个参数, 保存累加结果. 可以把acc看做f(n-1)的代替品 , 这时, 把那个f(n-1)的expr作用在 acc 上, 就是尾递归了! f(n, acc)最后一句就成了 f(n-1, some_expr_of acc) — 简简单单一个递归调用~ 举个最简单的例子: 阶乘运算 非尾递归的写法是: def factorial ( n : Int ) : Int = if ( n == 0 ) 1 else n * factorial ( n - 1 ) 改成尾递归以后: def factorialTR ( n : Int ) : Int = { @tailrec def fact ( n : Int , acc : Int ) : Int = { if ( n == 0 ) acc else fact ( n - 1 , n * acc ) } fact ( n , 1 ) } 简单的匿名函数有几种写法: 用 => : param => ret 用pattern matching(这时要用花括号): {case(param list) => ret } 下划线就表示参数: _*2 — 相比之下 简直受够了Python里面写lambda....... 一些概念和Java对应一下: Unit 对应Java里的 void Option 对应Java里的Object(有指针的对象), Java里的 null 对应Scala里 None , Java里的某个对象(非null) A 对应Scala里的 Some[A] , Scala里和 Option 类似的还有 Try trait 大概可以看做Java里的 Interface , 不过Scala的trait是可以包含函数实现的... Scala的 object 和Java里可不是一个意思, Scala里object是单例对象( Singleton ), 类似于java里的 static final 的对象 有人说Scala被过度设计了, 感觉有点道理: 快学完第二门Scala公开课的时候看看别人代码, 又发现了好多没见过的关键字... @@ 并不太喜欢省略点号 省略括号/分号之类的语法糖... Scala的collection有点多: Seq/List/Array/IndexedSeq/Vector 傻傻分不清楚.... OCaml OCaml也是工业界比较成功的语言了, 而且据说速度和Cpp不相上下, 我对它可以说是向往已久, 看到FUN平台上有公开课就赶紧报名了... OCaml算是比较\"正宗\"的FP语言(或许还不够\"纯\"?), 风味和别的语言差了好多, 但是写着写着就喜欢上了... 函数参数不用括号! 这是第一个冲击... 不过总是把函数放在最前面, 还是不大习惯, 比如 List.map , 更希望可以写成 a_list map a_fun 这样... 运算符完全分隔: 整数加法是 + , 浮点数加法要显示写成 +. , 这样做的好处就是可以做异常强大的类型推导, 比Scala不知高到哪里去了~ 各种错误都可以在编译时找到, 安全感倍增... let-in binding : 理解了这个概念以后, 大脑有了一种开窍的感觉! 这个就非常接近数学推导了, 比如在数学上证明什么东西的时候, 令x y z等于什么什么, 其实最终都是为了在最后一个表达式里面体现. 只是起一个名字而已, 并不是改变了这些\"identifier\"的数值. Scala里面的\"最后一个表达式即为返回值\"这一点, 我当时有点不太习惯, 不过在OCaml里面用let-in绑定这么一写, 这不是显然的么?! ...总之这个写法理解了以后, 感觉非常美妙... 函数的类型为 para1_type -> para2_type -> ... -> ret_type , 这样写了以后对于理解partial function之类的就有很大帮助, 另外OCaml并不区分数值和函数了: 都是用let声明, 一个数值可以看做是接收0个参数的函数. try-with 处理异常: 除了处理异常以外, 由于没发现break对应的写法, 现在要break的时候我就也写成 try-with ... basic composed types: tuple/record/array 这三个, 个人感觉并不是特别好用... @@ taged type (又叫sum type/variant type): 感觉非常强大, 定义了taged type以后再pattern matching不要太爽... 感觉我对这块理解还不太够, 看别人写的taged type可能可以写一些处理的函数, 自己定义taged type就比较虚了... 好的程序, 把类型定义写好了 基本上就完成一半了... 关于模块(module)的签名的写法什么的, 其实还是有点晕... 可能实践还是不够 OCaml里的\"Polymorphism\"大概相当于Java里的generic/Cpp里的template. pattern matching: 在OCaml里被大量的应用, 感觉比Scala里面的还好用不少... utop作为交互式终端体验还是很棒的 编辑器和ide还是不够丰富, 没找到(非vim/emacs)很好的方案... 不过FUN平台的网络编辑环境非常给力, 自动indent这个feature太喜欢了, 最后所有作业都是直接在web环境下写的... opam包管理器有时候会出现一些模块安装不上的问题 听说 Core 提供了比较完善的函数支持, 希望学一下(也就是看看real world OCaml) compared with Python 那么这两个语言和我现在用得最多的语言(Python)相比较一下吧: 作为Python, 非常羡慕Scala/OCaml的执行速度 眼馋pattern matching和各种compile time保护...... Python的REPL( bpython )还是比Scala worksheet / ocaml utop体验好不少 Python的\"自带电池\"特色是很大的优点, 很好奇Scala/OCaml有没有提供defaultdict/counter/datetime之类的模块......","tags":"tech","title":"Scala/OCaml初体验"},{"url":"http://x-wei.github.io/python_crawler_requests_lxml.html","text":"前一段时间写了不少Python的爬虫程序, 为此还看了极客学院上的一些 教程 , 现在来简单总结一下. 主要介绍用 requests + lxml 的方式, scrapy 的话之前写过一篇介绍性的 文章 , 这里就不重复了. 而且感觉一般简单的爬虫项目, 一个Python文件就基本可以搞定, 没必要用scrapy建立一个工程文件夹搞那么正式... 安装需要的库(python2): pip install requests, lxml 然后在Python程序最开始导入: import requests from lxml import etree requests基础用法 抓取html内容 用requests获取目标网址的html代码非常简单, 只需要用 requests.get 方法, 传入网址URL即可. 举个例子, 想要抓取 维基语录 的HTML内容, 代码很简单: url = 'https://zh.wikiquote.org/zh-cn/阿爾伯特·愛因斯坦' r = requests.get(url) html = r.text requests.get() 返回一个response对象 r , 可以用 r.ok 或者 r.status_code 检查对象是否正常返回(status code=200). 编码问题 处理非英文网页时经常遇到的问题就是编码的问题了(不知道py3是不是对Unicode支持好一点?), 前面得到的html其实并非字符串而是Unicode对象: >>> type(html) <type 'unicode'> Unicode对象处理的时候一不小心就会得到以下的错误: UnicodeEncodeError: 'ascii' codec can't encode characters in position 101-113: ordinal not in range(128) 所以在那些需要string类型的地方, 需要用 encode 函数转换: >>> type(html.encode('utf-8')) <type 'str'> 另外实际中还遇到过比较奇葩的情况, 是返回的response的编码并不对(这个编码是requests根据网页内容自己推断的, 所以有时会出错), 比如 这个网址 , requests以为它的encoding是'ISO-8859-1', 所以为了保险起见, 最好手动指定r.encoding : r.encoding = 'utf-8 ' 另: 还有一种经常用的解决utf编码的方式, 就是在文件开头加上这四句话: # coding: utf-8 import sys reload ( sys ) sys . setdefaultencoding ( 'utf-8' ) 不过, 看到有人说这种方式 并不好 , 所以最好别用这么暴力的方式吧... 用scrapy shell检查得到的html文件内容 需要注意的一点是, requests.get得到的html内容 并不一定 和在浏览器打开链接得到的内容相同! 为了检查是否得到了想要的html内容, 有两个方式, 一个是把得到的内容输出为一个.html文件, 然后用浏览器打开, 比如这样: >>> with open('tmp.html', 'w') as f: ... f.write(html.encode('utf8')) # 注意要显式指定编码 这样做其实并不方便, 输出到本地文件以后还要用文件浏览器找到那个文件再打开, 而且打开的网页并没有图片, 也没有css样式. 我比较喜欢用scrapy shell这个工具, 这个工具在之前的文章也 提到过 , 它非常适合快速测试一些东西. 首先安装一下scrapy吧还是: pip install scrapy 然后输入 scrapy shell 即可使用. 用 fetch(url) 可以把返回的结果存放在(scrapy shell默认的) response 变量中, 可以把 fetch 操作理解为 response = requests.get(url) . 然后查看得到的html文件 只需要 view(response) , 就会自动用浏览器打开下载的临时文件, 非常方便. $ scrapy shell --nolog [ s ] Available Scrapy objects: [ s ] scrapy scrapy module ( contains scrapy.Request, scrapy.Selector, etc ) [ s ] crawler <scrapy.crawler.Crawler object at 0x7f8aa3b70e50> [ s ] item {} [ s ] settings <scrapy.settings.Settings object at 0x7f8aa3b70cd0> [ s ] Useful shortcuts: [ s ] shelp () Shell help ( print this help ) [ s ] fetch ( req_or_url ) Fetch request ( or URL ) and update local objects [ s ] view ( response ) View response in a browser In [ 1 ] : url = 'https://zh.wikiquote.org/zh-cn/阿爾伯特·愛因斯坦' In [ 2 ] : fetch ( url ) In [ 3 ] : view ( response ) Out [ 3 ] : True 修改header, 伪装浏览器 对于有些网站, 直接用 requests.get 抓取会得到403forbidden错误, 这时就要修改一下get函数的 headers 参数了, 把一个Python字典传给headers参数, 这个字典理, 'user-agent'对应chrome/firefox使用的内容. 例子: hea = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.118 Safari/537.36'} r = requests.get('http://jp.tingroom.com/yuedu/yd300p/', headers = hea) headers参数对于那些不太好爬的网站非常有用, 不过关于如何知道往header里放什么东西, 需要用chrome-dev-tools, 这个后面再说. lxml以及xpath语法 还是继续上面维基语录的例子, 假设现在已经获取了网页的html文件, 下一步就是在html文件里提取想要的内容了. 比如我们想要从维基语录上抓取 爱因斯坦 的所有名言. 从html中提取感兴趣的内容, 一种选择是用正则表达式, 不过正则表达式写起来太蛋疼了 — (?<=blablah).*(?=blah) 之类的, 每次用都得从新查. 而且处理html代码时经常容易出错. html语言可以看做是一种xml语言, 而xml语言其实是分层次的(可以parse为一个xml树), 操作xml元素的神器就是xpath语言了. xpath基础语法 xpath的语法其实不难, 入门的话话二十分钟看看 这里 估计就差不多. 这里简单列一下: 选取节点 的语法有: / 从根节点选取, // 从所有匹配的节点选取 . 当前节点, .. 当前的父节点 nodename 选取节点, @ 选取节点的属性 通配符: * , 选取若干路径则用 | 分隔 text() : 获取该节点的文本内容 例子: //img/@src : 选取所有img节点的src属性 //img/../text : 选取img节点的父节点下的text节点(所以text和img为\"sibling\"关系) //*/@src : 选取任何节点的src属性 然后 过滤节点 的谓词语法有: (谓词放在方括号中) [1] 选取第一个元素, [last()] 选取最后一个, [position<3] 选取前两个 [@lang=\"eng\"] 选取属性lang等于\"eng\"的元素 遇到更复杂的xpath不会写的话 尝试翻译成英文然后Google一下, 几乎总会找到答案. 使用chrome-dev-tool获得元素的xpath 可以直接用chrome的开发者工具获取网页元素的xpath, 在该网页上按下crtl-shift-I就可以打开devtool了: 点击左上角那个指针的小图标, 然后再在网页上点击想要查找的元素, 就可以快速定位到它在html里对应的代码了: 在代码中点击右键, 可以得到xpath: 不过一般chrome找到的xpath并不具有通用性, 所以最好还是自己分析得到合适的xpath代码. chrome给找到的xpath是 //*[@id=\"mw-content-text\"]/ul[1]/li[1] , 经过分析和测试, //div[@id=\"mw-content-text\"]/ul[position()<last()]/li/text() 应该是比较正确的所有名言的xpath代码. 为了测试xpath, 可以直接在chrome-dev-tool里面按下ctrl-F查找xpath: 用lxml.etree操作xpath 学会了xpath, 接下来要在Python里使用xpath则需要lxml. 步骤是: 首先用网页html内容建立一个etree对象, 然后在使用它的 xpath 方法, 传入之前得到的xpath语句. 返回的结果为一个list, list里面就是所有匹配的元素了. url = 'https://zh.wikiquote.org/zh-cn/阿爾伯特·愛因斯坦' r = requests.get(url) sel = etree.HTML(r.text) for quote in sel.xpath('//div[@id=\"mw-content-text\"]/ul[position()<last()]/li/text()'): print quote.strip() xpath使用技巧 这里说一下xpath的实际使用技巧. 正好前面的代码也不完善, 结合这个例子来说. 先抓大再抓小 其实之前的xpath还有不完美的地方, 比如爱因斯坦的页面中有不少名言还有\"原文\"这一信息: 在一个 li 节点下面有可能还有东西, 所以我们可以先获得这一个个 li 元素, 然后再在每个 li 元素里面尝试查找\"原文\"的信息. 代码如下: for li in sel.xpath('//div[@id=\"mw-content-text\"]/ul[position()<last()]/li'): quote = li.xpath('./text()')[0] print quote.strip() origin = li.xpath('./ul/li/span/i/text()') if len(origin)>0: print 'origin:', origin[0] 更复杂的例子比如豆瓣电影的页面, 每一个电影的entry都有电影名/上映时间/国家等好多信息. 处理这样的页面, 必须要先把大的元素(整个电影信息的div)抓取, 然后再在每个大元素里分别提取信息. 用 string() 获得nested节点文字内容 上面的代码运行结果还有不满意的地方: 对于一些带有超链接的名言, 我们的程序不能获取那些带有超链接的文字, 比如这句话: 它的html代码是这样的: <li> 一个 <a href= \"/w/index.php?title=%E5%BF%AB%E4%B9%90&amp;action=edit&amp;redlink=1\" class= \"new\" title= \"快乐（页面不存在）\" > 快乐 </a> 的人总是满足与活于当下，而非浪费时间揣想 <a href= \"/wiki/%E6%9C%AA%E6%9D%A5\" title= \"未来\" > 未来 </a> 。 </li> 如果直接用 /text() 处理的话, 只能得到\"一个\"这俩字... 问题出在这个元素是nested的, 里面嵌套了别的元素(两个 <a> ), 而这种情况还非常常见, 所以怎么办呢? 需要用xpath的 string() 函数, 它可以返回节点的正确字符串表示. 所以代码再次修改, quote的获取改为: quote = li.xpath('string(.)') . xpath里提供了蛮丰富的 函数 , 遇到比较复杂的操作的时候可以参考一下. 删除不想要的节点 进行了上面的修改, 又引入了新的问题: 对于那些有\"原文\"信息的li元素而言, 用string()函数的话会把这些原文信息也包括在内了, 这不是我们想要的结果. 比如这样的节点: 这时, 可以用lxml提供的 remove 函数, 在li节点中把不需要的节点先去掉, 然后再使用string()就不会有不需要的内容了. 最终的代码为: for li in sel . xpath ( '//div[@id=\"mw-content-text\"]/ul[position()<last()]/li' ): print '---' origin = li . xpath ( './ul' ) badnodes = li . xpath ( './ul' ) # remove 'origin' stuff in the li element for bad in badnodes : bad . getparent () . remove ( bad ) quote = li . xpath ( 'string(.)' ) print quote . strip () if len ( origin ) > 0 : print origin [ 0 ] . xpath ( 'string(.)' ) . strip () 动态页面/模拟登录: 善用chrome-dev-tools 上面的维基语录的例子还算比较简单, 对于那些需要动态加载的网页或者需要登录才可以查看的内容, 就需要多用chrome开发者工具了. 由于这方面要根据不同网站去试验(+猜测), 所以这里介绍的不会太详细... 一般来说, 对于动态加载的网页, 可以打开ctrl-shift-I打开devtools以后, 选择network标签页然后刷新, 在最开始的地方一般会有form提交(可以用 requests.post 模拟)或者url请求之类的东西, 一路追踪过去即可. 这里展示一下用cookies模拟登录微博的过程. weibo电脑版的页面太过凌乱, 用微博手机版(weibo.cn). 用dev-tools获取登录cookies cookies就是一小段(加密后的)字符串, 它的大概是本地存储的保留用户信息的加密字符, 有的网站点选\"下次自动登录\"时, 其实就是生成了一个cookie保存在本地, 下次登录时只要向网站发送这串cookies字符, 如果cookies没有过期的话就可以直接登录了. 在要点击登录前, 打开devtools并选择network标签. 然后在登录以后, 找开头的几个requests, 定位到一个header带有cookie的request上面, cookie就在这里了(我试验发现, 好像需要登录以后再刷新一下, 这时dev-tools得到的cookies才是可用的): 另一种办法是用chrome自带的监测页面 , (设置capture→Include the actual bytes sent/received), 也可以得到cookies: 在requests里使用cookies 一旦获得了cookies字符串, 模拟登录就很简单: 在requests.get里传入headers参数: import requests hea = { 'Cookie' : '_T_WM=3a52fbed11ed299552cf910553be7d3b; SUB=_2A251Y_geDeTxGedG6lUQ9SrKyj2IHXVWr5hWrDV6PUJbkdAKLUejkW1CLxUVXEMZZq8EFgsGuIYNqC6MqQ..; gsid_CTandWM=4uno88c512gBK6O5nyuKd7CIW9R' } url = 'http://weibo.cn' html = requests . get ( url , headers = cook ) . content # use content instead of text print html 保存爬取的内容 保存文本内容: csv 保存文本信息我一般喜欢放进csv里面, 而用 pandas 操作csv文件会比较方便: 在程序中, 把每一个抓取的条目(item)放进一个字典, 然后append到dataframe里面, 最后直接 to_csv 搞定. 下面是个简单的示意代码, 假设我们要抓取一些文章的title, date和发表地点三个信息: import pandas as pd df = pd . DataFrame () # ... for item in __loop__ : #... title , place , date = __code_for_extracting_these_fields__ #... series = pd . Series ({ 'title' : title , 'place' : place , 'date' : date }) df = df . append ( series , ignore_index = True ) df = df [[ 'title' , 'date' , 'place' ]] # adjust column order df . to_csv ( 'melanthon.csv' , index = False , encoding = 'utf-8' ) 保存非文本内容 有些时候我们要下载图片/视频等非文本的信息, 我们可以用xpath定位到图片/视频的链接地址处, 那么下载到本地文件, 我查的有两个办法. 第一个方法简单粗暴: 用urlretrieve, 直接往函数里传入url和本地路径即可: from urllib import urlretrieve urlretrieve ( img_url , fpath ) 另一个方法还是用requests, 用分片的方式获取文件(我猜这种更适合大文件的下载?): resp = requests.get(url, stream=True) f = open(fpath, 'wb') for chunk in resp.iter_content(chunk_size=1024): if chunk: # filter out keep-alive new chunks f.write(chunk) f.close() 并行下载 在下载大文件的时候可以非常明显感受到, 下载文件的过程占据了大部分程序的执行时间. 比较简单的加速办法就是, 先把所有要下载的文件url(以及本地保存的fpath)放进一个list里, 最后在一起下载, 这时就可以使用Python的多进程模块进行加速了. 核心的代码只其实就是pool.map, 把爬去的函数map到要爬的url列表上: from multiprocessing.dummy import Pool pool = Pool ( 4 ) results = pool . map ( crawl_func , urls_list ) pool . close () pool . join () 下面是个实际的例子, 首先定义了一个download函数用于下载视频, 然后download_videos函数, 多线程下载视频. def download (( url , fpath ), headers = {}): fname = os . path . split ( fpath )[ - 1 ] print 'start downloading %s ...' % fname with open ( fpath , 'wb' ) as f : while 1 : resp = requests . get ( url , stream = True , headers = headers ); time . sleep ( 1.0 ) if resp . ok : break print resp . status_code for chunk in resp . iter_content ( chunk_size = 1024 ): if chunk : # filter out keep-alive new chunks f . write ( chunk ) print 'download finished: %s ' % fpath def download_videos ( video_urls_list ): # input = list of (url,fpath) pairs print 'downloading %d files in parallel...' % len ( video_urls_list ) from multiprocessing import Pool pool = Pool ( processes = 4 ) pool . map ( download , video_urls_list ) pool . close () pool . join () print 'all downloading finished !' 最后, 我写了一个极客学院课程视频的下载脚本, 用cookies模拟登录. 一百来行的代码, 跑一晚上可以下载好几十G的视频... gist放在: https://gist.github.com/X-Wei/46817a6614e3677391ab13e420b4cb9f (不过这里用的cookies早就过期了)","tags":"tech","title":"使用requests和lxml编写python爬虫小记"},{"url":"http://x-wei.github.io/ocamlMOOC_wk6_modules.html","text":"this week: programming-in-the-large using the module system of OCaml. 1. STRUCTURING SOFTWARE WITH MODULES in large project: mangage high number of definitions → abstractions built on top of other abstractions. layers of abstractions: hide information divide program into components identifiers organised to avoid naming conflicts module as namespace dot-notation: access module component. ex. List.length or first open List then just call length if open 2 modules having identical identifiers, the last opened module will be used. to define a module: module SomeModuleIdentifier = struct (* a seq of definitions *) end module name: start with an upper case to alias a module: module SomeModuleIdentifier = SomeOtherModuleIdentifier # module Stack = struct type ' a t = ' a list let empty = [] let push x s = x :: s let pop = function | [] -> None | x :: xs -> Some ( x , xs ) end ;; module Stack : sig type ' a t = ' a list val empty : ' a list val push : ' a -> ' a list -> ' a list val pop : ' a list -> ( ' a * ' a list ) option end # let s = Stack . empty ;; val s : ' a list = [] # let s = Stack . push 1 s ;; val s : int list = [ 1 ] # let x , s = match Stack . pop s with | None -> assert false | Some ( x , s ) -> ( x , s );; val x : int = 1 val s : int list = [] # let r = Stack . pop s ;; val r : ( int * int list ) option = None hierachical module structure a module can contain other module definitions a signature can also contain module signatures if module B is inside A , use A.B to get its namespace module Forest = struct type ' a forest = ' a list module Tree = struct type ' a tree = Leaf of ' a | Node of ' a tree forest end end ;; open Forest . Tree ;; let t = Leaf 42 ;; 2. INFORMATION HIDING a module should come with some user manual (\"contract\") to indicate to clients: function preconditions that must be verified data invariants that must be preserved definitions that user must not rely on (cause they'll change in the future) a module signature represents this contract, the type checker will enforce point 2 and 3 . module signatures a module's type is called signature or interface programmer can force a module to have a specific signature to define a signature: module type sig (* a seq of declarations of following form:*) val some_identifier : some_type type some_type_identifier = some_type_definition exception SomeException of some_type end to construct a module with a specific signature: module M: sig ... end = struct ... end to name a signature: module type S = sig ... end then use this name to annotate module: module M:S = struct ... end example: natural numbers module Naturals : sig (* Invariant: A value of type t is a positive integer *) type t = int val zero : t val succ : t -> t val pred : t -> t end = struct type t = int let zero = 0 let succ n = if n = max_int then 0 else n + 1 let pred = function | 0 -> 0 | n -> n - 1 end ;; abstract types we can use the module normally: open Naturals ;; let rec add : t -> t -> t = fun x y -> if x = zero then y else succ ( add ( pred x ) y );; but the invariant can be easily broken: let i_break_the_abstraction = pred (- 1 );; This don't have compiler error, as the type of pred is int , we can pass any int to it. ⇒ use abstract types that will give no choice to the client but to respect the rule. in the signature: module Naturals : sig (* Invariant: A value of type t is a positive integer *) type t (* remove the type value of t in the signature *) val zero : t val succ : t -> t val pred : t -> t end then calling pred (-1) will cause an error. → we have hiddent the definition of the type t the sig don't publish t's implementation anymore, so the checker ensures clients can't use that fact t is called an abstract type . With abstract type, users can't do pattern matching, to allow pattern matching while forbidding the direct application of data constructors, OCaml provides a mechanism called private types . see here . 3. CASE STUDY: A MODULE FOR DICTIONARIES An example of using abstract types to increase the modularity of programs. Define a dictionary signature: module type DictSig = sig type ( ' key , 'value ) t = ( ' key * 'value ) list (* internal repr of the dict is exposed@ *) val empty : ( ' key , 'value ) t val add : ( ' key , 'value ) t -> ' key -> 'value -> ( ' key , 'value ) t exception NotFound val lookup : ( ' key , 'value ) t -> ' key -> 'value end ;; module Dict : DictSig = struct type ( ' key , 'value ) t = ( ' key * 'value ) list (*......implementation *) end ;; Then a client can use this module: module ForceArchive = struct let force = Dict . empty let force = Dict . add force \"luke\" 10 let force = Dict . add force \"yoda\" 100 let force_of_luke = Dict . lookup force \"luke\" let all_jedis = List . map fst force (* here client knows that dict is a list!*) end ;; This is not very good if the internal implemtation of Dict is changed into For instance, change the implemention into a BST: type ( ' key , 'value ) t = | Empty | Node of ( ' key , 'value ) t * ' key * 'value * ( ' key , 'value ) t → change the signature of module to abstract type. 4. FUNCTORS Functors are functions from modules to modules. In other words, a functor is a module parameterized by another module. Continue the last example, we want to choose a Dict implementation externally. → module functor To declear a functor, add the Dict module in the parameter module ForceArchive ( Dict : DictSig ) = struct let force = Dict . empty let force = Dict . add force \"luke\" 10 let force = Dict . add force \"yoda\" 100 let force_of_luke = Dict . lookup force \"luke\" end ;; Then we can call the explicit implementation in the client: module Dict_list : DictSig = struct ... end ;; module Dict_bst : DictSig = struct ... end ;; module Client1 = ForceArchieve ( Dict_list ) module Client2 = ForceArchieve ( Dict_bst ) a functor is a module waiting for another module syntax: module SomeModuleIdentifier (SomeModuleIdentifier: SomeSignature) = struct ... end;; to apply a functor to a module: SomeModuleIdentifier (SomeModule) signature of a functor: functor (ModuleIdentifier: SomeSignature) -> sig ... end example: Set and Map They expects a module satisfying the following signature: module type OrderedType = sig type t val compare : t -> t -> int end Once a module E has this signature, Set.Make (E) offers over sets of E.t elements Map.Make (E) .... type parameterization of exception can be done using functor. In signature declaration: module type DictSig = sig (* before: type ('key, 'value) t *) type key (* make the key not polymorphic *) type 'value t val empty : 'value t val add : 'value t -> key -> 'value -> 'value t exception NotFound of key (* parameterize exception type in signature *) val lookup : 'value t -> key -> 'value end ;; In the implementation make the module a functor : add the Key module as argument module Dict ( Key : sig type t val compare : t -> t -> int end ) : DictSig = struct type key = Key . t (* key is the type of the Key module *) ... end ;; (... don't quite get it......) type constraint : DictSig with key=string","tags":"notes","title":"[OCaml MOOC] week6: MODULES AND DATA ABSTRACTION"},{"url":"http://x-wei.github.io/ocamlMOOC_wk5_imperative.html","text":"1. IMPERATIVE FEATURES IN OCAML functional language: immutable data structure identifiers instead of variables pure functions but imperative features are useful: exceptions to alter control flow ops to consume input and output mutable data structures for and while loop for iterations 2. GETTING AND HANDLING YOUR EXCEPTIONS exceptions in ocaml are just constructors of a special sum type exn . the constructors can have arguments new exception can be defined at any time unlike usual sum types, exn can be extended but exception can't be polymorphic raise exception to define an exception sum type: exception E;; to raise exceptions: raise E;; when an exception is raise, any computation is immediately stopped : let _ = raise E in [1;2;];; example: exception EmptyList ;; (*define an exn value*) let head = function | a :: r -> a | [] -> raise EmptyList ;; head [ 1 ; 2 ; 3 ];; head [] ;; handling exceptions using try-with to capture exceptions try expr with | p1 -> e1 | p2 -> e2 expr is evaluated, if during evaluation of expr , any E is raised, then it's matched with patterns in the with clause if E matches pi , then evaluate expression ei note: all the ei s must have the same type as expr example: mul elements of a list, once encountered a 0, stop the computation. let rec multl = function | [] -> 1 | a :: rest -> if a = 0 then 0 else a *( multl rest );; (*the above implementation is not efficient: if 0 is at the end, then have to pop the whole stack and get 0 at the end *) (*using exceptions to directly return as soon as we get a 0 (and throw away the stack) *) exception Zero ;; let mullexc l = let rec aux = function | [] -> 1 | a :: rest -> if a = 0 then raise Zero else a *( aux rest ) in try aux l with Zero -> 0 ;; runtime errors runtime errors can be: div by 0 incomplete pattern matching out-of-bound In these cases, ocaml raise an exception. 3. GETTING INFORMATION IN AND OUT the unit type has only 1 value: () often used as input/output type for funcitons with side effects Simple input/output print_int : print out an integer read_line : read one line of string other similar funs... # print_int ;; - : int -> unit = < fun > # print_int 123 ;; 123 - : unit = () # read_line ;; - : unit -> string = < fun > # read_line () ;; (* give the func a parameter *) hello - : string = \"hello\" 4. SEQUENCES AND ITERATIONS sequences of expressions ex. print several values. let _ = print_int 1 in let _ = print_int 2 in let _ = print_int 3 ;; cleaner way: separate such expressions just by single semi-colon: print_int 1 ; print_int 2 ; print_int 3 ; expression sequence: e1; e2; ...; en eval ei in turn drops all internal results, return the last expression all intermediate exprs should be of type unit (otherwise warning) note: precedence of ; : if true then print_int 1; print_int 2;; either use parentheses or use begin-end: (e1; ...; en) or: begin e1; e2;... en end iterations ex. print all integers from 1 to 10 can define a recursive foreach function: # let foreach starti endi f = let rec aux n = if n <= endi then ( f n ; aux ( n + 1 )) else () in aux starti ;; val foreach : int -> int -> ( int -> ' a ) -> unit = < fun > # foreach 1 10 print_int ;; 12345678910 - : unit = () for loop an easier way: for loop # for i = 1 to 10 do print_int i done ;; 12345678910 - : unit = () for id = e1 to e2 do e3 done the id cannot be altered loop body e3 is evaluated for each value of id type of the for loop is unit loop body e3 is expected to be unit (warning otherwise) alternative: backward for-loop for i = 10 downto 1 do print_int i done ;; while loop while e1 do e2 done condition e1 is evaluated, if true, the body e2 is evaluated type of the while loop is unit , body e2 expected to be unit to ignore the warning when loop body is not unit: use ignore :'a -> unit function 5. MUTABLE ARRAYS the (immutable) array: fixed size access to elements via index constant time for accessing element well-adapted to loop constructs example: find cubes which are also squares: let cubes n = Array . init n ( fun i -> i * i * i );; let sqrti n = truncate ( sqrt ( float n )) (*get sqrt of an integer*) ;; let issquare n = let s = sqrti n in s * s = n ;; let squaresubes n = let c = cubes n in for i = 0 to n - 1 do if issquare c .( i ) then ( print_int c .( i ); print_string \" \" ) done ;; ocaml arrays are real arrays: each cell can be modified in place using <- operator in-place modification: e1 <- e2 e1 denotes a mutable value type of the opeartion is unit let a = [| 0 ; 1 ; 2 ; 3 ; 4 |];; a .( 0 );; a .( 0 ) <- 100 ;; a .( 0 );; example: rotate array let rotate a = let n = Array . length a in let v = a .( 0 ) in for i = 0 to n - 2 do a .( i ) <- a .( i + 1 ) done ; a .( n - 1 ) <- v ;; 6. MUTABLE RECORD FIELDS records: tuples with distinct name components. some_type_identifier = { field_name_1 : some_type1 ; ...; filed_name_n : some_typen } let r = { field_name_1 = e1 ; ...; field_name_n = en } example: type point2D = { x : int ; y : int };; let origin = { x = 0 ; y = 0 };; let offset_h p dx = { p with x = p . x + dx };; (*create a new point*) let offset_v p dy = { p with y = p . y + dy };; we can define some fields of a record to be mutable: type some_type_identifier = {...; mutable field_name_i: some_type_i;...} these mutable fields can be modified in place using <- type point2D = { mutable x : int ; mutable y : int }; let origin = { x = 0 ; y = 0 };; let move p dx dy = p . x <- p . x + dx ; p . y <- p . y + dy ;; (* modify p in place*) move origin 2 2 ;; 7. VARIABLES, AKA REFERENCES more consise way: the predefined ref type (that wraps the above): ' a ref = {mutable contents: 'a) the ref function: ' a -> 'a ref !r : prefix operator to read the contents of the reference r r:=v : update the content of the reference # let i = ref 0 ;; val i : int ref = { contents = 0 } # i ;; - : int ref = { contents = 0 } # i := ! i + 1 ;; - : unit = () # i ;; - : int ref = { contents = 1 } example: log2 of an integer let log2int n = let count = ref 0 and v = ref n in while ! v > 1 do count := ! count + 1 ; v := ! v / 2 done ; ! count ;; log2int 16 ;; example 2: read int list from stdin # let read_int () = in_of_string ( read_line () );; # let read_intlist () = let l = ref [] in let doread () = try while true do l := ( read_int () ) :: ! l done with _ -> () in doread () ; List . rev ! l ;; # read_intlist () ;; 1 2 3 4 5 6 &#94; C - : int list = [ 1 ; 2 ; 3 ; 4 ; 5 ; 6 ]","tags":"notes","title":"[OCaml MOOC] week5: EXCEPTIONS, INPUT OUTPUT AND IMPERATIVE CONSTRUCTS"},{"url":"http://x-wei.github.io/ocamlMOOC_wk4_higher_order_fun.html","text":"1. FUNCTIONAL EXPRESSIONS syntax for functional expr: function some_identifier -> some_expr the type of the functional expr is t1 -> t2 where t1 is the type of some_identifier, t2 is type of some_expr ex. function x -> x + 1 ;; ( function x -> 2 * x ) 5 ;; (*annonymous function*) the previous way of defining function: let f x = e , is just an abbreviation for common let-binding: let f = function x -> e In fact, the most general form of a function definitoin contains a seq of pattern matching: function | pattern_1 -> expr_1 | pattern_2 -> expr_2 When only 1 pattern, the | is omitted. example let rc length = function | [] -> 0 | _:: r -> 1 + length r ;; type expr = | Int of int | Add of expr * expr ;; let rec eval = function | Int n -> n | Add ( e1 , e2 ) -> ( eval e1 ) + ( eval e2 );; 2. FUNCTIONS AS FIRST-CLASS VALUES functional types are just values of a particular type, thus this allows the uniform way of naming a value let x = ... Types govern function application. We can apply e1 to e2 when: e1 has type t1 -> t2 t1 matchs the type of e2 let f1 = [( function x -> x + 1 ); ( function x -> 2 * x )];; ( List . hd fl ) 17 ;; let apply_twice f x = f ( f x );; apply_twice ( function x -> 2 * x ) 1 ;; let rec apply_n_times f n x = if n <= 0 then x else apply_n_times f ( n - 1 ) ( f x );; apply_n_times ( function x -> 2 * x ) 10 1 ;; let compose f g = ( function x -> f ( g x ));; let fg = compose ( function x -> x + 1 ) ( function x -> 2 * x ) in fg 10 ;; function pitfalls function apply from left to right ( function application associates to the left ): exp1 exp2 exp3 is equivalent to: (exp1 exp2) exp3 let double = function x -> 2 * x ;; double double 5 ;; double ( double 5 );; 3. FUNCTIONS WITH MULTIPLE ARGUMENTS anonymous function with several arguments: use keyword fun : fun p1 ... pn -> exp unlike function keyword, fun only admits one case/branch remark: funs with several arguments are just abbrevations for single-argument functions that returns a function: let f1 = function n -> ( function x -> n + x );; ( f1 17 ) 73 ;; f1 17 73 ;; let f2 = fun n x -> n + x ;; f2 17 73 ;; ( f2 17 ) 73 ;; in fact, fun x1 ... xn -> e is just abbreviation for: function x1 -> (..(function x2 -> ... -> (function xn -> e)..) 4. PARTIAL FUNCTION APPLICATION let f = fun x y -> exp is equivalent to: let f = function x -> (function y -> exp) ⇒ partially apply f (ie, f x ) will give a function. let f1 = fun x y x -> x + y * z ;; let f2 = f1 1 ;; let f3 = f2 2 ;; f3 4 ;; what happens at func-application: when applying f = function x->e to a : → evaluate e in the context x=a → the arrow -> will block any evaluation let f = fun x y -> ( x / 0 ) + y ;; let f2 = f1 17 ;; (*error will not happen here, as the `->` blocks the evaluation!*) f2 42 ;; partial evaluation sometimes we can do part of a calculation as soon as we have the frist few arguments ⇒ extract that part of calculation before the arrow ! let egal l1 l2 = List . sort compare l1 = List . sort compare l2 ;; let egalp l1 = let l1sorted = List . sort compare l1 in (*sort the 1st argument before going to the next functional abstraction*) function l2 -> l1sorted = List . sort compare l2 ; 5. MAPPING FUNCTIONS ON LISTS many useful functions in the List module, either open List at beginning, or with pointed notation ( List.hd ) implementation let rec map f = function | [] -> [] | h :: r -> ( f h ):( map f r );; map ( function x -> x * x ) [ 1 ; 2 ; 3 ; 4 ; 5 ];; let map2 f l1 l2 = match ( l1 , l2 ) with | [] , [] -> [] | h1 :: r1 , h2 :: r2 -> ( f h1 h2 )::( map2 f r1 r2 ) | _ -> raise ( Invalid_argument \"List.map2\" );; map2 ( fun x y -> x + y ) [ 1 ; 2 ; 3 ] [ 10 ; 20 ; 30 ];; examples example1: int vectors/matrices row vector: int list matrix: list of row vectors turn infix operators into functions : using parentheses (+) (/) ( * ) ...(note: spaces are necessary for * , otherwise this turns into comments... let vsum = List . map2 (+);; (* use partial application *) vsum [ 1 ; 2 ; 3 ] [ 10 ; 20 ; 30 ];; let msum = List . map2 ( List . map2 (+));; (* nested partial application *) msum [[ 1 ; 2 ;]; [ 3 ; 4 ]] [[ 10 ; 20 ]; [ 30 ; 40 ]];; example2: get all sublists of a list type: a' list -> 'a list list write using induction: let rec sublists = function | [] -> [ [] ] | h :: r -> let rp = sublists r let appendh = function lst -> h :: lst in rp @ ( map appendh rp ) 6. FOLDING FUNCTIONS ON LISTS map: apping a unary function on list, all elements considered isolated. folding: combining all elements of a list using a binary operator . 2 different ways of folding: fold-left/fold-right fold_right fold_right: ('a -> 'b -> 'b) -> 'a list -> 'b -> 'b implementation and usage: let rec fold_right f l b = match l with | [] -> b | h :: r -> f h ( fold_right f r b );; fold_right (+) [ 1 ; 2 ; 3 ; 4 ] 0 ;; let concat = fold_right ( fun x y -> x :: y );; ( partial application of fold_right ) concat [ 1 ; 2 ; 3 ] [ 3 ; 4 ; 5 ];; fold_left fold_left: ('a -> 'b -> 'a) -> 'a -> 'b list -> 'a note: the default value's position is different from that of fold_right ! let rec fold_left f a l = match l with | [] -> a | h :: r -> fold_left f ( f a h ) r ;; fold_left (+) 0 [ 1 ; 2 ; 3 ];; let reverse = fold_left ( fun x y -> y :: x ) [] ;; (* partial application *) reverse [ 1 ; 2 ; 3 ; 4 ];; examples example1: Inner product of int vectors first get pairwise product, then sum up. let product v1 v2 = List . fold_left (+) 0 ( List . map2 ( * ) v1 v2 );; product [ 1 ; 2 ; 3 ] [ 4 ; 5 ; 6 ];; example2: countif let countif p l = List . fold_left ( fun acc elem -> if p elem then acc + 1 else acc ) 0 l ;; countif ( fun x -> x > 0 ) [ 3 ; 1 ;- 2 ; 0 ; 4 ];;","tags":"notes","title":"[OCaml MOOC] week4: HIGHER ORDER FUNCTIONS"},{"url":"http://x-wei.github.io/ocamlMOOC_wk3_advanced_ds.html","text":"Last week, we only defined flat data structures which are nice to aggregate values but quite limited when you try to structure values. This week: algebraic datatypes . 1. TAGGED VALUES ⇒ change the return type to a type query_result , which can be either of these: an error a new database (in case of successful insertion/deletion) a contact and its index (in case of successful search) in ocaml, can define such a type (called sum type ) by : # type query_result = | Error | NewDatabase of database | FoundContact of contact * int ;; More generally, to define disjoint union of types: type some_type_identifier = | SomeTag of some_type | ... | SomeTag of some_type tag must start with uppercase letter Taga are also called conscturcors , grammar is like java constructors: SomeTag (some_expr, ..., some_expr) (the parenthesis can be omitted if only 1 expr is required) enumeration: type color = Black | Gray | White;; observing tagged values must prvide an expression for each possible case of the value. A case is described by a pattern: SomeTag (some_pattern, ..., some_pattern) A branch is composed of a pattern an an expr separated by an arrow. some_pattern -> some_expr pattern matching is a seq of branches: match some_expr with | some_pattern -> some_expr |... | some_pattern -> some_expr example: let engine db query = match query with | Insert contact -> insert db contact | Delete contact -> delete db contact | Search name -> search db name ;; synatactic shortcut: function keyword (for functions with only 1 argument) pitfalls ill-typed pattern non-exhaustive case analysis These errors can be caught by the checker. 2. RECURSIVE TYPES data structures with unbounded depth, ie, list/tree. For example, an integer list can be defined as: # type int_list = | EmptyList | SomeElement of int * int_list ;; type int_list = EmptyList | SomeElement of int * int_list in the machine: functions on such datastruct usually use pattern matching: # let rec length = function | EmptyList -> 0 | SomeElement ( x , l ) -> 1 + length l ;; val length : int_list -> int = < fun > The predefined type in ocaml: t list empty list: [] ( [] is just a special tage corresponding to EmptyList) head and tail: i::r ( :: is just a special tage corresponding to SomeElement) a list can be defined by enumeration: [some_expr; ...; some_expr] list concatenation: @ # let rec length = function | [] -> 0 | x :: xs -> 1 + length xs ;; val length : ' a list -> int = < fun > # length [ 1 ; 2 ; 3 ;];; - : int = 3 # let rec rev = function | [] -> [] | x :: xs -> ( rev xs )@[ x ];; val rev : ' a list -> ' a list = < fun > # rev [ 1 ; 2 ; 3 ; 4 ];; - : int list = [ 4 ; 3 ; 2 ; 1 ] the rev function above has quad-complexity → here is the tail rec version: # let rec rev_aux accu = function | [] -> accu | x :: xs -> rev_aux ( x :: accu ) xs ;; val rev_aux : ' a list -> ' a list -> ' a list = < fun > # let rev l = rev_aux [] l ;; val rev : ' a list -> ' a list = < fun > 3. TREE-LIKE VALUES the database type is formed in a (binary)tree-like fashion: # type database = | NoContact | DataNode of database * contact * database ;; type database = NoContact | DataNode of database * contact * database impose the BST invariant: Now the functions insert/search/delete is BST fashion: 4. CASE STUDY: A STORY TELLER type-directed programming: writing the right type declaration is half success. define a story type (and other types: type story = { context : context ; perturbation : event ; adventure : event list ; conclusion : context ; } and context = { characters : character list } and character = { name : string ; state : state ; location : location ; } and event = | Change of character * state | Action of character * action and state = Happy | Hungry and action = Eat | GoToRestaurant and location = Appartment | Restaurant ;; 5. POLYMORPHIC ALGEBRAIC DATATYPES parametric programming: example — list is parametrized by the element type. Hence in List module contains polymorphic functions. Good for code reuse. define your own polymorphic types, using ' a to indicate unkonw types: type ('a1,...,1aN) some_type_identifier = some_type example: type ' a option = | None | Some of ' a ;; type ( ' a , ' b ) either = | Left of ' a | Right of ' b ;; type square = { dimension : int );; type circle = { radius : int );; type shape = ( square , circle ) either ;; another example: bst: type ' a bst = | Empty | Node of ' a bast * a' * ' a bst ;; let rec insert x = function | Empty -> Node ( Empty , x , Empty ) | Node ( l , y , r ) -> if x = y then Node ( l , y , r ) else if x < y then Node ( insert x l , y , r ) else Node ( l , y , insert x r );; 6. ADVANCED TOPICS precise typing when 2 types have the same structure but different semantical meaning: a sum type with only one constructor can be useful to distinguish them. example: type euro = Euro of float ;; type dollar = Dollar of float ;; let euro_of_dollar ( Dollar d ) = Euro ( d /. 1 . 33 );; let x = Dollar 4 ;; let y = Euro 5 ;; let valid_comparison = ( euro of dollar x < y ) disjunctive patterns Use or-patterns to factorize branches into a unique branch: some_pattern_1 | some_pattern_2 means observation of either pattern 1 or pattern 2. constraint: both must contain the same identifiers . ex: let remove_zero_or_one_head = function | 0 :: xs | 1 :: xs -> xs | l -> l let remove_zero_or_one_head' = function | ( 0 | 1 ):: xs -> xs | l -> l as-patterns convenient ot name a matched component : some_pattern as x ( if the value can be observed using some_pattern, name it x) ex. let rec duplicate_head_at_the_end = function | [] -> [] | ( x ::_) as l -> l @[ x ] guard: pattern matching branch using when a guard (some bool-expression) can add an extra constraint to a pattern: ex. let rec push_max_at_the_end = function | ( [] | [_]) as l -> l | x ::(( y ::_) as l ) when x <= y -> x ::( push_max_at_the_end l ) | x :: y :: ys -> y :: push_max_at_the_end ( x :: ys );; (*when x>y, should permuate x and y*)","tags":"notes","title":"[OCaml MOOC] week3: MORE ADVANCED DATA STRUCTURES"},{"url":"http://x-wei.github.io/ocamlMOOC_wk2_basics_ds.html","text":"this week: structure code with types: tuples, records, arrays. 1. USER-DEFINED TYPES primary use of types: document your code use type type_identifier = some_type to define a new type ( type_identifier is synonym/abbrevation of some_type ) type_identifier must start with lowercase letter already known types: int, bool, string, char ... use : to add type annotation to identifiers let x : some_type = some_expr annotation param/return type of a function: let f(x: some_type): return_type = some_expr or let f x : return_type = some_expr example: colors # type color = int ;; type color = int # let red : color = 0 ;; val red : color = 0 # let white : color = 1 ;; val white : color = 1 # let blue : color = 2 ;; val blue : color = 2 example: positive integers # type positive = int ;; type positive = int # let abs ( x : int ) = ( if x < 0 then - x else x : positive );; val abs : int -> positive = < fun > # let abs' ( x : int ): positive = if x < 0 then - x else x ;; val abs' : int -> positive = < fun > pitfalls in the REPL, careful with unintended hiding of type identifiers: # type t = int ;; type t = int # let x : t = 0 ;; val x : t = 0 # type t = bool ;; type t = bool # let f ( x : t ) = not x ;; val f : t -> bool = < fun > # let z = f x ;; Error : This expression has type t / 1027 = int but an expression was expected of type t / 1029 = bool limitations of type synonyms ⇒ but ocaml has ways to define more precise types to avoid such error statically 2. TUPLES: positioned components Some objects are naturally made of several components, example: 2d point. # let origin = ( 0 , 0 );; val origin : int * int = ( 0 , 0 ) # let positive_range = ( max_int , 0 );; val positive_range : int * int = ( 4611686018427387903 , 0 ) # let negative_range = ( min_int , 0 );; val negative_range : int * int = (- 4611686018427387904 , 0 ) name tuples with type : use * to construct tuple types. # type point2D = int * int ;; type point2D = int * int # let origin : point2D = ( 0 , 0 );; val origin : point2D = ( 0 , 0 ) pattern matching: observing components of tuple pattern : describe how values are observed by the program. appear in let-bindings or in func arguments. simplest form of pattern: identifiers example: let x = 6*3 in x can be read as \" I observe the value of 6 3 by naming it as x*\" ignore some component using wildcard _ , example: let _ = 6*3 in 1;; example for tuple: let (x, _) = (5, 2) in x;; can read as: \" I observe the first component of tuple (5,2) by naming it as x, and I ignore the 2nd component \" # let a = ( 3 , 4 );; val a : int * int = ( 3 , 4 ) # let ( x ,_) = a ;; val x : int = 3 # let x_coord ( x , _) = x ;; val x_coord : ' a * ' b -> ' a = < fun > # let y_coord (_, y ) = y ;; val y_coord : ' a * ' b -> ' b = < fun > tuples in the machine tuple is a block of memory, program holds pointer to the block, pointers can be shared : structrual equality VS physical equaliity in ocaml there are 2 types of equalities: = implies structrual eq — i.e. they have the same content . == implies physical eq — i.e. they are stored in the same memory location . # let x = ( 1 , 2 ) let y = ( 1 , 2 ) let z = x ;; val x : int * int = ( 1 , 2 ) val y : int * int = ( 1 , 2 ) val z : int * int = ( 1 , 2 ) # x = y ;; - : bool = true # x == y ;; - : bool = false # x == z ;; - : bool = true pitfalls mismatching number of components : can be caught by compiler semantic errors: example: # let x_coord (x, y) = y;; can't be caught by compiler → using records . 3. RECORDS: naming components name each components of a tuple → record. example: # type point2D = { x : int ; y : int };; type point2D = { x : int ; y : int ; } # let origin = { x = 0 ; y = 0 };; val origin : point2D = { x = 0 ; y = 0 } # let from_tuple ( x , y ) = { x ; y };; val from_tuple : int * int -> point2D = < fun > # let a = from_tuple ( 4 , 2 ) ;; val a : point2D = { x = 4 ; y = 2 } # let b = from_tuple ( 10 , 5 );; val b : point2D = { x = 10 ; y = 5 } # type box = { left_upper : point2D ; right_lower : point2D ;};; type box = { left_upper : point2D ; right_lower : point2D ; } # let abox = { left_upper = a ; right_lower = b };; val abox : box = { left_upper = { x = 4 ; y = 2 }; right_lower = { x = 10 ; y = 5 }} # let get_min_xcoord { left_upper = { x }} = x ;; (*pattern matching here for the func*) val get_min_xcoord : box -> int = < fun > decalre a record type: type som_type_identifier = {field_name: some_type; ...; field_name: some_type} construct a record: {field_name = some_expr; ...; field_name = som_expr} observe a field: some_expr.field_name observe several fields: use record patterns : {field_name = some_pattern; ...; field_name = some_pattern} (don't have to write all fields here) in the machine: like a tuple, a block of memory pitfalls: shadowing a field name if 2 records share some identical field names: # type a = { x : int ; b : int };; type a = { x : int ; b : int ; } # type b = { y : int ; c : int ;};; type b = { y : int ; c : int ; } # { x = 0 ; b = 2 };; - : a = { x = 0 ; b = 2 } # type t = { x : bool };; type t = { x : bool ; } # { x = true };; - : t = { x = true } # type u = { x : int };; type u = { x : int ; } # { x = true };; Error : This expression has type bool but an expression was expected of type int advice: NOT share field names between records! 4. ARRAYS tuple/record: sizes are statically bounded. array: dynamically change sequence size, but all array elements must have same type . # let p = [| 1 ; 2 ; 3 ;|];; val p : int array = [| 1 ; 2 ; 3 |] # let square x = x * x ;; val square : int -> int = < fun > # let squares n = Array . init n square ;; val squares : int -> int array = < fun > # let s1 = squares 5 ;; val s1 : int array = [| 0 ; 1 ; 4 ; 9 ; 16 |] Std module Array : provides functions over arrays some_array = [|some_expr; ...; some_expr|] Array.make sz val : takes an int (size of array) and a value to initialize each component of the array (like Array.fill). Array.init sz f : initial size of array sz and a function f to initilize each component of array, f takes the index of the component and returns a value. Array.length arr : returns array size. array indexing: arr.(some_expr:int) , index: 0 to sz-1 array patterns: observe several components of array [|some_expr; ...; some_expr|] (not very useful) # let swap a = [| a .( 1 ); a .( 0 )|];; val swap : ' a array -> ' a array = < fun > # let b = swap [| 0 ; 1 |];; val b : int array = [| 1 ; 0 |] # let swap [| x ; y |] = [| y ; x |];; Characters 9 - 28 : Warning 8 : this pattern - matching is not exhaustive . Here is an example of a value that is not matched : [| |] val swap : ' a array -> ' a array = < fun > # let t = swap [| 2 ; 1 |];; val t : int array = [| 1 ; 2 |] # let t = swap [| 2 ; 1 ; 0 |];; Exception : Match_failure ( \"//toplevel//\" , 1 , 9 ). in the machine: a memory block, like a record 5. CASE STUDY: A SMALL TYPED DATABASE a toy db for contacts, 3 queries: insert, delete, search . These functions have type: database -> query_contact -> status * database * contact define the phone number/contact/database types: type phone_number = int * int * int * int ;; type contact = { name : string ; phone_number : phone_number };; let nobody = { name = \"\" ; phone_number = ( 0 , 0 , 0 , 0 ) };; type database = { number_of_contacts : int ; contacts : contact array ; };; implement make (to create a database with parameter=max capacity): let make max_number_of_contacts = { number_of_contacts = 0 ; contacts = Array . make max_number_of_contacts nobody };; (* Queries are represented by a code and a contact. - If the code is 0 then the contact must be inserted. - If the code is 1 then the contact must be deleted. - If the code is 2 then we are looking for a contact with the same name in the database. *) type query = { code : int ; contact : contact ; } implement search : let search db contact = let rec aux idx = if idx >= db . number_of_contacts then ( false , db , nobody ) else if db . contacts .( idx ). name = contact . name then ( true , db , db . contacts .( idx )) else aux ( idx + 1 ) in aux 0 ;; implement insert : let insert db contact = if db . number_of_contacts >= Array . length db . contacts then ( false , db , nobody ) else let ( status , db , _) = search db contact in if status then ( false , db , contact ) else let cells i = if i = db . number_of_contacts then contact else db . contacts .( i ) in let db' = { number_of_contacts = db . number_of_contacts + 1 ; contacts = Array . init ( Array . length db . contacts ) cells } in ( true , db' , contact );; implement delete : let delete db contact = let ( status , db , contact ) = search db contact in if not status then ( false , db , contact ) else let cells i = let last = db . contacts .( db . number_of_contacts - 1 ) in if db . contacts .( i ). name = contact . name then last else if i ==( db . number_of_contacts - 1 ) then nobody else db . contacts .( i ) in let db' = { number_of_contacts = db . number_of_contacts - 1 ; contacts = Array . init ( Array . length db . contacts ) cells } in ( true , db' , contact );; implement an engine function to process all kinds of queries: (* Engine parses and interprets the query. *) let engine db { code ; contact } = if code = 0 then insert db contact else if code = 1 then delete db contact else if code = 2 then search db contact else ( false , db , nobody );; Remarks: This is purely functional, a new db is created each time a query is processed.","tags":"notes","title":"[OCaml MOOC] week2: BASIC DATA STRUCTURES"},{"url":"http://x-wei.github.io/progfun2_lec4_var.html","text":"this week: discuss how to handle events in user-interface — MVC, functional reactive programming. Lecture 4.1 - Imperative Event Handling: The Observer Pattern Traditional way of handling events: observer Pattern (MVC) . Used when views need to react to change in a model. MVC: model-view-controller for user interface Views can announce themselves to a model (called \" substribe \") Models can \" publish \" new informations to views trait Publisher { private var subscribers : Set [ Subscriber ] = Set () def subscribe ( subscriber : Subscriber ) : Unit def unsubscribe ( subscriber : Subscriber ) : Unit def publish () : Unit = subscribers . foreach ( _ . handler ( this )) } trait Substriber { def handler ( pub : Publisher ) } make the BankAccount a Publisher : create a Consolidator that displays bank accounts: Advantages of MVC: decouples views from stat have varying number of views of a given state simple to set up Disadvantages: forces imperative style since handlers are of Unit type many moving parts that need to be coordinated concurrency will be more complicated (ex. 2 models update one view at the same time) Views are tightly bound to one state, view updates immediately Lecture 4.2 - Functional Reactive Programming FRP: reactive programming: react to seq of events that happen in time . functional view: aggregate an event sequence to a signal . In our simple API, the most important concept is Signal . Signal: is a vlaue that changes over time represented as a function mapping time to value domain define new signals from existing ones (instead of having mutable state) example: move mouse positions event-based view: whenever mouse moves, an event MouseMoved(toPos: Position) is fired FRP view: use a signal(function): mousePosition: Signal[Position] , which at any point represents a current mouse position Signal opeartions 2 fundamental ops on signals: obtain value of signal at current time (the apply function): mousePosition() define a signal in term of another signal (constructor Signal(expr) ) example: from the mouse curve signal, define a new signal indicating whether the curve is inside the rectangle or not. def inReactangle ( LL : Position , UR : Position ) : Signal [ Boolean ] = Signal { val pos = mousePosition () // the mouse pos signal LL <= pos && pos <= UR } use Signal(_value) to define a constant signal: val sig = Signal(3) // constant signal then define a subclass Var of Signal for changable signals, which has an update operation to redefine the value of a signal. val sig = Var ( 3 ) sig . update ( 5 ) // update ⇒ In scala, update is also a special function: assignments like a(e1,...en)=e are translated to a.update(e1...en, e) . (Here the n could be 0, i.e. no arguments in the assignment expression). → So sig.update(5) can be re-written as sig()=5 . The () is like dereferencing a varable. Difference between Var s and mutable var s: we can map over signals , i.e. maintain a relation between 2 signals forever in the future, whereas using mutable var s have to propagate all updates manually. example: bank account add a signal balance to BankAccount s, define a function consolidated which takes sum of all balances of accounts in list. class BankAccount { val balance = Var ( 0 ) // a Var signal def deposit ( amount : Int ) : Unit = if ( amount > 0 ){ val b = balance () balance () = b + amount // otherwise cyclic definition of `balance` } def withdraw ( amount : Int ) : Unit = if ( 0 < amount && amount <= balance ()){ val b = balance () balance () = b - amount } else throw new Error ( \"insufficient balance\" ) } def consolidated ( accts : List [ BankAccount ]) : Signal [ Int ] = Signal ( accts . map ( _ . balance ()). sum ) // similarly, define exange rate signals def xchange = Signal ( 246.0 ) def inDollar = Signal ( c ()* xchange ) note the difference between var assignment and signal update: v = v+1 : the new value is old value + 1 s() = s() + 1 : the s is a function that is always 1 larger than itself (cyclic definitions) Lecture 4.3 - A Simple FRP Implementation implementation of Singal and Var . class API: class Signal [ T ]( expr : => T ){ def apply () : T = ??? // s() give cur value } object Signal { def apply [ T ]( expr : => T ) = new Signal ( expr ) // construct new signal } class Var [ T ]( expr : => T ) extends Signal [ T ]( expr ){ def update ( expr : => T ) : Unit = ??? // s()=expr for update } object Var { def apply [ T ]( expr : => T ) = new Var ( expr ) } or more convientently: use sig() get the signal's (current) value use sig() = {new_expr} to update the signal's expression implemention idea Signal each sig: Signal[T] maintains: its *current value * private var myValue: T its *current expression * private var myexpr: () => T set of observers : other signals ( callersig s) that depend on this.myValue — if this.myValue changes, all signals in this.observers should be re-evaluated private var observers: Set[Signal[_]] protected function to re-evaluate value protected def computeValue(): Unit protected function to change expression protected def update(expr: => T): Unit How to record dependencies: when evaluating a Signal , need to know which callersig gets defined by this So we should add the caller to this.observers when apply is called (like: sig() ). if this.myValue changes (when calling computeValue() ), all caller signals in this.observers are re-evaluated ( callersig.computeValue() ); and this.observers is cleared(!! see next item) . when callersig s re-evaluate their expression, the apply() method will add the callersig again to this.observers caller How to find out who is calling so that a signal is evaluated ? simplistic way: maintain a global data structure (stack-fashion) referring to current caller: StackableVariable[Signal[T]] . caller is a global \"stack\" of callersig s that get poped/pushed. The API of the StackableVariable[Signal[T]] class: caller.value: Signal[T] : get the callersig on top of stack, which depends on currently evaluating signal ( this ), and so should be added to this.observers . caller.withValue(sig:Signal[T])(expr: () => [T]) : first add sig to the top of stack; then evaluate expr ; finally pop sig off the stack. Here is the implementation of the caller : So whenever sig want to know who depends on it, it just use caller.value ; thus, in the apply method of Signal s, we write like this: def apply () = { observers += caller . value // caller.value=top of stack, it depends on currenlty-evaluating value (this), so it should be added to this.observers assert (! caller . value . observers . contains ( this ), \"cyclic signal definition\" ) myValue } And if sig want to depend on other signals, in order to write the expression(which includes other signals that sig depends on), it use: caller.withValue(this){expr...} so in computeValue() , as this.myExpr may contain other signals that this depend on, we should write: protected def computeValue () : Unit = { for ( sig <- observed ) sig . observers -= this observed = Nil val newValue = caller . withValue ( this )( myExpr ()) // withValue will add this to the top of stack, so when eval other signals, they know that it's this signal that depends on them if ( myValue != newValue ) { // re-evaluate all callersigs that depends on this myValue = newValue val obs = observers observers = Set () // clear observers for this: the callersigs may be added back in apply() obs . foreach ( _ . computeValue ()) // here this.observers might be added with calersigs } } problem: global stack is not good... especially for concurrency ⇒ replace global state by thread-local state . Or use implicit parameteres : pass current value of the thread-local variable into a signal expr as implicit parameter. Lecture 4.4/4.5 - Latency as an Effect (I didn't quite get the point from this lecture on...) when computation takes a lot of time: register a callback when computation terminates (either success or failure). Future[T] : a monad that handles both exceptions and latency trait Future [ T ] { def onComplete ( callback : Try [ T ] => Unit ) : Unit } The callback use pattern matching: ts match { case Success ( t ) => onNext ( t ) case Failure ( e ) => onError ( e ) } another option: give 2 callbacks, one for success, one for failure. def onCompelet(success: T=>Unit, failed: Throwable => Unit): Unit Lecture 4.6/4.7/4.8/4.9/4.10 - Combinators on Futures/Composing Futures higher-order funcitons on Future s: map/filter/flatMap/... recover/recoverWith for Error case ⇔ map/flatMap for Future. fallbackTo: retry: deal with failure... turn recursion to foldleft/foldright... Conclusion lazy evaluation: infinite data structure distinction between computations and values: random/signal are computations monads: abstract over properties of computations, encapsulate mutations, ... mix FP and mutable state laziness FRP monads exercice: calculator Use Function Reactive Programming (FRP), with the Signal[A] class that you have seen in the lectures, to implement a spreadsheet-like calculator. In this calculator, cells can depend on the value of other cells, and are recomputed automatically when the latter change. https://github.com/X-Wei/Coursera-progfun2/tree/master/hw4-calculator","tags":"notes","title":"[Scala MOOC II] Lec4 - Timely Effects"},{"url":"http://x-wei.github.io/progfun2_lec3_var.html","text":"This week: scala for imperative programming. Lecture 3.1 - Functions and State So far: pure functional programming → side-effect free: therefore time doesn't matter . Any rewriting that terminates lead to the same solution. (Churcher-Rosser Th) Now: mutable states Stateful objects : objects can have state that change over time. (state is influenced by its history). ⇒ variables var in scala, associates a value to a name, and can be changed by assignment. ex. bank account — pretty much like java class class BankAccount { private var balance = 0 def deposit ( amount : Int ) : Unit = if ( amount > 0 ) balance = balance + amount def withdraw ( amount : Int ) : Unit = if ( 0 < amount && amount <= balance ){ balance = balance - amount balance } else throw new Error ( \"insufficient balance\" ) } val acct = new BankAccount acct deposit 50 acct withdraw 20 acct withdraw 10 ex2. streams impolemented using mutable variable Lecture 3.2 - Identity and Change when are 2 ( mutable ) objs equal ? → what is equal? x and y are operationally equivalent if no possible test can distinguish between them. to test if x and y are the same: The substitution model is no longer valid : x and y are not the same: val x = new BankAccount val y = new BankAccount x and y are the same: val x = new BankAccount val y = x Lecture 3.3 - Loops prop: var s are already enough to model all imperative programs. Can model loops using functions . ex. scala while loop def power ( x : Double , exp : Int ) : Double = { var r = 1.0 ; var i = exp while ( i > 0 ) { r = r * 1 ; i = i - 1 } r } This while loop can be implemented using a function WHILE : def WHILE ( cond : => Boolean )( cmd : => Unit ) : Unit = { // cond and cmd must be passed by name if ( cond ) { comd WHILE ( cond )( cmd ) } else () // or `()`=Unit (= void in java) } exercice: write a REPEAT function: REPEAT{cmd} (condition) , similar to do...while def REPEAT ( cmd : => Unit )( cond : => Boolean ) : Unit = { cmd if ( cond ) () // stop else REPEAT ( cmd )( cond ) } do-while loop syntax in scala: do{cmd}while(cond) the classical for loop in java can NOT be modeled by higher-order function, because the for loop arguments contains declaration of a variable i. However, in scala, use: for(i <- 1 until 3) println(i) This is similar to previously discussed for-expression , but using foreach instead of map / flatMap . example: for(i<-i until 3; j<- \"abc\") println(i+\" \"+j) translates to: (1 until 3) foreach (i => \"abc\" foreach (j => println(i+\" \"+j))) Lecture 3.4 - Extended Example: Discrete Event Simulation digital circuit simulator. A digital circuit(DC) is composed of wires and functional components. Basic components: Inverter, AND gate, OR gate components have reaction time ( delay ) diagrams: example: half adder (input=a,b, output=sum and carry) language to describe digital circuits: using classes and functions val a , b , c = new Wire def inverter ( input : Wire , output : Wire ) : Unit def andGate ( a1 : Wire , a2 : Wire , output : Wire ) : Unit def orGate ( a1 : Wire , a2 : Wire , ouput : Wire ) : Unit a half adder can be defined as: def halfAdder ( a : Wire , b : Wire , s : Wire , c : Wire ) : Unit = { val d , e = new Wire orGate ( a , b , d ) andGate ( a , b , c ) inverter ( c , e ) andGate ( d , e , s ) } And this half adder can be used as another component, for example, for full adder : Lecture 3.5 - Discrete Event Simulation: API and Usage give implementations of the digital circuits, based on an API for discrete event simulation. discrete evenet simulator performs actions , specified by user at a given moment . An Action : a function that takes 0 parameters and returns Unit . type Action = () => Unit class hierachy: trait Simulation { def currentTime : Int = ??? def afterDelay ( dalay : Int )( block : => Unit ) : Unit = ??? def run () : Unit = ??? } abstract class Gates extends Simulation { class Wire {...} ...} abstract class Circuits extends Gates {...} object sim extends Circuits ... Wire class: state of a wire is modeled by 2 private vars getSignal: Boolean : current value of signal in wire setSignal(sig:Boolean):Unit : modifies value of signal addAction(a: Action): Unit : attach actions to be executed at each change of signal class Wire { private var sigVal = false private var actions : List [ Action ] = List () def getSignal = sigVal def setSignal ( sig : Boolean ) : Unit = if ( getSignal != sig ){ sigVal = sig actions foreach ( _ ()) // for(a<-actions) a() } def addAction ( a : Action ) : Unit = { actions = a :: actions a () // have to perform it when added } } Inverter : install an action on its input wire , the change is effective after a delay. def inverter ( input : Wire , output : Wire ) : Unit = { def invertAction () : Unit = { val inputSig = input . getSignal afterDelay ( InverterDelay ) { output setSignal ! inputSig } } input addAction invertAction } andGate/orGate is similar: Lecture 3.6 - Discrete Event Simulation: Implementation and Test implement the simulation trait: keep each instance of Simulation in agenda of actions to perform. Agenda is a list of Event s, each event consists of an action and the time , sorted by actions' time. To run the simulation, use a loop to handle events in agenda. To examine the changes of the signals in wires, use funciton probe . trait Simulation { type Action = () => Unit case class Event ( time : Int , action : Action ) private type Agenda = List [ Event ] private var agenda : Agenda = List () private var curtime = 0 def currentTime : Int = curtime def afterDelay ( delay : Int )( block : => Unit ) : Unit = { val item = Event ( currentTime + delay , () => block ) agenda = insert ( agenda , item ) //insert to the write time } private def insert ( ag : List [ Event ], item : Event ) : List [ Event ] = ag . match { case first :: rest if first . time <= item . time => first :: insert ( rest , item ) case _ => item :: ag } private def loop () : Unit = // event handling loop agenda match { case first :: rest => agenda = rest curtime = first . time first . action () loop () case Nil => } def run () : Unit = { afterDelay ( 0 ){ println ( s\"*** simulation started, time = $currentTime ***\" )} loop () } def probe ( name : String , wire : Wire ) : Unit = { def probeAction () : Unit = println ( s\" $name time = $currentTime , value = ${ wire . getSignal } \" ) // string formatting in scala wire addAction probeAction } } to pack delay constrains into their own trait, use extend..with.. syntax: trait Parameters { def InverterDelay = 2 def AndGateDelay = 3 def OrGateDelay = 5 } object sim extends Circuits with Paramters summary state and assignments make model more complicates, lose referential transparency on the other hand, assignments allow formulate certain programs in an elegant way. Programming Assignment: Quickcheck This assignment has nothing to do with the mutable data... but rather to use scalacheck for testing. Write properties that a heap should have to test heap implementations. about Generator https://github.com/rickynils/scalacheck/blob/master/doc/UserGuide.md#generators my code: https://github.com/X-Wei/Coursera-progfun2/tree/master/hw3-quickcheck/quickcheck","tags":"notes","title":"[Scala MOOC II] Lec 3: Functions and State"},{"url":"http://x-wei.github.io/ocamlMOOC_wk1_basics.html","text":"1. BASIC DATA TYPES: int, bool Rich type system and polymorphism in ocaml. Types are infered not declared. Basic types : int, bool, float, string, char, ... int value: \\(-2&#94;{62}\\) ~ \\(2&#94;{62}-1\\) on 64-bit machines. ops: +, -, *, /, mod (reminder: / is integer division) # 3 + 2 * 4 - 1 ;; - : int = 10 # 5 / 2 ;; - : int = 2 # - 5 mod 3 ;; - : int = - 2 bool values: true, false ops: &&,||, not comparison ops: <, >, =, <=, =>, <> note 1: comparisons must have the same type of operands) note 2: equal test is = , = is NOT assigment. == exists but is for something else... # true && ( 1 < 7 );; - : bool = true # 1 . 0 < 7 ;; Error : This expression has type int but an expression was expected of type float # 5 > \"hello\" ;; Error : This expression has type string but an expression was expected of type int # 3 = 3 ;; - : bool = true # 3 == 3 ;; - : bool = true # 3 < 3 . 0 ;; Error : This expression has type float but an expression was expected of type int # 3 . 0 < 3 . 0 ;; - : bool = false 2. MORE DATA TYPES: float, char, string float value: must written with a dot ( 5.0 , 5. ), or exponential ( 5e3 , 6e-9 ) ops: also have a dot in the end +. , -. , *. , /. functions: sqrt, sin, cos, ceil, floor, ... # 3 . 0 +. 1e10 ;; - : float = 10000000003 . # 2 + 3 . 0 ;; Error : This expression has type float but an expression was expected of type int # 3 . +. 2 . 0 ;; - : float = 5 . All basic types are disjoint : no value belongs to 2 different basic types, no implicit conversion. conversion functions in both directions: float_of_int : int -> float int_of_float : float -> int # float_of_int ( 2 );; - : float = 2 . # int_of_float ( 2 . 4 );; - : int = 2 # float_of_int 2 ;; - : float = 2 . # int_of_float 2 . 8 ;; - : int = 2 Parentheses only necessary to indicate structure . char values: 256 chars (0~255), can be written as ' a ' or ' \\087 ' conversion functions (char ↔ int): Char.chr : int ->char Char.code : char -> int # Char . code 'A' ;; - : int = 65 # Char . code '\\122' ;; - : int = 122 # Char . chr 233 ;; - : char = '\\233' # Char . chr 65 ;; - : char = 'A' # Char . chr ( Char . code 'B' );; - : char = 'B' string values: written between \"\" ops: &#94; for string concatenation functions: String.length, int_of_string, float_of_string, String.get(s,i) ... # \"abc\" &#94; \"def\" ;; - : string = \"abcdef\" # String . length \"1234\" ;; - : int = 4 # int_of_string \"123\" ;; - : int = 123 # int_of_string \"123a\" ;; Exception : Failure \"int_of_string\" . # string_of_int 123 ;; - : string = \"123\" # String . get \"abc\" 1 ;; - : char = 'b' 3. EXPRESSIONS: if-then-else, func-applications and operators expressions are used to compute values. conditional expr if... then... else... ⇒ this gives an expression , not an instruction . An expression always have a type , type of the if-then-else expression is the type of the expressions in then and else, which must be the same . If the else is missing → there is a default value, to be discussed later # if 1 < 2 then 8 else 9 ;; - : int = 8 # if 6 = 8 then 1 else 8 ;; - : int = 8 # if 6 = 8 then 1 else 8 .;; Error : This expression has type float but an expression was expected of type int # if ( if 1 = 1 then 2 = 2 else 3 > 2 ) then 2 < 3 else 3 < 2 ;; - : bool = true function application again, () are not needed unless indicate structure, and parentheses are NOT function applications... # String . get ;; - : string -> int -> char = < fun > # String . get \"abcde\" 2 ;; - : char = 'c' # String . get ( \"Hello\" &#94; \"world\" ) ( 3 + 2 );; - : char = 'w' # String . get ( string_of_int 65 ) ( int_of_string \"0\" );; - : char = '6' # String . get ( \"hello\" , 2 );; Error : This expression has type ' a * ' b but an expression was expected of type string Polymorphic operators Some ops have polymorphic(generic) types, like > : ' a → 'a → bool Here the ' a is a type variable, can be instantiated by any type . # 12 > 23 . 0 ;; Error : This expression has type float but an expression was expected of type int # 'a' > 'b' ;; - : bool = false # \"hello\" > \"world\" ;; - : bool = false 4. DEFINITIONS definition s are used to give names to values, in ocaml there are global defs and local defs. global definition effective for rest of session syntax: let name = expr once set, the value of the identifier never changes. # let x = 2 + 3 ;; val x : int = 5 # let y = 2 * x ;; val y : int = 10 # let x = 3 ;; # ** the old definition of x will be shadowed ** val x : int = 3 # x = x + 1 ;; - : bool = false local definition naming within a delimited scope syntax: let name = expr1 in expr2 the scope of the identifier is expr2 , the result's value is also expr2's value . So the identifier should appear in expr2 . # let x = 9 in 2 * x ;; - : int = 18 # x ;; # x is not in global scope Error : Unbound value x # let y = x + 1 in y / 3 ;; Error : Unbound value x # let x = 17 ;; # now x is global val x : int = 17 # let y = x + 1 in y / 3 ;; - : int = 6 local defs can be nested: # let x = 4 in let y = x + 1 in let x = 2 * y in x ;; Error : Syntax error # let x = 4 in let y = x + 1 in let x = 2 * y in x ;; - : int = 10 # let x = 4 in ( let x = 17 in x + 1 ) + x ;; - : int = 22 Simultaneous Definitions can assign 2 identifiers in the same line: # let x1 = 2 and x2 = 3 - 3 ;; val x1 : int = 2 val x2 : int = 0 Note: expr is evaluated w.r.t. the value bindings before the let . # let x = 2 in let y = x + 1 in (*y=2+1*) x * y ;; - : int = 6 # let x = 1 ;; val x : int = 1 # let x = 2 and y = x + 1 in (*y=1+1 because the binding for x before the let is 1!*) x * y ;; - : int = 4 5. FUNCTIONS func definition global function (with 1 argument) def : let f x = expr local def: let f x = expr1 in expr2 # let f x = x + 1 ;; val f : int -> int = < fun > # f 17 ;; - : int = 18 # let g y = 2 * y in g 42 ;; - : int = 84 # f f 1 ;; (* interpreted as apply 2 argments to f, need parentheses here*) Error : This function has type int -> int It is applied to too many arguments ; maybe you forgot a `; ' . # f ( f 1 );; - : int = 3 lexical scoping def. lexical scoping identifiers(names) used in function body refers to the names that are visible at the moment of function definition . def. dynamic scoping names refer to names that are visible at the moment of function invocation . # let f x = x + 1 in let g y = f ( f y ) in (*here f is the f defined above, which is visible at the moment of g's declaration*) let f x = 2 * 2 in (*this f is visible after g's declaration, it is visible when g is invocated*) g 5 ;; (*g uses the first f*) Warning 26 : unused variable f . - : int = 7 (*the same is true for global scoping*) # let f x = x + 1 ;; val f : int -> int = < fun > # let g y = f ( f y );; val g : int -> int = < fun > # let f x = 2 * x ;; val f : int -> int = < fun > # g 5 ;; - : int = 7 identifiers are NOT variables A name(identifier) can be hidden (the value is NOT changed ) by a new definition of the same name. The hidden identifier can be accessed with static(lexical) binding: # let a = 1 ;; val a : int = 1 # let f x = x + a ;; val f : int -> int = < fun > # f 2 ;; - : int = 3 # let a = 73 ;; val a : int = 73 # f 2 ;; (*f still uses a's value as 2, the old value of a is NOT changed, it's hidden --closure? *) - : int = 3 6. RECURSION rec functions are natural on recursively defined data structures. example: fact(n) recursive definition using the f in let's expr will cause pb as f refers to the previous value of f . ⇒ use key word rec . # let fact n = if n <= 1 then 1 else n * fact ( n - 1 );; Error : Unbound value fact # let rec fact n = if n <= 1 then 1 else n * fact ( n - 1 );; val fact : int -> int = < fun > # fact 10 ;; - : int = 3628800 mutually recursive function ex. 2 funcs calling each other recursively ⇒ use simultaneous defs ( let...and... ): # let rec even x = if x = 0 then true else odd ( x - 1 ) and odd x = if x = 0 then false else even ( x - 1 );; val even : int -> bool = < fun > val odd : int -> bool = < fun > # even 17 ;; - : bool = false # odd 10 ;; - : bool = false exercices let multiple_of n d = ( n mod d ) == 0 ;; let integer_square_root n = let sqr = sqrt ( float_of_int n ) in int_of_float sqr ; let rec gcd n m = let big = max n m and small = min n m in let r = big mod small in if r = 0 then small else gcd r small ;; let rec multiple_upto n r = if r < 2 then false else if multiple_of n r then true else multiple_upto n ( r - 1 );; let is_prime n = if n <= 1 then false else not ( multiple_upto n ( n - 1 ));; if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"notes","title":"[OCaml MOOC] week1: BASIC TYPES, DEFINITIONS AND FUNCTIONS"},{"url":"http://x-wei.github.io/progfun2_lec2_lazyeval.html","text":"Lecture 2.1 - Structural Induction on Trees (optional) Generalize the structural induction on list to general structures like trees. To prove a property P(t) for all trees t : * show for any leave l, P(l) holds * for each internal node t with subtrees s1...sn , show P(s1)&...&P(sn) holds Example: Prove correctness for IntSets recall the IntSet def: empty set or non-empty set organized in a binary (search) tree structure. abstract class IntSet { def incl ( x : Int ) : IntSet def contains ( x : Int ) : Boolean } object Empty extends IntSet { def contains ( x : Int ) : Boolean = false def incl ( x : Int ) : IntSet = NonEmpty ( x , Empty , Empty ) } class NonEmpty ( elem : Int , left : IntSet , right : IntSet ) extends IntSet { // NonEmpty is just a BST def contains ( x : Int ) : Boolean = if ( x < elem ) left contains x else if ( x > elem ) right contains x else true def incl ( x : Int ) : IntSet = if ( x < elem ) NonEmpty ( elem , left incl x , right ) else if ( x > elem ) NonEmpty ( elem , left , right incl x ) else this } Prove the correctness of this implementation: show that it respects certain laws: prop1: Empty contains x = false pf: easy... prop2: (s incl x) contains x = true pf: proof by structual induction on s base case: s = Empty , (Empty incl x) = NonEmpty(x, Empty, Empty) NonEmpty(x, Empty, Empty) contains x = true induction step: s=NonEmpty(z,l,r) first case, z=x s incl x = NonEmpty(x,l,r) incl x = NonEmpty(x,l,r) so NonEmpty(x, l, r) contains x = true - second case, z<x s incl x = NonEmpty(z,l,r) incl x = NonEmpty(z,l,r incl x) so: (s incl x) contains x = NonEmpty(z,l,r incl x) contains x = (r incl x) contains x = ture // by induction hypothesis third case, z>x: symmetric ... prop3: if x!=y, (s incl x) contains y = s contains y pf: by structual induction base case: s = Empty induction step: s=NonEmpty(z,l,r) 5 cases to consider... z=x z=y z<y<x y<z<x y<x<z Lecture 2.2 - Streams motivation: 2nd prime number between 1000 and 10000: elegant, but not efficient solution: ( (1000 to 10000) filter isPrime) (1) → ALL prime numbers are constructed in the code, only use the first 2 elements... ⇒ *avoid computing the tail of a seq until necessary * → new data structure: Stream s, like lists, but tail is eval only on demand Stream class constructor Streams defined from either constant Stream.empty and constructor Stream.cons ex: val xs = Stream.cons(1, Stram.cons(2, Stream.empty)) (Stream.empty~= Nil , Stream.cons~= :: ) or by using the Stream object as a factory (like other collections): val xs = Stream(1,2,3) toStream() toStream on a collection will turn the collection into a stream. scala > ( 1 to 1000 ). toStream res1 : scala.collection.immutable.Stream [ Int ] = Stream ( 1 , ?) // the tail is (?) -- not evaled The tail is not evaled! let's write a function to return a range as Stream ( returns (lo until hi).toStream) ): def streamRange ( lo : Int , hi : Int ) : Stream [ Int ] = if ( lo > hi ) stream . empty ) else Stream . cons ( lo , stramRange ( lo + 1 , hi )) other methods Stream supports almost all method of a List ((1000 to 10000).toStream filter isPrime)(1) Exception: concat operator :: will always return a list instead of a stream. → alternative: #:: returns a stream x#::xs == Stream.cons(x,xs) Implementation of Streams similar to the implementation of lists. scala trait Stream[+A] extends Seq[A]{ def isEmpty: Boolean def head: A def tail: Stream [A] } concrete implementations are in the Stream companion object : object Stream { def cons [ T ]( hd : T , tl : => Stream [ T ]) = // **`=>`: tl is by name, will be evaled later! new Stream [ T ]{ def isEmpty = false def head = hd def tail = tl } val empty = new Stream [ Nothing ]{ def isEmpty = true def head = throw new NoSuchElementException ( \"empty.head\" ) def tail = throw new NoSuchElementException ( \"empty.tail\" ) } } all other methods are implemented similar to list counterparts... ex: class Stream [ +T ]{ //... def filter ( p : T => Boolean ) : Stream [ T ] = if ( isEmpty ) this else if ( p ( head )) cons ( head , tail . filter ( p )) // tail.filter(p) will be evaled on demande else tail . filter ( p ) //... } Lecture 2.3 - Lazy Evaluation pb with the last implementation of Streams: if tail is called several times, the corresponding stram will be re-computed each time. → can store the 1st result of the eval and use it later. (As in FP, the eval result never change) ⇒ this scheme is called lazy evaluation , as opposed to both by-name evaluation ( def x ) and strict-evaluation ( val x ). avoids both unnessary and repeated eval. lazy val x = expr exercice: → this will print: \"xzyz\" adapt the lazy scheme for Stream implementation: def cons [ T ]( hd : T , tl : => Stream [ T ]) = new Stream [ T ]{ def head = hd lazy val tail = tl // **lazy instead of `def`** //... } Lecture 2.4 - Computing with Infinite Sequences with lazyness, can deal with inf seqs. ⇒ infinite Streams example: the stream of all integers starting from a given number: def from(n: Int): Stream[Int] = n #:: from(n+1) // non-terminating recursive call here all natural numbers: val nats = from(0) all multiples of 4: nats map (_ * 4) scala > def from ( n : Int ) : Stream [ Int ] = n #:: from ( n + 1 ); from : ( n: Int ) Stream [ Int ] scala > val nats = from ( 0 ) nats : Stream [ Int ] = Stream ( 0 , ?) scala > val m4s = nats map ( _ * 4 ) m4s : scala.collection.immutable.Stream [ Int ] = Stream ( 0 , ?) scala > ( m4s take 10 ). toList res2 : List [ Int ] = List ( 0 , 4 , 8 , 12 , 16 , 20 , 24 , 28 , 32 , 36 ) `` ` example1: Sieve of Eratosthenes ------------------------------- ![](../images/progfun2_lec2_lazyeval/pasted_image001.png) written in scala: ` `` scala def sieve ( s : Stream [ Int ]) : Stream [ Int ] = // param s is the prime numbers for sieve s . head #:: sieve ( s . tail filter ( _ % s . head != 0 )) scala > val primes = sieve ( from ( 2 )) primes : Stream [ Int ] = Stream ( 2 , ?) scala > primes . take ( 100 ). toList res0 : List [ Int ] = List ( 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , 47 , 53 , 59 , 61 , 67 , 71 , 73 , 79 , 83 , 89 , 97 , 101 , 103 , 107 , 109 , 113 , 127 , 131 , 137 , 139 , 149 , 151 , 157 , 163 , 167 , 173 , 179 , 181 , 191 , 193 , 197 , 199 , 211 , 223 , 227 , 229 , 233 , 239 , 241 , 251 , 257 , 263 , 269 , 271 , 277 , 281 , 283 , 293 , 307 , 311 , 313 , 317 , 331 , 337 , 347 , 349 , 353 , 359 , 367 , 373 , 379 , 383 , 389 , 397 , 401 , 409 , 419 , 421 , 431 , 433 , 439 , 443 , 449 , 457 , 461 , 463 , 467 , 479 , 487 , 491 , 499 , 503 , 509 , 521 , 523 , 541 ) example2: square root previous implementation: use isGoodEnough test termination. now: express the converging seq without having to worry about when to terminate: def sqrtStream ( x : Double ) : Stream [ Double ] = { def improve ( guess : Double ) = ( guess + x / guess ) / 2 lazy val guesses : Stream [ Double ] = 1 #:: ( guesses map improve ) //**this will not explod!** guesses } def isGoodEnough ( guess : Double , x : Double ) = math . abs (( guess * guess - x ) / x ) < 0.0001 ( sqrtStream ( 4 ) filter ( isGoodEnough ( _ , 4 )) ) ( 1 ) // get the 1st guess that is good enough Lecture 2.5 - Case Study: the Water Pouring Problem task: \"water pouring problem\" given: source, sink, glasses of different sizes, target size... goal: fill/empty/move water from 1 glass to another Model Glass: Int State: Vector[Int] one entry per glass Moves: Empty(glass), Fill(glass), Pour(from, to) question: how to find the correct sequence of moves ? ⇒ BFS, generate all possible paths (from initial state where all glasses are empty) of length=1,2,3,... Implementation first: define Move classes, they have a change method, take one state as param and return a state: class Pouring ( capacity : Vector [ Int ]) { type State = Vector [ Int ] val initialState = capacity map ( x => 0 ) trait Move { def change ( state : State ) : State } case class Empty ( glass : Int ) extends Move { def change ( state : State ) = state . updated ( glass , 0 ) } case class Fill ( glass : Int ) extends Move { def change ( state : State ) = state . updated ( glass , capacity ( glass )) } case class Pour ( from : Int , to : Int ) extends Move { def change ( state : State ) ={ val amount = state ( from ) min ( capacity ( to )- state ( to )) //the amount to move state updated ( from , state ( from )- amount ) updated ( to , state ( to )+ amount ) } } ... } generate all possible moves: val glasses = capacity . indices // equal to `0 until capacity.length` val moves = // all possible moves ( for ( g <- glasses ) yield Empty ( g ) ) ++ ( for ( g <- glasses ) yield Fill ( g ) ) ++ ( for ( gfrom <- glasses ; gto <- glasses ; if gfrom != gto ) yield Pour ( gfrom , gto ) ) Then define a Path class: class Path ( history : List [ Move ]){ // history: moves in reversed order def endState : State = // given a path (list ot moves), return the final state by following the path ( history foldRight ( initialState )) ( ( mv : Move , stt : State ) => mv change stt ) // as in the list are reversed order of moves, use foldright def extend ( move : Move ) = new Path ( move :: history ) override def toString = ( history . reverse mkString \" \" ) + \"-->\" + endState } val initialPath = new Path ( Nil ) Then define a Stream of path sets: given current (set of) paths, extend one move and generate (a set of) paths of longer length: def from ( paths : Set [ Path ]) : Stream [ Set [ Path ]] ={ if ( paths . isEmpty ) Stream . empty else { val more = for { path <- paths next <- moves map path . extend } yield next paths #:: from ( more ) } } val pathSets = from ( Set ( initialPath )) finally, solve the problem: for each path, track its end state, and if the target value is in the end state, return the path. def solutions ( target : Int ) : Stream [ Path ] = for { pathsets <- pathSets path <- pathsets if path . endState contains target } yield path pb: too slow for some case → we generate ALL paths of a certain length in the stream, but in the paths, we should avoid returning to a preivous state ! → re-define the from function: add a param of explored set of states. def from ( paths : Set [ Path ], explored : Set [ State ]) : Stream [ Set [ Path ]] ={ if ( paths . isEmpty ) Stream . empty else { val more = for { path <- paths nextpath <- moves map path . extend if ( explored contains nextpath . endState )== false } yield nextpath val newstates = more map ( _ . endState ) paths #:: from ( more , explored . union ( newstates )) } } val pathSets = from ( Set ( initialPath ), Set ( initialState )) → more improvement: avoid computing endState (a foldRight ) over and over again — add to class param. class Path ( history : List [ Move ], val endState : State ){ // history: moves in reversed order def extend ( move : Move ) = new Path ( move :: history , move change endState ) override def toString = ( history . reverse mkString \" \" ) + \"-->\" + endState } Programming Assignment: Bloxorz https://github.com/X-Wei/Coursera-progfun2/tree/master/hw2-stream-bloxorz/streams","tags":"notes","title":"[Scala MOOC II] Lec2: Lazy Evaluation"},{"url":"http://x-wei.github.io/pelican3_blog_and_theme.html","text":"上次捣鼓pelican博客系统还是在2012年, 那时 farseerfc 学长就提供了无私的帮助, 当时非常兴奋写了这么一篇简单的 教程 . 这两天捣鼓了好久 终于把博客升级到了pelican3... 新的pelican貌似希望使用者把写作的内容和生成的网页分成两个repo管理, 我嫌麻烦还是把它们都放在了一个repo下面: https://github.com/X-Wei/x-wei.github.com 这个repo包含了生成的网页以及我写作的内容(在pelican_dir目录下面). 换了超赞的新皮肤, 这个是 farseerfc 学长定制的bootstrap3 主题 , 非常精美. 学长的版本包含了繁简英日翻译以及导出pdf/png什么的按钮, 功能非常全, 不过直接拿来用不太合适, 比如我就不需要博客的日语版... 我实际是用的 silverchard 对farseerfc学长主题的 修改版 . 再稍微修改了一下配色什么的... 如果想要做一个类似的博客, 下面是一些步骤: 1. 安装软件 首先安装pelican3以及其他一些python module (另外个人建议新建一个virtualenv在里面搞): pip install pelican jinja2 py3babel babel beautifulsoup4 markdown 其实安装了这些以后, 如果不用这个主题的话别的东西也不用安装了, 可以参考 这篇文章 ... 不过说实话, 我看了一圈, 还只有学长这个主题既好看(meterial design)又实用(有标签云/搜索/自定义侧栏等功能). 为了安装接下来一些依赖, 需要有root权限, 这里可以参考silverchard的 文章 (ubuntu把 yum 改成 apt 就可以), 注意opencc需要用他提到的那种方法用github上的版本, 否则会报错... 2. clone repo 第二步可以直接克隆我的git repo到本地: git clone https://github.com/X-Wei/x-wei.github.com.git 然后在下载下来的文件夹里, 删除除了 pelican_dir 以外的所有文件/文件夹, 进入 pelican_dir , 这里面有个content文件夹, 里面就是我的博客的markdown源文件, 每个文件夹为一个category分类( images , static , pages 目录除外), 想要写自己的博客只要把这里的markdown文件替换掉即可. 另外注意, 如果写的markdown文件里面有引用图片的话, 需要把图片放在 images 目录下面, 然后在md文件里图片地址写成 ../images/XXX.jpg 3. 更多配置 然后可以修改一下pelicanconf.py文件, 比如修改一下博客名字/作者名字/disqusid之类的... 关于一些config变量的用法, 可以参考pelican的文档: http://docs.getpelican.com/en/3.6.3/settings.html , 以及bootstrape3主题的文档: https://github.com/DandyDev/pelican-bootstrap3/wiki/Variables . 4. 本地预览 写好博客内容以及配置好pelican以后, 可以首先在本地预览一下. 在 pelican_dir 目录下, 输入: make html 这会在 pelican_dir/outpt 下生成(预览版的)网站的所有文件. 想要查看效果, 只要继续在 pelican_dir 下输入: make serve 然后浏览器打开 http://localhost:8000/ 即可看到预览的效果. 5. push到github 如果想要像我一样建立一个github.io域名的博客, 需要首先有一个github账号, 然后新建一个名字为 your_github_id.github.io 的git repo, 然后在本地把刚才那个 pelican_dir 文件夹复制到这个repo的根目录. 之后在 pelican_dir 下运行: make publish , 这会在 your_github_id.github.io 的根目录(也就是 pelican_dir 的上级目录)下生成网站的所有文件. 这些文件生成以后, 只需要push到github即可(pelican的makefile本来有push到github page的选项, 不过没弄明白, 于是还是这么搞好了): git add -A . git commit -m 'your commit message' git push push成功以后, 等几分钟, 就可以在 your_github_id.github.io 看到你自己博客了 !","tags":"soft","title":"pelican3建立github博客&主题配置"},{"url":"http://x-wei.github.io/learn-torch-6-optim.html","text":"ref: http://rnduja.github.io/2015/10/26/deep_learning_with_torch_step_7_optim/ doc: https://github.com/torch/optim/blob/master/doc/intro.md Before we implement the gd update step by defining a gradientUpdate function and calling it in a loop. function gradientUpdate(model, x, y, criterion, learningRate) local pred = model:forward(x) -- assumes pred is what criterion expects as input local loss = criterion:forward(pred, y) model:zeroGradParameters() local grad_cri = criterion:backward(pred, y) model:backward(x, grad_cri) model:updateParameters(learningRate) end But this is functionality is implemented in the optim module. In addition to just grad-descent, it has more complicated optimization algorithms implemented. Interface The interface for all optimization algos are: params_new, fs, ... = optim._method_(feval, params[, config][, state]) explination: params : current parameters vector ( 1D tensor ), this will be updated during optimization feval : a user-defined closure that respects this API: f, df/dx = feval(x) config : a table of parameters for the algorithm (e.g. learning rate) state : a table of state variables params_new : the resulting new parameter (in a 1D tensor), which minimizes the function f fs : a table of f values evaluated during the optimization, fs[#fs] is the optimized function value note: As optim expects the input to be 1D tensors, we need to flatten the parameters in our model, this can be achieved via: params, gradParams = model:getParameters() the reuslting params and gradParams are all flattened into 1D tensor. Example: sgd to train mlp the XOR function Here is an example for learning an XOR using a mlp with one hidden layer. model, criterion First, define the model and criterion (use MSE here, see it as a regression problem): require 'nn' inputs = 2; outputs = 1; HUs = 20 -- parameters model = nn.Sequential() -- make a multi-layer perceptron model:add(nn.Linear(inputs, HUs)) model:add(nn.Tanh()) model:add(nn.Linear(HUs, outputs)) criterion = nn.MSECriterion() data Then generate dataset of XORs: sample 2d inputs, and lables are -1 if the samples are of the sign, otherwise +1. Generate 128 training samples: batchSize = 128 batchInputs = torch.DoubleTensor(batchSize, inputs) batchLabels = torch.DoubleTensor(batchSize) for i = 1, batchSize do local input = torch.randn(2) local label if input[1] * input[2] > 0 then -- calculate label for XOR function label = -1 else label = 1 end batchInputs[i]:copy(input) batchLabels[i] = label end feval() closure Then define the feval function that returns the loss and the gradient wrt the loss: function feval(params) gradParams:zero() local outputs = model:forward(batchInputs) local loss = criterion:forward(outputs, batchLabels) local dloss_doutputs = criterion:backward(outputs, batchLabels) model:backward(batchInputs, dloss_doutputs) return loss, gradParams end finally, apply optim.sgd to the batch for 500 epochs: require 'optim' local sgdcfg = {learningRate=0.01} for epoch=1,500 do optim.sgd(feval, params, sgdcfg) end can take some examples to test: x = torch.Tensor(2) x[1] = 0.5; x[2] = 0.5; print(model:forward(x)[1]) x[1] = 0.5; x[2] = -0.5; print(model:forward(x)[1]) x[1] = -0.5; x[2] = 0.5; print(model:forward(x)[1]) x[1] = -0.5; x[2] = -0.5; print(model:forward(x)[1]) The output is: -0.0073583598776157 0.24137506111789 0.31254747107449 -0.14114052583337 And the signs are correct for XOR function.","tags":"notes","title":"[learning torch] 6. optim (optimization tools)"},{"url":"http://x-wei.github.io/learn-torch-5-nngraph.html","text":"ref: http://rnduja.github.io/2015/10/07/deep_learning_with_torch_step_4_nngraph/ doc: https://github.com/torch/nngraph/ The aim of this library is to provide users of nn package with tools to easily create complicated architectures. luarocks install nngraph optionally can install graphvis for graph visualization. From previous posts, to build networks, there are 2 important classes: Module as a nn layer, can do forward and backward prop Container to combine several Module s The nngraph library provides a way to build any complex network focusing on the network graph and avoiding the use of Containers. nngraph.Node Any Module can be wrapped into a nngraph.Node . For all nn.Module , nngraph overrides the __call__ meta method (the function call operator () ), by calling a module, a Node is returned: th> require 'nngraph'; [0.0000s] th> module = nn.Identity() [0.0001s] th> module nn.Identity [0.0000s] th> module() nngraph.Node [0.0000s] The arguments of __call__ are other (parent) nodes that come into this node, in this way we specify which module(s) will feed into the current node/module: th> h1 = nn.Linear(20, 10)() [0.0001s] th> h2 = nn.Linear(10, 1)(h1) -- h1 is input of h2 [0.0001s] Simple sequential example We make a network by calling nn.gModulet() , this takes 2 arguments: a table of inputs , and a table of outputs : th> mlp = nn.gModule({h1}, {h2}) [0.0002s] Then we can plot the model using graphviz: -- draw graph (the forward graph, '.fg'), use it with itorch notebook graph.dot(model.fg, 'MLP') -- or save graph to file MLP.svg and MLP.dot graph.dot(model.fg, 'MLP', 'MLP') (The first and last nodes are dummy nodes and regroup all inputs and outputs of the graph.) DAG example https://github.com/torch/nngraph#a-network-with-2-inputs-and-2-outputs Here is an example to build a model with 2 inputs and 2 outputs: h1 = nn.Linear(20, 20)() h2 = nn.Linear(10, 10)() hh1 = nn.Linear(20, 1)(nn.Tanh()(h1)) hh2 = nn.Linear(10, 1)(nn.Tanh()(h2)) madd = nn.CAddTable()({hh1, hh2}) oA = nn.Sigmoid()(madd) oB = nn.Tanh()(madd) gmod = nn.gModule({h1, h2}, {oA, oB}) x1 = torch.rand(20) x2 = torch.rand(10) gmod:updateOutput({x1, x2}) gmod:updateGradInput({x1, x2}, {torch.rand(1), torch.rand(1)}) graph.dot(gmod.fg, 'Big MLP') alternatively, use - to make your code looks like the data flow : h1 = - nn.Linear(20,20) h2 = - nn.Linear(10,10) hh1 = h1 - nn.Tanh() - nn.Linear(20,1) hh2 = h2 - nn.Tanh() - nn.Linear(10,1) madd = {hh1,hh2} - nn.CAddTable() oA = madd - nn.Sigmoid() oB = madd - nn.Tanh() gmod = nn.gModule( {h1,h2}, {oA,oB} ) Annotations of nodes https://github.com/torch/nngraph#annotations Use node::annotate() to annotated nodes, can add name/description, or change node color. WIth annotation, can enable debugging too: https://github.com/torch/nngraph#debugging","tags":"notes","title":"[learning torch] 5. nngraph (another way to construct nn)"},{"url":"http://x-wei.github.io/learn-torch-4-criterion.html","text":"ref: http://rnduja.github.io/2015/10/05/deep_learning_with_torch_step_3_nn_criterions/ doc: https://github.com/torch/nn/blob/master/doc/criterion.md Criterion : abstract class, given input and target(true label), a Criterion can compute the gradient according to a certain loss function. Criterion class important methods: forward(input, target) : compute the loss function, the input is usually the prediction/log-probability prediction of the network, target is the truth label of training data. backward(input, target) : compute gradient of the loss function. subclasses of Criterion : classification critierions: cross-entropy, neg loglikelihood, ... regression criterions: MSE, Abs, KL divergence, ... embedding criterions misc criterions Classification criterion examples ClassNLLCriterion negative log likelihood criterion https://github.com/torch/nn/blob/master/doc/criterion.md#nn.ClassNLLCriterion crt = nn.ClassNLLCriterion([weights]) optional argument weights is to assign class weights (1D tensor), which is useful for unbalanced dataset. For NLL criterion, the input given through a forward(input, target) is expected to be the log-probabilities of each class. The target is expected to be a class index (1 to n). The probabilities of each class can be computed by applying softmax on logits , the log-proba is just to take the log of the probabilities. Can use directly logsoftmax layer to achieve this (ex. add nn.LogSoftMax as last layer of a sequential container). If the input x is log-proba of each class, the loss is just: loss = forward(x, target) = -x[target_class] CrossEntropyCriterion https://github.com/torch/nn/blob/master/doc/criterion.md#nn.CrossEntropyCriterion This combines a logsoftmax and a NLLcriterion, so the input is expected to be logits (scores) MarginCriterion https://github.com/torch/nn/blob/master/doc/criterion.md#margincriterion computes hinge loss of binary classification problem. input x is expected to be svm scores, target y is expected to be ±1 labels. Regression criterion examples MSECriterion https://github.com/torch/nn/blob/master/doc/criterion.md#nn.MSECriterion criterion = nn.MSECriterion() the loss is just MSE, input and target both have n elements: loss = forward(x,y) = sum[ (xi-yi)&#94;2 ] / n AbsCriterion L1 distance between x and y. DistKLDivCriterion KL divergence for class probabilities A Complete Example updating function First write a function for grad-desc updating for a model , input to the model is x , truth label is y . function gradientUpdate ( model , x , y , criterion , learningRate ) local pred = model : forward ( x ) -- assumes pred is what criterion expects as input local loss = criterion : forward ( pred , y ) model : zeroGradParameters () local grad_cri = criterion : backward ( pred , y ) model : backward ( x , grad_cri ) model : updateParameters ( learningRate ) end This function implements an update step, given a training sample ( x , y ): the model computes its output by model:forward(x) criterion takes model's output, and computes loss by criterion:forward(pred, y) , note : the output of model shall be what criterion expects, e.g. pred=log-class-proba for NLL criterion. criterion gives the gradient of loss function wrt the model output by cri:backward(pred, y) model computes the gradient of its parameters using the gradient from criterion by model:backward(x, grad_cri) the model do a gradient descent step to modify its parameters by model:updateParameters(learningRate) This is the function that we should pass to an optimizer. model, criterion and data the model is just a linear layer (5 inputs, 1 output ), output = Ax+b lua model = nn.Sequential() model:add(nn.Linear(5,1)) the criterion is just hinge loss: criterion = nn.MarginCriterion(1) For the data, just use 2 datapoints: lua x1 = torch.rand(5) y1 = torch.Tensor({1}) x2 = torch.rand(5) y2 = torch.Tensor({-1}) training To train the model, we run the update funcion on the data points 1000 times ( epochs ): lua for i = 1,1000 do gradientUpdate(model, x1, y1, criterion, 0.01) gradientUpdate(model, x1, y1, criterion, 0.01) end evaluating to see the prediciton, just use model:forward(x) lua print('prediction for x1='..model:forward(x1)[1]..' expected value='..y1[1]) print('prediction for x2='..model:forward(x2)[1]..' expected value='..y2[1]) to see loss, use criterion:forward(model_out, y) lua print('loss after training for x1 = ' .. criterion:forward(model:forward(x1), y1)) print('loss after training for x2 = ' .. criterion:forward(model:forward(x2), y2))","tags":"notes","title":"[learning torch] 4. Criterion (loss function)"},{"url":"http://x-wei.github.io/learn-torch-3-container.html","text":"doc: https://github.com/torch/nn/blob/master/doc/containers.md ref: http://rnduja.github.io/2015/10/04/deep_learning_with_torch_step_2_nn_containers/ Container, similarly to Module, is the abstract class defining the base methods inherited from concrete containers. Container contains modules (layers) . Container class important methods: add(module) get(index) : get module at the index size() important subclasses: Sequential Parallel Concat Sequential Sequential is just a stack of layers , add layer by model:add() . Here is a simple 2-layer MLP example: th> mlp = nn.Sequential() [0.0000s] th> mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units [0.0001s] th> mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function [0.0001s] th> mlp:add( nn.Linear(25, 1) ) -- 1 output [0.0001s] th> mlp:forward(torch.range(1,10)) 1.2697 [torch.DoubleTensor of size 1] Parallel module = Parallel(inputDimension,outputDimension) Creates a container module that applies its ith child module to the ith slice of the input Tensor by using select on dimension inputDimension . It concatenates the results of its contained modules together along dimension outputDimension . So if the input for parallel model is x , the input for its ith child module should be: x.select(inputDimension, i) , and the parallel model should be: torch.cat( out1, out2, ouputDimension) (concat along this dimension). th> mlp = nn.Parallel(2,1) -- select(split) on dim2 for input, concat along dim1 for output [0.0000s] th> mlp:add(nn.Linear(10,3)) -- input=1st slice of x (x:select()), output1: size=3 [0.0001s] th> mlp:add(nn.Linear(10,2)) -- output2: size=2 [0.0001s] th> x = torch.randn(10,2) [0.0001s] th> x 0.3242 -1.3911 0.7433 -0.2725 0.3947 0.3332 1.1618 0.6743 0.6655 -1.0901 0.0419 -0.7845 -0.8508 -1.4670 -0.3842 -0.4107 0.5238 2.3616 1.4136 -0.1327 [torch.DoubleTensor of size 10x2] [0.0002s] th> mlp:forward(x) -0.0456 -0.5682 -0.3488 -1.3786 -0.6320 [torch.DoubleTensor of size 5] Concat module = nn.Concat(dim) Concat concatenates the output of its \"parallel\" children modules along dim : these child modules take the same inputs , and their output is concatenated. th > mlp = nn .Concat ( 1 ) -- ouput concat along dim 1 [ 0.0001 s ] th > mlp :add ( nn .Linear ( 10 , 2 ) ); [ 0.0001 s ] th > mlp :add ( nn .Linear ( 10 , 3 ) ); [ 0.0000 s ] th > x = torch .randn ( 1 , 5 ) th > mlp :forward ( x ) 0 .7497 -0 .1909 0 .3280 -0 .3981 0 .0207 [ torch.DoubleTensor of size 5 ]","tags":"notes","title":"[learning torch] 3. Container (models)"},{"url":"http://x-wei.github.io/learn-torch-2-module.html","text":"Module is an abstract class which defines fundamental methods necessary for a Layer . doc: https://github.com/torch/nn/blob/master/doc/module.md Module class variables in Module : output : Tensor, the ouput computed from last call of forward(input) gradInput : Tensor, gradient wrt input of module, computed from last call of updateGradInput(input, gradOutput) important methods in Module : forward(input) : return corresponding output of layer backward(input, gradOutput) : return gradInput wrt the given input Linear https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear Linear extends Module , it's just linear transformation of input: y=Ax+b (parameters/variables: weight , bias ) gradWeight , gradBias are respectively the gradient of each parameter. th > ln = nn .Linear ( 3 , 2 ) -- 3 input , 2 output [ 0.0001 s ] th > ln .weight :fill ( 1 ); ln .bias :zero (); [ 0.0000 s ] th > x = torch .Tensor ( { 1 , 2 , 3 } ) [ 0.0000 s ] th > y = ln :forward ( x ) [ 0.0000 s ] th > gradinput = ln :backward ( x , y ) [ 0.0001 s ] th > gradinput 12 12 12 [ torch.DoubleTensor of size 3 ] [ 0.0001 s ] th > ln .gradInput 12 12 12 [ torch.DoubleTensor of size 3 ] [ 0.0001 s ] th > ln .gradWeight 1 .1132e + 171 1 .2000e + 01 7 .3587e + 223 1 .7112e + 243 2 .3276e + 251 5 .0404e + 180 [ torch.DoubleTensor of size 2 x3 ] [ 0.0001 s ] th > ln .gradBias 6 6 [ torch.DoubleTensor of size 2 ] [ 0.0001 s ] Identity output reproduces input, this layer can be used to model the input layer of a neural network. th> id = nn.Identity() [0.0000s] th> y = id:forward(x) [0.0000s] th> y 1 2 3 [torch.DoubleTensor of size 3] [0.0001s] th> id:backward(x,y) 1 2 3 [torch.DoubleTensor of size 3] [0.0001s] th> id.gradInput 1 2 3 [torch.DoubleTensor of size 3] Other modules https://github.com/torch/nn/blob/master/doc/simple.md some examples: Add Mul CMul Reshape","tags":"notes","title":"[learning torch] 2. Module (layers)"},{"url":"http://x-wei.github.io/learn-torch-1-tensor.html","text":"A Tensor is the fondamental data type in torch, (similar to numpy for tensorflow), it's a potentially multi-dimensional matrix. See doc: https://github.com/torch/torch7/blob/master/doc/tensor.md basic ops Indicate shape in constructor: th> x = torch.Tensor(3,4) [0.0000s] th> x 3.7366e+193 9.4775e+170 3.3018e+180 4.8950e-85 1.3808e+267 7.6859e+261 3.7512e-81 1.4692e+195 9.7016e+189 6.9641e+252 9.1815e+170 4.5239e+217 [torch.DoubleTensor of size 3x4] By default the elements of a newly allocated memory are not initialized , might contain arbitrary numbers ! x:dim() : return nb of dimensions x:nElement() : return nb of elements (\"size\") x:size() : return \"shape\", shortcut is: #x x:resize(sz1, sz2, ...) : will not throw exception when total size is inconsistent! th > # x 3 4 [ torch . LongStorage of size 2 ] [ 0.0001 s ] th > x : size () 3 4 [ torch . LongStorage of size 2 ] [ 0.0001 s ] th > x : dim () 2 th > x = torch . Tensor ( 3 , 4 ) [ 0.0000 s ] th > x : resize ( 2 , 6 ) 1.7479e+270 7.0981e+194 7.4861e-114 1.7479e+270 8.2791e-114 3.6822e+180 4.8548e-27 6.9204e-72 8.8289e+199 1.1567e+247 4.8548e-27 7.7700e-109 [ torch . DoubleTensor of size 2 x6 ] [ 0.0002 s ] th > x : resize ( 2 , 7 ) Columns 1 to 6 1.7479e+270 7.0981e+194 7.4861e-114 1.7479e+270 8.2791e-114 3.6822e+180 6.9204e-72 8.8289e+199 1.1567e+247 4.8548e-27 7.7700e-109 6.9006e-72 Columns 7 to 7 4.8548e-27 1.0240e-259 [ torch . DoubleTensor of size 2 x7 ] [ 0.0001 s ] th > x : resize ( 2 , 4 ) 1.7479e+270 7.0981e+194 7.4861e-114 1.7479e+270 8.2791e-114 3.6822e+180 4.8548e-27 6.9204e-72 [ torch . DoubleTensor of size 2 x4 ] fill with constant value: th> x:fill(1) 1 1 1 1 1 1 1 1 1 1 1 1 [torch.DoubleTensor of size 3x4] other constructors: th > y = torch . Tensor ( x ) -- note: y is just a reference of x!!! [ 0.0001 s ] th > y = torch . Tensor ({ { 2 , 3 , 4 },{ 1 , 2 , 3 } }) [ 0.0000 s ] th > y 2 3 4 1 2 3 [ torch . DoubleTensor of size 2 x3 ] Storage One could say that a Tensor is a particular way of viewing a Storage : a Storage only represents a chunk of memory, while the Tensor interprets this chunk of memory as having dimensions. th > s = x : storage () -- 'flatten' version of x [ 0.0000 s ] th > for i = 1 , # s do s [ i ] = i end [ 0.0000 s ] th > x 1 2 3 4 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] short for range: torch.range(1,5) Slicing https://github.com/torch/torch7/blob/master/doc/tensor.md#extracting-sub-tensors Slicing using sub() and select() . th > x 1 2 3 4 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] th > print ( x : sub ( 2 , 3 , 2 , 4 )) -- x[2:3, 2:4] slicing 6 7 8 10 11 12 [ torch . DoubleTensor of size 2 x3 ] th > print ( x : select ( 2 , 2 )) -- select(dim, index), dim=1 for rows, =2 for cols 2 6 10 [ torch . DoubleTensor of size 3 ] Or use slicing/indexing shortcut : [{ {dim1start, dim1end},...}] [dim1, dim2,...] th > x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [ torch . DoubleTensor of size 5 x6 ] [ 0.0002 s ] th > x [ 2 ][ 3 ] 9 [ 0.0000 s ] th > x [{ 2 , 3 }] 9 [ 0.0000 s ] th > x [{ 2 ,{ 2 , 5 }}] 8 9 10 11 [ torch . DoubleTensor of size 4 ] [ 0.0001 s ] th > x [{ {}, 3 }] 3 9 15 21 27 [ torch . DoubleTensor of size 5 ] All are references All tensor operations in this class do not make any memory copy. All these methods transform the existing tensor, or return a new tensor referencing the same storage . th > y = torch . Tensor ( x ) [ 0.0000 s ] th > y 1 2 3 4 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] [ 0.0001 s ] th > y : select ( 1 , 1 ): zero () -- x will be effected [ 0.0001 s ] th > y 0 0 0 0 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] [ 0.0001 s ] th > x 0 0 0 0 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] If don't want to modify original tensor, use clone() : th > y = x : clone () [ 0.0000 s ] th > y : sub ( 1 , 3 , 1 , 2 ): fill ( - 1 ) - 1 - 1 - 1 - 1 - 1 - 1 [ torch . DoubleTensor of size 3 x2 ] [ 0.0001 s ] th > y - 1 - 1 0 0 - 1 - 1 7 8 - 1 - 1 11 12 [ torch . DoubleTensor of size 3 x4 ] [ 0.0001 s ] th > x 0 0 0 0 5 6 7 8 9 10 11 12 [ torch . DoubleTensor of size 3 x4 ] Matrix ops Some matrix operations random matrix: th > torch . manualSeed ( 1 ) [ 0.0000 s ] th > A = torch . rand ( 3 , 4 ) [ 0.0001 s ] th > print ( A ) 0.4170 0.9972 0.7203 0.9326 0.0001 0.1281 0.3023 0.9990 0.1468 0.2361 0.0923 0.3966 [ torch . DoubleTensor of size 3 x4 ] transpose th > At = A : t () [ 0.0000 s ] th > At 0.4170 0.0001 0.1468 0.9972 0.1281 0.2361 0.7203 0.3023 0.0923 0.9326 0.9990 0.3966 [ torch . DoubleTensor of size 4 x3 ] matrix mul is just * th > A * At 2.5568 1.2773 0.7330 1.2773 1.1059 0.4544 0.7330 0.4544 0.2431 [ torch . DoubleTensor of size 3 x3 ] inner product: dot() th > A [ 1 ]: dot ( At : select ( 2 , 1 )) 2.5568154905493 inverse: torch.inverse(sq_mat) More operations can be found at: https://github.com/torch/torch7/blob/master/doc/maths.md example: torch.ones()/eye()/zeros() torch.cat() : concat tensors th > torch . cat ( torch . ones ( 3 , 2 ), torch . zeros ( 2 , 2 ), 1 ) 1 1 1 1 1 1 0 0 0 0 [ torch . DoubleTensor of size 5 x2 ] torch.conv2()","tags":"notes","title":"[learning torch] 1. Tensor"},{"url":"http://x-wei.github.io/ocamlMOOC_wk0_intro.html","text":"Intro ML lang family statically strongly typed languages fisrt-class functions type inference pattern matching highlights of ocaml safty: static typing, pattern matching efficiency: high performance expressiveness: functional+type inference+polymorphism package manager: opam debugger profiler REPL: \"toplevel\" bytecode compiler: ccamlc native compiler: ocamlopt programming environment fully online: https://try.ocamlpro.com/fun-demo/tryocaml_index.html note: the local REPL ocaml (toplevel) is not very good, either use ledit ocaml , or use utop . Quick tour of the language list [] is empty list a::l element a in head and l as remaining tail. [1;2;3;4] function : sum of list write with pattern matching: let rec sumlist = [] -> 0 | a :: rest -> a + ( sumlist rest );; This func has auto-inferred as of type: int list ->int . all types are computed and enforced at compile time. polymorphism and higher order note: polymorphism ~= generic in java/template in cpp turn the 0 and + in previous function into parameters, call this function fold let rec fun fold op e = function [] -> e | a :: rest -> op a ( fold op e rest ) infered the most general type: val fold : ('a -> 'b -> 'b) -> 'b -> 'a list -> 'b = <fun> pattern matching write a function to remove all consecutive duplicates from a list: let rec destutter = function | [] -> [] | x :: y :: rest -> if x == y then destutter ( y :: rest ) else x :: destutter ( rest );; ⇒ compiler tell us we forgot to add the case when only one element is in list !! And this case: let rec destutter = function | [] -> [] | x :: [] -> x :: [] | x :: y :: rest -> if x == y then destutter ( y :: rest ) else x :: destutter ( rest );;","tags":"notes","title":"[OCaml MOOC] week0: intro and overview"},{"url":"http://x-wei.github.io/progfun2_lec1_forexpr.html","text":"Recap: Functions and Pattern Matching case classes ex: json json objects can be seq, num, str, bool,... ⇒ represented as abstract class and case classes. pattern matching → question: what is the type of the {case(key, value)=>\"...\"} clause? it is (JBinding => String) type, which is a shorthand for Function1[JBinding, String] . Function1 Trait subclass a function type function types can also be extended ! element accessing can be written as function calls because Seq s are functions! Partial Match if there is no match → throw MatchError val f: String=>String = {case \"ping\" => \"pong\"} f(\"ping\") // no pb f(\"abc\") // MatchError ⇒ define f as partial function val f: PartialFunction[String,String] = {case \"ping\" => \"pong\"} f.isDefinedAt(\"ping\") f.isDefinedAt(\"abc\") isDefinedAt is a method for the PartialFunction class. The f definition is translated to: But the PartialFunction will only apply for level 1: Recap: Collections scala collections hirarchy: collections share some general methods ( core methods ): map flatMap filter foldLeft/foldRight (idealized) implementation of map and flatMap on Lists: For expressions for-expr can simplify combinations of core methods . the lhs of a generator can also be a pattern! pat <- expr is translated to : x <- expr withFilter { case pat => true ; case _ => false } map { case pat => x } 1.1 - Queries with For for notation is equivalent to common ops on databases(ex. sql). ex. books in library case class Book(title: String, authors: List[String]) query1: books with author name is \"Bird\" for ( b <- books ; a <- b.authors if a startsWith \"Bird,\" ) yield b.title query2: books with \"Program\" in the title: for ( b <- books if b.title indexOf \"Program\" >= 0 ) yield b.title query3: names of authors who wrote >=2 books for { b1 <- books b2 <- books if b1 != b2 a1 <- b1.authors a2 <- b2.authors if a1 == a2 } yield a1 → pb: the authors will be doubled → b1,b2 and b2,b1 ⇒ change line 3 to b1.title < b2.title → still pb: print 3 times if authors write 3 books... ⇒ sol1. use distinct function sol2. decalre books as Set instead of List. 1.2 - Translation of For for expressions → higer order functions map, flatMap, filter can all be implemented with for expression: In reality: scala translates for expr to map/flatMap/filter. implemention of for-expr: 3 rules rule 1: in for(..) only a simple generator for(x <- l1) yield e2 is translated to: l1.map(x => e2) rule2: in for(..), followed by the generator there is a filter for(x <- l1 if f; s) yield e2 //s is seq of other generators and filters is translated to: for( x <- l1.withFilter(x=>f) ) yield e2 rule3: in for(..), starts with 2 generators → flatMap for( x<-l1; y<-l2; s) yield e3 is translated to: l1.flatMap( x => for(y<-l2; s) yield e3 ) example: for { b <- books a <- b.authors if a startsWith \"Bird\" } yield b.title translated to: books flatMap ( b => b.authors.filter( a => a startsWith \"Bird\").map(y=>y.title) ) NB: for expr is not restricted to collections, it supports any types with map/flatMap/withFilter method. ⇒ use for expr for your own types as well. 1.3 - Functional Random Generators goal: use for expr on rand generators. for expr support any type with map/flatMap/filter ⇒ ex. rand value generator. generate rand value of type T: trait Generator[+T] { def generate: T } first implement Generator[Int], then use this to implement booleans, pairs, lists, sets, trees,...... val integers = new Generator[Int]{ val rand = new java.util.Random def generate = rand.nextInt() } val booleans = new Generator[Boolean{ def generate = integers.generate > 0 } val pairs = new Generator[(Int, Int)]{ def generate = (integers.generate, integers.generate) } → can we avoid the new Generator[...] ? ideally we want to write booleans as pairs as: val booleans = for ( x <- integers ) yield x > 0 def pairs [ T , U ]( t : Generator [ T ], u : Generator [ U ]) = for { x <- t ; y <- u } yield ( x , y ) the for expr will be translated to map/flatMap/filter... ⇒ define map and flatMap on the Generator trait so that it supports for expr! trait Generator[+T]{ self => // syntax: `self` is an alias of `this` def generate: T def map[S](f: T=>S): Generator[S] = new Generator[S]{ def generate = f(self.generate) // can't use `this` here: inf loop // or use Generator.this.generate } def flatMap(f: T=>Generator[S]): Generator[S] = new Generator[S]{ def generate = f(self.generate).generate } } ex. the booleans expression: val booleans = for (x<-integers) yield x>0 is translated to: val booleans = integers map (x=>x>0) which is then expands to: val booleans = new Generator[Boolean]{ val f = (x => x>0) def generate = f(integers.generate) } after reduction, the expression is: val booleans = new Generator[Boolean{ def generate = integers.generate > 0 } which is the initial implementation... other base Generators (The T* syntax is variable parameter) rand List Generator def lists : Generator [ List [ Int ]] = for { isEmpty <- booleans list <- if ( isEmpty ) emptyLists else nonEmptyLists } yield list def emptyLists = single ( Nil ) def nonEmptyLists = for { head <- integers tail <- lists // recursive call to `lists` } yield head :: tail rand (binary) Tree Generator two types of tree nodes: leaf or inner node def trees : Generator [ Tree ] = for { isLeaf <- booleans tree <- if ( isLeaf ) leafs else inners } yield tree def leafs : Generator [ Leaf ] = for { x <- integers } yield Leaf ( x ) def inners : Generator [ Inner ] = for { l <- trees r <- trees } yield Inner ( l , r ) Application: random testing test: check postconditions (expected results) → generate random test inputs a generic wrapper: def randTest[T](g: Generator[T], numTimes: Int=100)(testfcn: T=>Boolean): Unit = { for( i <- 0 until numTimes){ val value:T = g.generate assert(testfcn(value), \"test failed for \"+value) } println(\"passed\" + numTime + \"tests\") } ScalaCheck instead of writing tests, write properties that are assumed to hold. forAll { (l1:List[Int], l2:List[Int]) => l1.size + l2.size == (l1++l2).size } 1.4 - Monads from last section: not only collections, but also any type with map and flatmap can use for expr ⇒ monads. . Monads is a type M[T] with 2 operations: flatMap (\"bind\") and unit , and satisfy some laws. train M[T]{ def flatMap[U](f: T=>M[U]): M[U]): M[U] } def unit[T](x:T): M[T] examples: List is a monad, unit(x) = List(x) Set , with unit(x) = Set(x) Option , with unit(x) = Some(x) Generator , with unit(x) = single(x) map can be defined as a combination of flatMap and unit : m map f == m flatMap (x => unit( f(x)) ) Monad laws associativity (m flatMap f) flatMap g == m flatMap ( x => f(x) flatMap g ) ↔ (x+y+z) = x+(y+z) left unit unit(x) flatMap f == f(x) right unit m flatMap unit == m the Try type We define a Try class, which is similar to Option class. abstraxt class Try[+T] case class Success[T](x:T) extends Try[T] case class Failure(ex: Exception) extends Try[Nothing] we can write Try(expr) to give a computation a try, by implementing the apply method of object Try: object Try{ def apply[T](expr: =>T): Try[T]= // expr is passed BY NAME, otherwise will cause exception in eval try Success(expr) // java syntax of try-catch catch{ case NonFatal(ex) => Failure(ex) } } } if Try is a Monad ⇒ can be written in for expr: ⇒ define map and flatMap on Try type. question: is Try a monad with unit(x)=Try(x) ? ⇒ no, left-unit fails: Try(expr) flatMap f != f(expr) (lhs never nonfatal exception, but rhs will raise) Try is not a monad, but it can still use for expr... Conclusion for exprs are useful not only for collections: Generator , Option , Try","tags":"notes","title":"[Scala MOOC II] Lec1: For Expressions and Monads"},{"url":"http://x-wei.github.io/progfun1_lec6_collections.html","text":"6.1 - Other Collections so far: only seen List. → more ( immutable ) collections. vector List: is linear -- access to head is faster than middle or end element. Vector: better rand access performance. represented as very shallow trees(32-split at each node) Vector support similar operations as List (head, tail,map, fold) , except concat :: , instead Vectors have +: and :+ : x +: xs create new vector with x in head xs :+ x create new vector with x in tail implementation of :+ : create copies Seq Seq is base class for Vector and List , Seq itself is subclass of Iterable . Hierarchy of Iterables: Array ans String Array and String support same op as Seq, can implicitly be converted to seq when needed. But they are NOT subclasses of Seq as they come from java. scala> val xs: Array[Int] = Array(1,2,3) xs: Array[Int] = Array(1, 2, 3) scala> xs map (2*_) res3: Array[Int] = Array(2, 4, 6) scala> val ys = \"Hello\" ys: String = Hello scala> ys filter (_.isUpper) res4: String = H Range represents a seq of evenly spaced integers. represented as an obj with 3 values: lower bound, upper bound, step value. 3 op: to : inclusive until : exclusive by : step value 1 to 6 // 1 2 3 4 5 6 1 until 6 // 1 2 3 4 5 1 to 6 by 2 // 1 3 5 6 to 1 by -2 // 6 4 2 More ops on Seq xs exists p xs forall p xs zip ys : takes 2 seq, returns a single seq, each elem is a pair scala> List(1,2,3) zip Vector(2,3,4) res7: List[(Int, Int)] = List((1,2), (2,3), (3,4)) xs.unzip : reverse zip, return 2 lists xs flatMap f : apply f to all elements, and concat the results scala> \"hello\" flatMap (c => List('.', c)) res9: String = .h.e.l.l.o flatten : flatten a seq of seq into just one seq... xs flatMap f = (xs map f).flatten The result is concated, instead of being a list of list. xs.sum / xs.product : for numeric collection xs.max / xs.min : an ordering must exist Examples cartesen product: 1..M x 1..N: (1 to M) flatMap( x => (1 to N) map (y => (x,y)) ) scalar of 2 vectors def scalaProd(xs: Vector[Double], ys: Vector[Double]): Double = (xs zip ys) map (xy => xy._1 * xy._2).sum alternative: use pattern matching in map (need to add braces {} !): (xs zip ys) map ( {case(x,y) => x*y}).sum test isPrime for a number def isPrime(n: Int): Boolean = (2 until n) forall (d => n%d!=0) 6.2 - Combinatorial Search and For-Expressions goal: handle nested seq. ⇒ extend usage of higher order functions instead of using nested loops. example: given n, find all (i,j) st: 1<=j<i<=n, and i+j is prime. (1 until n) flatMap ( i => (1 until i) map (j => (i,j)) ) filter ( {case(i,j) => isPrime(i+j)}) this works, but less understandable... ⇒ for expressions. for/yield expressions example: class of person case class Person(name: String, age: Int) To obtain names of persons over 20 years old: persons filter (_.age>20) map (_.name) equivalent to : for (p <- person if p.age > 20) yield p.name The for expression is similar to jave for-loop, except it builds a list of the results of all iterations . for expression form: for( s ) yield e where s can contain generators and filters : generator: p<-e where e is a collection, p is a pattern filter: if f where t is boolean expr rewrite the prime sum example: for { i <- 1 until n j <- 1 until i if isPrime ( i + j ) } yield ( i , j ) exercice: write the scalaProd using for expr: def scalaProd ( xs : Vector [ Double ], ys : Vector [ Double ]) : Double = ( for ( ( x , y ) <- xs zip ys ) yield x * y ) sum more on scala for expr: http://www.artima.com/pins1ed/for-expressions-revisited.html 6.3 - Combinatorial Search Example goal: combine set and for -expr to solve the n-queens problem. Sets 3 fundamental iterable collection types: seq, set , map. set support most operations on seq (c.f. doc on Iterable) sets are unordered set don't have dup elements contains: s contains e Example: N-Queens 8*8 chess board, 8 queens ⇒ places queens st: no two queen in same row/col/diag algo: put one queen in each row (in the right column) recursive solution: suppose already have solutions for first n-1 rows. represent each (partial)solution as a list of column index. return all possible solutions (as a Set). def queens(n: Int): Set[List[Int]] = { def placeQueens(k: Int): Set[List[Int]] = ???// place first k rows placeQueens(n) } Fill in the helper function: def placeQueens ( k : Int ) : Set [ List [ Int ]] = { if ( k == 0 ) Set ( List ()) else for { queens <- placeQueens ( k -1 ) // queens ( List [ Int ]) = one solution to k -1 c <- 0 until n if isSafe ( c , queens ) // if column c doesn ' t conflict with the partial solution queens } yield c :: queens // the kth row solution is in head of list } Now fill in the function isSafe that check if a column is valid wrt queens for above rows: def isSafe(col: Int, queens: List[Int]): Boolean = { val k = queens.length // current row is k val indexedQueens = (k-1 to 0 by -1) zip queens // List[(row, col)] indexedQueens forall { case(r,c) => c!=col && k-r!=math.abs(c-col) } } Now the queens function will work. Write another function to print the solution out: def show ( queens : List [ Int ]) : String = { val lines = for ( col <- queens.reverse ) yield Vector.fill ( queens.length ( \"* \" ) . updated ( col , \"X \" ) . mkString \"\\n\" + lines.mkString ( \"\\n\" ) } ( queens ( 4 ) map show ) mkString \"====\\n\" 6.4 - Maps Map[Key, Value] ex: val romanNumerals = Map(\"I\" ->1, \"V\" ->5, \"X\" -> 10) val captial = Map(\"France\" -> \"Paris\", \"US\" -> \"Washington\") Maps are iterables and functions : maps Key to Value like a function call ex. romanNumerals(\"I\") will throw NoSuchElementException if the key is not in map. ⇒ use get : scala> captial(\"France\") res3: String = Paris scala> captial get \"France\" res4: Option[String] = Some(Paris) scala> captial get \"China\" res5: Option[String] = None have a look at the Option type: The Option Type definition: trait Option[+A] case class Some[+A](value: A] extends Option[A] object None extends Option[Nothing] so the get function of map gives: None if key is not in map Some(x) if map associates key to x since Some is case class, can use pattern matching to decompose. def showCaptical(country: String) = captial.get(country) match{ case Some(cap) => cap case None => \"missing data\" } Sorted and GroupBy two useful SQL queries. orderBy can be expressed using sortWith and sorted: val fruit = List(\"apple\", \"pear\", \"orange\", \"pineapple\") scala> fruit sortWith ( .length < .length) res7: List[String] = List(pear, apple, orange, pineapple) scala> fruit.sorted res8: List[String] = List(apple, orange, pear, pineapple) GroupBy(f) : partition a collection into a map of collections according to f scala> fruit groupBy (_.head) res9: scala.collection.immutable.Map[Char,List[String]] = Map(p -> List(pear, pineapple), a -> List(apple), o -> List(orange)) map Example: polynomials can be seen as map from exponents to coefficients: ⇒ represent polynm as maps class Poly(val terms: Map[Int, Double]) { def + (other: Poly) = new Poly(terms ++ // ++ on maps: will override older entries in terms! (other.terms maps adjust)) // adjust defined below def adjust(term: (Int, Double)):(Int, Double) = { val (exp, coeff) = term terms get exp match{ case None => Pair(exp, coeff) // or write: exp -> coeff case Some(coeff1) => Pair(exp, coeff+coeff1) } } override def toString = (for( (exp, coeff) <- terms.toList.sorted.reverse) yield coeff+\"x&#94;\"+exp) mkString \"+ \" } default values withDefulatValue : makes a map into a total map rewrite the poly example: class Poly ( terms0 : Map [ Int , Double ]) { val terms = terms0 withDefaultValue 0.0 def + ( other : Poly ) = new Poly ( terms ++ // ++ on maps : will override older entries in terms ! ( other.terms maps adjust )) // adjust defined below def adjust ( term : ( Int , Double )) : ( Int , Double ) = { val ( exp , coeff ) = term exp -> ( coeff + terms ( exp )) } override def toString = ( for ( ( exp , coeff ) <- terms.toList.sorted.reverse ) yield coeff + \"x&#94;\" + exp ) mkString \"+ \" } } change to variable parameter for better constructor calling: add another constructor def this(bindings: (int, Double)*) = // bindings is s sequence this(bindings.toMap) Exercie: re-implement the + using foldLeft : def + (other: Poly) = new Poly( (other.terms foldLeft terms)(addTerm) ) def addTerm(terms:Map[Int, Double], term: (Int, Double)): Map[Int, Double] = { val (exp, coeff) = term terms + ( exp -> (coeff + terms(exp)) ) // map + (k,v) updates the map } 6.5 - Putting the Pieces Together: T9 keyboard mnem convert a seq of numbers to a scentence val mnem = Map('2'->\"ABC\", '3'->\"DEF\", 4->\"GHI\", '5'->\"JKL\", '6'->\"MNO\", '7'->\"PQRS\", '8'->\"TUV\", '9'->\"WXYZ\" ) val in = Source.fromURL(\"...\") val words = in.getLines.toList // dictionary: list of valid words goal: design a translate(phoneNumber) that produces all phrases of words that can serve as mnemonics(助忆) for a phone number. Step 1: invert the mnem map to a map from A~Z to 2~9: val charCode: Map[Char, Char] = for ( (digit, str) <-mnem ; ltr<-str) yield ltr- > digit Step 2: map a word to the string it represents, ex. \"java\"->\"5282\" def wordCode(word: String):String = word.toUpperCase map charCode Step 3: maps from digit string to all possible valid strings, ex. \"5282\"->List(\"java\", \"kata\",...) ⇒ just a groupby!! val wordsForNum: Map[String, Seq[String]] = words groupBy wordCode → Error: key not found: \"-\", some words contains a \"-\"... → drop such words... val words = in.getLines.toList filter (wd => wd forall (ch => ch.isLetter)) Step 4: return all ways to encode a number as a list of words → recursive, recurse on the nb of first words~ def encode ( number : String ) : Set [ List [ String ]] = { if ( number.isEmpty ) Set ( List ()) else { val phraseList = for { split <- 1 to number.length // the first word in phrase uses numbers [ 1 : split ] firstWd <- wordsForNum [ number.take ( split ) ] followingWds <- encode ( number.drop ( split )) } yield firstWd :: follwingWds phraseList.toSet } } ⇒ error in the line with wordsForNum : key not found... ⇒ wordsForNum[\"7\"] will give error ⇒ make the wordsForNum a total map, using withDefaultValue val wordsForNum: Map[String, Seq[String]] = (words groupBy wordCode) withDefaultValue( Seq() ) Step 5: return strings (instead of List[String]) def translate(number: String): Set[String] = { encode(number) map (_.mkString(\" \")) } summary: Conclusion https://www.scala-exercises.org/","tags":"notes","title":"[Scala MOOC I] Lec6: Collections"},{"url":"http://x-wei.github.io/progfun1_lec5_lists.html","text":"5.1 - More Functions on Lists already known methods: xs.head xs.tail sublist and ele access: xs.length xs.last xs.init : all elementh except last element xs.take(n) : sublist of first n elements xs.drop(n) : the rest of list after taking first n elements xs(n) : = xs.apply(n) , element at index n More methods: concatenation: xs ++ ys ( ::: is legacy usage) xs.reverse xs.updated(n,x) : return a same list, except xs(n)=x (Note: Lists are immutable, so cannot modify) xs.indexOf(x) : index or -1 xs.contains(x) : same as xs.indexOf(x)>=0 complexity: head, tail: simple to implement ⇒ complexity of last ? def last[T](xs: List[T]): T = xs match{ case List() => throw new Error(\"last of Nil\") case List(x) => x case y::ys => last(ys) } ⇒ complexity of last = O(n) implement init : def init[T](xs: List[T]): T = xs match{ case List() => throw new Error(\"init of Nil\") case List(x) => List() case y::ys => y::init(ys) } ⇒ complexity = O(length of xs) implement concatenation ::: / ++ (NB: ::: is right-associative, xs:::ys = ys. :::(xs) . ) def concat[T](xs: List[T], ys: List[T]) = xs match{ case List() => ys case z::zs => x:concat(xs, ys) } ⇒ complexity of concat = O(length of xs) implement of reverse def reverse[T](xs: List[T]) = xs match{ case List() => List() case y::ys => reverse(ys) ++ y } ⇒ complexity of reverse: every call contains a concat, thus complexity=O(n2) exercice: remove nth element: def removeAt[T](n:Int, xs: List[T]): List[T] = (xs take n ) ++ (xs drop n+1) 5.2 - Pairs and Tuples example: sort list faster than insertion sort → merge sort. sort 2 sublist, merge them ⇒ list is sorted def msort(xs: List[Int]): List[Int] = { val n = xs.length/2 if(n==0) xs else{ def merge(xs: List[Int], ys: List[Int]) = ... // see below val (fst, snd) = x splitAt n // splitAt returns 2 sublists merge(msort(fst), msort(snd)) } } merge: def merge(xs: List[Int], ys: List[Int]): List[Int] = xs match { case Nil => ys case x::zs => match ys{ case Nil => xs case y::ws => { if(x<y) x::merge(zs, ys) else y::merge(xs, ws) } } } Pair/Tuple written as (x, y) in scala. pair can be used as patterns : similar for tuples. val pair = (\"a\", 2) val (label, value) = pair tuple implementation: ⇒ can use _1 _2 to access elements exercice: rewrite the merge function using a pattern matching over pairs : def merge(xs: List[Int], ys: List[Int]): List[Int] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x:zs, y:ws) => if(x<y) x::merge(zs, ys) else y::merge(xs, ws) } 5.3 - Implicit Parameters pb: how to apply msort to list of other element types. using type parameters ? msort[T] ⇒ the compare operator is not always defined ! ⇒ pass the lt function as a parameter : def msort[T](xs: List[T])(lt: (T,T)=>Boolean) = ... another option: scala.math.Ordering[T] impor math.Ordering def msort[T](xs: List[T])(ord: Ordering) = ...// use ord.lt(x,y) msort(nums)(Ordering.Int) pb: pass each time the function parameter is cumbersome... ⇒ use implicite parameters def msort[T](xs: List[T])(implicite ord: Ordering) = ...// use ord.lt(x,y) ⇒ the function calls can ignore the implicite parameter, the compiler will figure it out. 5.4 - Higher-Order List Functions functions over list have similar pattern: transform each element retrive elements that satisfy some cretirion combing elements using an operator map apply an operation to every elements. abstract class List[T]{ def map[U](f: T=>U): List[U] = this match { case Nil => this case x:xs => f(x)::xs.map(f) } } filtering def filter(p: T=>Boolean): List[T] = this match { case Nil => this case x:xs => f(p(x)) x::xs.filter(p) else xs.filter(p) } other methods that extracts sublist: exercice: implement a function pack : def pack[T](xs: List[T]): List[List[T]] = xs match { case Nil => Nil case x::ys => { val (head, tail) = xs span (c => c==x) head :: pack(tail) } } exercice2: implement a function encode : def encode[T](xs: List[T]): List[(T, Int)] = xs match { case Nil => Nil case x::ys => { val (head, tail) = xs span (c => c==x) (x, head.length) :: pack(tail) } } another version: use the pack: def encode[T](xs: List[T]): List[(T, Int)] = pack(xs) map (l => (l.head, l.length)) 5.5 - Reduction of Lists fold/reduce: combine elements using an operator. reduceLeft (can apply only to non-empty lists) inserts a binary operator between adj elements: ex. def sum(xs: List[Int]) = (0::xs) reduceLeft ( (x,y)=> x+y) def prod(xs: List[Int]) = (1::xs) reduceLeft ( (x,y)=> x*y) write shorter function values using underscore _ : every _ represents a new parameter def sum(xs: List[Int]) = (0::xs) reduceLeft ( _+_ ) def prod(xs: List[Int]) = (1::xs) reduceLeft ( _*_ ) foldLeft foldLeft is like reduceLeft, but can apply on Nil, and takes an accumulator z => returns z when calling on Nil. def sum(xs: List[Int]) = (xs foldleft 0) ( _+_ ) def prod(xs: List[Int]) = (xs foldleft 1) ( _*_ ) foldRight/reduceRight dual functions to foldLeft and reduceLeft , but produce a tree leaned to right if the operation is associative and communitive, foldLeft and foldRight should give same results. Other times need to think. ex. concat if apply foldLeft ⇒ type error, because the :: operator will be applied to 2 elements of type T. 5.6 - Reasoning About Concat proof of programs structural induction pb: prove some properties of concat: 类似数学归纳法: ex. prove (xs ++ ys) ++ zs = xs ++ (ys ++ zs) : induction on xs def concat[T](xs: List[T], ys: List[T]) = xs match{ case Nil => ys case z::zs => x:concat(xs, ys) } base case: xs=Nil (Nil ++ ys ) ++ zs = Nil ++ (ys ++ zs) induction step: x::xs 5.7 - A Larger Equational Proof on Lists pb: want to prove that xs.reverse.revese == xs base case: Nil.reverse.revers = Nil induction step pb: cannot advance ⇒ generalize the argument.","tags":"notes","title":"[Scala MOOC I] Lec5: Lists"},{"url":"http://x-wei.github.io/Rnotes-4-regression.html","text":"R里面的统计函数有很多, 这里只用线性模型 lm 以及(一维)非参估计最常用的三个smoother: Nadaraya-Watson kernel( NW, ksmooth ), Local Polynomial( LP, loess ), Smoothing Spline( SS, smooth.spline ). 用这三个smoother作为例子, 介绍R里面统计回归的一些用法. 数据的形式是: 目标是估计函数m(). 例子使用R自带的 cars 数据集, 它包含两列: 汽车速度speed和刹车距离dist. > data(cars) > summary(cars) speed dist Min. : 4.0 Min. : 2.00 1st Qu.:12.0 1st Qu.: 26.00 Median :15.0 Median : 36.00 Mean :15.4 Mean : 42.98 3rd Qu.:19.0 3rd Qu.: 56.00 Max. :25.0 Max. :120.00 > ?cars > plot(cars $ speed , cars $ dist ) Theory 首先简单介绍一下这4个smoother的原理: linear model 认为m是线性形式(包含intercept): Nadaraya-Watson kernel smoother m_NW 在x处的取值为Yi的加权平均, 权重是按照kernel K()确定的. 另外m_NW(x)还可以看做是最小化加权的square-error: Local Polynomial smoother m_NW(x)最小化加权sq-err那个表达式里, 可以是用一个 常数函数 mx来估计在x处的取值, LP将它泛化为p-1阶多项式的形式, m在x附近是多项式形式. m(u)=poly(x-u), 这个多项式的系数为beta(x): 最后m_LP在x处的取值为: Smoothing Spline 设定m的形式为knot在xi的spline, 加上penalize项: 另外SS可以看作是bandwidth随x变化的kernel smoother. fit model formula lm 和 loess 的文档里都提到参数为formula, 它大概是指示要fit的表达式形式. 这里面的加减号不是算数意义上的加减. 看例子: dist ~ speed : 表示dist是speed的函数 y ~ . : 表示y是所有其他变量的函数 y ~ x1+x2 : y 是x1和x2的函数 y ~ x - 1 : y是x的函数, 且没有intercept项 fit models 这几个函数的fit写法各不相同, 有的要提供formula, 有的要提供x和y值, 需要看文档: fit.lm <- lm ( dist ~ speed , data = cars ) fit.nw <- ksmooth ( cars $ speed , cars $ dist , kernel = \"normal\" ) fit.lp <- loess ( dist ~ speed , data = cars ) fit.ss <- smooth.spline ( cars $ speed , cars $ dist ) predict, fitted/residuals predict的写法也是各不相同, 一般都是用 predict 函数, 然而这个函数在作用到不同smoother上面, 参数和返回值也都不一样......orz 关于 xx smoother的predict函数用法参考? predict.xx . 最奇葩的是NW, 它不能用 predict 函数, 而要fit的时候在 skmooth 函数里传入 x.points 参数... 看predict例子: newspeed <- 1 : 26 pred.lm <- predict ( fit.lm , newdata = data.frame ( speed = newspeed ), interval = \"prediction\" )[, \"fit\" ] pred.nw <- ksmooth ( cars $ speed , cars $ dist , kernel = \"normal\" , bandwidth = 2 , x.points = newspeed ) $ y pred.lp <- predict ( fit.lp , newdata = newspeed ) pred.ss <- predict ( fit.ss , x = newspeed ) $ y 另外, 如果想看smoother在design points(Xi)处的预测值, 可以用 fitted 函数(NW还是不能用), 例子: fitted.lm <- fitted ( fit.lm ) fitted.nw <- ksmooth ( cars $ speed , cars $ dist , kernel = \"normal\" , bandwidth = 2 , x.points = cars $ speed ) $ y fitted.lp <- fitted ( fit.lp ) fitted.ss <- fitted ( fit.ss ) 要看每个点的residual ri=yi-yhat_i, 用 residuals 函数(NW不行): resd.lm <- residuals ( fit.lm ) resd.nw <- ksmooth ( cars $ speed , cars $ dist , kernel = \"normal\" , bandwidth = 2 , x.points = cars $ dist ) $ y - cars $ dist resd.lp <- residuals ( fit.lp ) resd.ss <- residuals ( fit.ss ) 可以画出这几个方法的fit: abline(fit.lm, lty=1, col=1) # linear model is just a straigh line lines(newspeed, pred.nw, lty=2, col=2) lines(newspeed, pred.lp, lty=3, col=3) lines(newspeed, pred.ss, lty=4, col=4) legend(\"topleft\", c(\"lm\", \"nw\", \"lp\", \"ss\"), lty=1:4, col=1:4) bandwidth&df: Hat Matrix 三个非参估计的smoother都有\"带宽\"(bandwidth)或者\"自由度\"(df)的概念, 带宽即NW或LP表达式里的h. 自由度df是带宽的函数, smoother的df可以用它的 hat matrix S计算出来. 一个smoother的hat matrix S, 是把训练值Y映射到估计值Yhat的矩阵: 而df则是S的迹: df = tr(S). df的 根据script(P28), S的第j列可以用这个smoother fit一个unit vector来得到: 所以计算S可以用下面的代码: n <- nrow ( cars ) Snw <- Slp <- Sss <- matrix ( 0 , nrow = n , ncol = n ) In <- diag ( n ) # identity matrix for ( j in 1 : n ){ y <- In [, j ] # unit vector ej Snw [, j ] <- ksmooth ( cars $ speed , y , kernel = \"normal\" , bandwidth = 2 , x.points = x ) $ y Slp [, j ] <- fitted ( loess ( y ~ cars $ speed )) Sss [, j ] <- fitted ( smooth.spline ( cars $ speed , y )) } df.nw <- sum ( diag ( Snw )) df.lp <- sum ( diag ( Slp )) df.ss <- sum ( diag ( Sss )) 发现三个非参smoother的自由度不同, 所以上面画图的比较并没有意义, 为了让三者的自由度相同, 可以设定ksmooth/loess/smooth.spline的参数. 控制带宽, Lp的参数为 span , SS的参数为 spar ; 而指定想要的自由度则分别是 enp.target 和 df . cat ( \"let all 3 np smoother use the same df=\" , df.nw ) Slp <- Sss <- matrix ( 0 , nrow = n , ncol = n ) In <- diag ( n ) # identity matrix for ( j in 1 : n ){ y <- In [, j ] Slp [, j ] <- fitted ( loess ( y ~ cars $ speed , enp.target = df.nw )) Sss [, j ] <- fitted ( smooth.spline ( cars $ speed , y , df = df.nw )) } sum ( diag ( Slp )) sum ( diag ( Sss )) 发现SS的df参数使用以后控制的非常接近NW的df了, 不过lp的df还是不够接近, 用span来控制应该更准确一些, 为了找到合适的span, 用以下代码来寻找使得df=df.nw的span取值: dflp <- function ( span , val ){ for ( j in 1 : n ) Slp [, j ] <- loess ( In [, j ] ~ cars $ speed , span = span ) $ fitted return ( sum ( diag ( Slp )) - val ) } chosen_span <- uniroot ( dflp , c ( 0.2 , 0.5 ), val = df.nw ) $ root 如果不用这个循环计算的话, 可以用 sfsmisc 包里的 hatMat 函数: 参数 trace 取TRUE的话, 直接返回hat matrix的迹, 否则返回整个hat matrix. 需要把要计算的smoother包装成一个pred.sm函数传入, 这个函数接受x和y, 返回fitted数值. 例子: hatMat(cars $ speed , T, pred.sm = function(x,y) ksmooth(x, y, kernel=\"normal\", bandwidth=2, x.points=x) $ y ) hatMat(cars $ speed , T, pred.sm = function(x,y) fitted(loess(y~x, span=chosen_span)) ) hatMat(cars $ speed , T, pred.sm = function(x,y) fitted(smooth.spline(x, y, df=df.nw)) ) CV and Hat Matrix 为了预测smoother的performance, 用loo CV来估计MSE(mean sq err)的值. loo CV可以用下面这个通用函数得到(注意看对于参数的要求): ##' Calculates the LOO CV score for given data and regression prediction function ##' ##' @param reg.data: regression data; data.frame with columns 'x', 'y' ##' @param reg.fcn: regr.prediction function; arguments: ##' reg.x: regression x-values ##' reg.y: regression y-values ##' x: x-value(s) of evaluation point(s) ##' value: prediction at point(s) x ##' @return LOOCV score loocv <- function ( reg.data , reg.fcn ){ ## Help function to calculate leave-one-out regression values loo.reg.value <- function ( i , reg.data , reg.fcn ) return ( reg.fcn ( reg.data $ x [ - i ], reg.data $ y [ - i ], reg.data $ x [ i ])) ## Calculate LOO regression values using the help function above n <- nrow ( reg.data ) loo.values <- sapply ( 1 : n , loo.reg.value , reg.data , reg.fcn ) ## Calculate and return MSE return ( mean ( ( loo.values - reg.data $ y ) &#94; 2 ) ) } 比如, 为了计算NW的CV数值, 需要这样: regfcn.nw <- function ( regx , regy , x ) ksmooth ( regx , regy , kernel = \"normal\" , bandwidth = 2 , x.points = x ) $ y loocv ( data.frame ( x = cars $ speed , y = cars $ dist ), regfcn.nw ) 不过, 如果得到了hat Matrix S, 根据公式4.5, loo CV可以这样一次计算出来: 试一下: # compute CV using shortcut euqation yhat.nw <- Snw %*% cars $ y # or use regfcn.nw(cars$speed, cars$dist, cars$speed) mean ( ( ( cars $ y - yhat.nw ) / ( 1 - diag ( Snw )) ) &#94; 2 ) 得到的结果和之前用loocv一样, 都是253.9128 !~ 或者只用df, 计算generalized CV, 公式为: # compute GCV mean( (cars $ dist -yhat.nw)&#94;2 ) / ( 1 - df.nw/n )&#94;2 得到gcv=269.3911, 和looCV也比较接近.","tags":"notes","title":"R语言从入门到放弃 (4). 统计回归"},{"url":"http://x-wei.github.io/Rnotes-3-plot.html","text":"R关于绘图应该可以写很多, 不过这里只列举在compstat这门课里最经常用的几个函数. 关于R的绘图, 详细了解可以运行 demo(graphics) 或者 example(\"plot\") . R里面的绘图命令分为两类: 一类是\"high-level\"的\"创建新图片\"命令, 运行以后会新画一个图; 另一类则是\"low-level\"的命令, 不会创建新图片, 而只会在当前图片中修改添加(例如添加线条, 添加点等). 下面分别简单介绍, 最后再介绍其他一些绘图配置的命令. configurations/parameters 介绍一下常用的参数意义, 以及画图的配置. 详细文档见 ?par . 这些参数可以放在绘图命令中. par(mfrow=c(1,2)) : 这个命令是设定画图的布局, 把放置图片的区域分为一行两列, 第一个plot的图片在左边, 第二个在右边. col : 设定画图(线段/点)的颜色, 可以用数字(col=1, 2, ...)也可以用英文(col=\"red\", \"gray\", \"blue\"等) lty : 设定线段类型, 例如lty=1为实线, lty=2为虚线 pch : 设定点的类型(pch=\"point character\") cex : 似乎是设定文字大小的, 一般设置cex=0.6 high level命令: plot 创建新图片的命令主要就是plot这个函数 (其他还有如 hist , contour , boxplot , 不太常用, 看文档应该能会用). plot 的内容或者为一系列x和y坐标, 或者为一个formula(见plot.formula). : type : 指定画线( type=\"l\" )或者画点( type=\"p\" , 默认值). 当type=\"l\"的时候, 可以设定 lty 参数, type=\"p\"则可以设定 pch 参数. 当 type=\"n\" 的时候表示不画任何东西 — 然后可以用后面介绍的low level命令往里面添加东西. main : 设定图片的标题 xlab / ylab : x/y轴的文字 xlim/ylim : 设定x/y轴的范围, 例如: xlim=c(0,10) . 这个参数比较重要, 如果想要根据数据来动态调整的话, 可以使用extendrange函数, 它返回比数据range稍大一点的range: > extendrange(1:10) [1] 0.55 10.45 一个plot的例子: plot(x, y, type=\"l\", lty=2, xlab=\"x axis label\", ylab=\"y=10sin(x)+x\", main=\"sample plot\", xlim=c(-1, 101), ylim=extendrange(y) ) low level命令 points points(x, y) 添加(多个)点, 这些点的坐标在x和y中. 此时可以用 pch 参数指定点的类型. lines lines(x, y) 添加多条线段, 这些线段依次经过一系列的点, 而点的xy坐标放在x和y中. 此时可以设定 lty 指定线段类型. abline 这个函数作用是添加一条直线, 可以传入 lty 参数. abline有多种用法: abline(a=a, b=b) : 指定斜率和截距(y=ax+b) abline(h=y0) / abline(v=x0) : 画水平/垂直的线 abline(reg=lm_fit) : reg为有coeff的对象(如一个线性fit), 画出fitted line rug rug(x) 的作用是在x轴添加小线段, 位置为参数 x . lengend 添加图例, 直接看例子: legend(\"topleft\", c(\"data\", \"fitted\"), lty=1:2) 这样在左上角(其他可选项见文档)添加图例, 文字\"data\"对应lty=1的数据, \"fitted\"对应lty=2的. example 下面是一个low level绘图例子(在前一个图基础上添加各种东西): newpts.x <- c ( 20 , 30 , 40 , 50 ) newpts.y <- c ( 40 , 70 , 60 , 80 ) points ( newpts.x , newpts.y , pch = 'x' , col = \"blue\" ) lines ( newpts.x , newpts.y , lty = 3 , col = \"red\" ) abline ( h = 30 , col = \"gray\" ) abline ( v = 50 , col = \"blue\" , lty = 4 ) fit <- lm ( y ~ x ) # fit to linear model abline ( fit , lty = 1 ) legend ( \"topleft\" , c ( \"data\" , \"linear fit\" , \"lines example\" , \"vertical line\" ), lty = c ( 2 , 1 , 3 , 4 ), col = c ( rep ( \"black\" , 3 ), \"blue\" ))","tags":"notes","title":"R语言从入门到放弃 (3). 绘图"},{"url":"http://x-wei.github.io/Rnotes-2-seq_func.html","text":"首先, R似乎默认所有的变量都为向量vector, 即使一个单独的数字也是长度为1的, 所以 1 等价于 c(1) . > a <- 1 > a [ 1 ] 1 > length ( a ) [ 1 ] 1 > a [ 1 ] [ 1 ] 1 > typeof ( a ) [ 1 ] \"double\" # means \"double vector\" (I think) > 1 == c ( 1 ) [ 1 ] TRUE fancy indexing R的vector/list/matrix支持类似numpy(稍有不同)的fancy indexing, 以下是例子: > v <- 1 : 10 > v [ 1 : 3 ] # slicing [ 1 ] 1 2 3 > v [ - ( 1 : 3 )] # EXCLUDING first 3 elements [ 1 ] 4 5 6 7 8 9 10 > v [ -1 ] # EXCLUDING first element [ 1 ] 2 3 4 5 6 7 8 9 10 > v [ c ( 2 , 4 , 9 )] [ 1 ] 2 4 9 > v [ v %% 2 == 0 ] # indexing using logical array [ 1 ] 2 4 6 8 10 # works also for lists > l <- list ( 1 , 2 , 3 , \"aa\" ) > l [ -1 ] [[ 1 ]] [ 1 ] 2 [[ 2 ]] [ 1 ] 3 [[ 3 ]] [ 1 ] \"aa\" 以下是矩阵的fancy indexing例子: > mat <- matrix ( 1 : 9 , nrow = 3 , ncol = 3 ) > mat [, 1 ] [, 2 ] [, 3 ] [ 1 ,] 1 4 7 [ 2 ,] 2 5 8 [ 3 ,] 3 6 9 > mat [ 1 ,] [ 1 ] 1 4 7 > mat [ 1 , c ( 2 , 3 )] [ 1 ] 4 7 > mat [ -1 ,] [, 1 ] [, 2 ] [, 3 ] [ 1 ,] 2 5 8 [ 2 ,] 3 6 9 > mat [, 2 ] [ 1 ] 4 5 6 names / dimnames 好玩的是可以用 names / dimnames 函数给每个值加上一个名字: > names ( v ) <- paste ( \"elem\" , sep = \"-\" , 1 : length ( v )) > v elem -1 elem -2 elem -3 elem -4 elem -5 elem -6 elem -7 elem -8 elem -9 elem -10 1 2 3 4 5 6 7 8 9 10 > names ( v ) <- paste ( \"elem\" , sep = \"_\" , 1 : length ( v )) > v elem_1 elem_2 elem_3 elem_4 elem_5 elem_6 elem_7 elem_8 elem_9 elem_10 1 2 3 4 5 6 7 8 9 10 > v $ elem_1 Error in v $ elem_1 : $ operator is invalid for atomic vectors > v [ \"elem_1\" ] elem_1 1 > names ( l ) <- paste ( \"elem\" , sep = \"_\" , 1 : length ( l )) > l $ elem_1 [ 1 ] 1 > l [ 1 ] $ elem_1 [ 1 ] 1 > l [ \"elem_1\" ] $ elem_1 [ 1 ] 1 上面例子看到, vector不能使用 $ 来获得\"field\", 但是list可以, 这是list和vector的一个区别. 下面是矩阵的例子: > dimnames ( mat ) <- list ( paste ( \"row\" , sep = \"_\" , 1 : nrow ( mat )), paste ( \"col\" , sep = \"_\" , 1 : ncol ( mat )) ) > mat col_1 col_2 col_3 row_1 1 4 7 row_2 2 5 8 row_3 3 6 9 > mat [ \"row_1\" , \"col_1\" ] [ 1 ] 1 c() 关于 c 这个函数, 值得一提的除了它自动\"展开\"参数的list/vector以外(上次博客提到), 还有就是它会自动cast, 文档里是这么说的: The output type is determined from the highest type of the components in the hierarchy NULL < raw < logical < integer < double < complex < character < list < expression. 其中的logical, integer, character都属于(atomic) vector, list和他们不同, 见后文. 以下是例子: > c(1,2,TRUE) # logical < integer [1] 1 2 1 > c(1,2,\"char\") # integer < character [1] \"1\" \"2\" \"char\" > c(1,2,list(1)) # integer < list [[1]] [1] 1 [[2]] [1] 2 [[3]] [1] 1 vector VS list ( matrix VS data.frame ) 用于集合主要是vector和list, 他们的区别是: vector只能存放同样类型的元素, 而list可以存放不同类型的元素 . vector可以是: numeric, logical, char... 类比一下, vector类似java里的array, list类似python的list. 另外访问第i个元素, vector是 v[i] , 而list需要用两个括号 l[[i]] ( l[i] 还是一个list, l[[i]] 才是想要的东西... ) 看例子: > c(1,2,3) # numeric vector [1] 1 2 3 > c(1,2,\"a\") # c() 自动cast把前两个数字转成了char, 变成一个char类型的vector [1] \"1\" \"2\" \"a\" > list(1,2,\"a\") # list [[1]] [1] 1 [[2]] [1] 2 [[3]] [1] \"a\" 同理, matrix和data.frame也类似, matrix的所有元素必须相同, 而data.frame可以每一列各不相同(不过一列之中需要相同). 另外data.frame也支持用 $ 选取一列, matrix则不支持. functions R的函数定义为如下形式, 注意, 函数体的最后一句就是返回值, 不用显示写\"return\" (类似scala). myfunnction <- function ( params , ... ){ # ... the.return.value } 另外注意到上面函数定义, 参数里有三个点 ... , 这个不是必须的, 它的的作用见下一节. R是函数式语言: 一个function可以作为参数传递, 例子就是 apply , 见下一节. apply/lapply/sapply apply apply这个函数的doc写到用法为: apply(X, MARGIN, FUN, ...) X 是操作的数据( 一般为matrix ), MARGIN 为选择对行或列操作(类似numpy的 axis 参数), FUN 就是作为参数传入的函数了. > mat <- matrix ( 1 : 12 , nrow = 3 , ncol = 4 ) > mat [, 1 ] [, 2 ] [, 3 ] [, 4 ] [ 1 ,] 1 4 7 10 [ 2 ,] 2 5 8 11 [ 3 ,] 3 6 9 12 > apply ( mat , 1 , sum ) # apply on row [ 1 ] 22 26 30 > apply ( mat , 2 , sum ) # apply on col [ 1 ] 6 15 24 33 这里类似做reduce操作, 而MARGIN就是指定要reduce哪一个维度. 另外文档里的三个点 ... 很有意思, 它是 参数FUN的额外参数 ! 下面是一个例子, 给FUN传入了一个匿名函数: function(x,power) sum(x&#94;power) , 它计算x里元素的power次方, 然后加起来. 所以在apply里可以指定FUN这个 power 参数的数值, 这就对应着apply用法里的这三个点 ... . > apply(mat, 1, function(x,power) sum(x&#94;power), power=1 ) [1] 22 26 30 > apply(mat, 1, function(x,power) sum(x&#94;power), power=2 ) [1] 166 214 270 lapply/sapply 如果说上面 apply 一般用在matrix上, 用于将一个matrix reduce 为向量的话, lapply/sapply 就是 map 操作了: 作用在一个vector/list上, 返回对每一个元素进行操作后的新list. 它们的区别大概是: lapply返回list, sapply返回vector. > sq <- function ( x ) x &#94; 2 > l <- list ( 1 , 2 , 3 , 4 ) > v <- 1 : 4 > lapply ( v , sq ) # lapply returns a list [[ 1 ]] [ 1 ] 1 [[ 2 ]] [ 1 ] 4 [[ 3 ]] [ 1 ] 9 [[ 4 ]] [ 1 ] 16 > lapply ( l , sq ) # lapply returns a list [[ 1 ]] [ 1 ] 1 [[ 2 ]] [ 1 ] 4 [[ 3 ]] [ 1 ] 9 [[ 4 ]] [ 1 ] 16 > sapply ( v , sq ) # sapply returns a vector [ 1 ] 1 4 9 16 > sapply ( l , sq ) # sapply returns a vector [ 1 ] 1 4 9 16","tags":"notes","title":"R语言从入门到放弃 (2). 向量(列表)及函数"},{"url":"http://x-wei.github.io/Rnotes-1-basics.html","text":"这个\"从入门到放弃\"系列是为了应付eth的 computational statistics 这门课... 对R无爱... terminology 首先在stat里面有一些叫法和以前不太一样: predictor variable : 就是机器学习里面说的feature (Xi) design points : 是机器学习里的训练数据(X1...Xn) response variable : 要预测的变量(y) Rstudio 这个是用R编程的标配, 确实很方便, 不用可惜. 另外R markdown也不错, 可以边写markdown边运行代码, 建议编辑器窗口里新建rmd文件. 默认有四个窗口: 编辑器, 变量, 命令行, 画图/帮助. 快捷键 一些常用的快捷键: alt+- : 用于输入赋值符号 <- (编辑器窗口中) ctrl+enter : (在console里)执行光标所在的行, 等同于按钮\"run\" (r markdown编辑器窗口中) ctrl+alt+I : 插入一个R代码的chunk (r markdown编辑器窗口中) ctrl+shift+enter : 运行当前code chunk (选中一个函数名) F1 : 在右下角帮助窗口里显示相关文档. R 文档 获得文档也可以输入: ?funcname 或者 help(funcname) 输入 ??funcname 则是非精确匹配的查找. 另外获得例子可以使用 demo() / example() , 例如: demo(graphs) example(\"plot\") 下面是一个R文档的例子(运行 ?lm 得到): 章节Arguments里介绍了每个参数的意义和用法, 然后往下翻还有一个Values章节: Values里的每一项是这个函数的返回值(object)的内容, 比如要获得一个lm object的参数, 只需要: lm_obj$coefficients R 101 首先, R非常让我非常不习惯的一点是: 在R里面, dot . 只是一个普通的字符 , 它的用法和python里的下划线 _ 类似, 只是起到分割函数名里的单词而已... 在其他语言里, dot . 一般都是用来调用函数或者获取field的. 所以见到 cv.ss 之类的名字的时候别以为是变量cv的一个field... 这个就相当于python里一个叫 cv_ss 的变量... 获取一个object的field, 在R里面是用dollar $ (另外R里的object似乎并没有method!) . 另外, 赋值使用的是 <- , 等号似乎只在指定函数参数的时候用到. 常用函数 接下来列举一些常用的函数, 它们的详细用法可以看文档... c(...) combine values into vector/list. 注意的是它会自动把list/vector参数展开: > c(1,2,3) [1] 1 2 3 > c(c(1,2),3) [1] 1 2 3 利用这个性质, 可以这样向一个列表(向量)添加东西: l <- c(l, new_element) numeric(l) 生成长度为l的(全0)向量. 经常用这个命令生成一个\"数组\" 然后用一个循环向里面写入东西. > numeric(5) [1] 0 0 0 0 0 获取向量第i个元素: vec[i] (btw, R index是从1开始的) matrix(v, nrow=nr, ncol=nc) 生成一个矩阵, 全部初始化为v, nr行, nc列. 获取i行j列: mat[i,j] 获取第i行: mat[i,] 第j列: mat[,j] seq(from, to, by) 类似py的range函数... 另外 1:n 是 seq(1,n,by=1) 的简写 > seq(1,10, 2) [1] 1 3 5 7 9 > 1:5 [1] 1 2 3 4 5 for循环: for(i in 1:n){... rep 直接看例子: > rep(1, 10) [1] 1 1 1 1 1 1 1 1 1 1 > rep(c(1,2), 10) [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 > rep(c(1,2), each=10) [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 length(lst) 用来获得一个向量/列表的长度 dim(mat) 获得一个矩阵的维度, 类似numpy的 mat.shape cbind/rbind 按行/列合成矩阵 > cbind(c(1,2),c(3,4)) [,1] [,2] [1,] 1 3 [2,] 2 4 str / summary 输出一个obj的信息 cat / print 打印, 区别是 cat 可以依次打印多个字符串/对象 which / which.max / which.min which.max/min类似于argmax/min > which(2>c(1,2,1)) [1] 1 3 > which.max(c(1,2,1)) [1] 2 > which.min(c(1,2,1)) [1] 1 paste 这个函数可以用来给一个数组的前面加一个前缀, 看例子: > paste(\"a\", 1:5, sep=\"-\") [1] \"a-1\" \"a-2\" \"a-3\" \"a-4\" \"a-5\" operators: with % 一些运算符需要用%包裹起来, 比如: - %% : 取模 - %*% : 矩阵相乘 - %/% : 整数相除(类似java里int的除法) - a %in% b : 判断元素是否属于b","tags":"notes","title":"R语言从入门到放弃 (1). 基础"},{"url":"http://x-wei.github.io/double-hop-ssh.html","text":"问题: 现在本地ssh连接eth的daint服务器, 连接需要进行两次ssh: 第一次本地连接到ela服务器, 第二次从ela再次ssh到daint. 现在配置本地的ssh代理使之可以一次完成. 第一步: 生成sshkey ssh-keygen cat ~/.ssh/id_rsa.pub | ssh your_username@ela.cscs.ch 'cat >> ~/.ssh/authorized_keys' ssh your_username@ela.cscs.ch 第二步: 修改.ssh/config文件内容 用文本编辑器打开 .ssh/config 文件, 添加如下内容: Host daint Hostname daint101 User your_username ForwardAgent yes ForwardX11 yes Port 22 IdentityFile ~/.ssh/id_rsa ProxyCommand ssh -q -Y your_username@ela.cscs.ch netcat %h %p -w 10 直连第二层ssh 进行了以上配置以后, 连接到cscs就不再需要两层ssh的命令了, 直接: $ssh daint 即可 ! 用rsync同步远程文件夹 例子: 把本地文件夹同步到第二层ssh的目录下. rsync -a --progress ~/Downloads/nn_coref-master daint:/scratch/daint/your_username/code","tags":"soft","title":"两层ssh连接(2-hop ssh connection)代理配置"},{"url":"http://x-wei.github.io/progfun1_lec4_pattern_matching.html","text":"4.1 - Objects Everywhere scala is pure OO: every value is an obj, every operation is a method of obj. scala.Int scala.Boolean maps to JVM standard primitive types. Implement Boolean withous primitive type in scala: Then defin false and true as objects, give implementation for ifThenElse() funciton: object true extends Boolean{ def ifThenElse[T](t: =>T, e: =>T) = t } object false extends Boolean{ def ifThenElse[T](t: =>T, e: =>T) = e } 4.2 - Functions as Objects Function values are treated as objects in scala. the type A => B is an abbrevation for scala.Function1[A, B] Functions are objects with apply methods. anonymous function (x: Int) => x*x is expanded to : new Function1[Int, Int]{ def apply(x: Int) = x*x } function call f(a,b) is expanded to: f.apply(a,b) List(1,2) is expanded to : List.apply(1,2) 4.3 - Subtyping and Generics 2 forms of polymorphism: subtyping and generics Type Bounds assertAllPos taks either EmptySet or NonEmptySet , the return type is EmptySet (when para=EmptySet) or NonEmptySet (when para=NonEmpty). To express this situation: def assertAllPos[S <: IntSet](r: S): S = ... Here <: IntSet means IntSet is an upper bound of the type parameter. and we can mix the two notations: [S >: NonEmpty <: IntSet] Covariance Given NonEmpty <: IntSet , should we have: List[NonEmpty] <: List[IntSet] ? In this case the typse are called *covariant. * In java, arrays are covariant: NonEmpty[] <: IntSet[] but this might cause problems: will get ArrayStoreException in 3rd line. Liskov substitution principle : when a type can be a subtype of another If A<:B, then everything one can do with an object of type B, one should also be able to do it with object of type A. In scala the Array is not covariant: Array[IntSet] is not supertype of Array[NonEmpty] 4.5 - Decomposition ex: arithmetic expression interpreter: a tree of numbers and sums, both are subtype of Expr . 1st try: test and accessor methods → tedious... And to add more operands need to add many more methods... 2nd try: (non-solution) type testing and type casting isinstantceof , asinstanceof discouraged in scala. 1st solution: OO decomposition add eval method to class Expr. limitation: if we want to simplify an expression, ⇒ there is no local simplification ! ⇒ pattern matching 4.6 - Pattern Matching goal: reverse the construction process. which subclass it is? what were the arguments? case classes add case in class definition. ex. case class Number(n: Int) extends Expr case class Sum(e1: Expr, e2: Expr) extends Expr adding case adds companion objects with apply methods: according to the expansion, Number(2) is equal to Number.apply(2) which is actaully new Numver(2) pattern matching match : extension of switch in java, ex: def eval(e: Expr): Int = e match{ case Number(n) => n case Sum(e1, e2) => e1 + e2 } patterns include: constructor, ex. Number(n) variable, ex. n, e1, e2 wildcard patters, ex. _ constant, ex. 1, true or their combination, ex Sum(Number(1), Var(x)) The whole match expression is replaced with the expression to the rhs of the first match pattern. exercice: implement a show function. def show(e: Expr): String = e match { case Number(n) => n.toString case Sum(l, r) => show(l) + \"+\" + show(r) } 4.7 - Lists example: difference between List and Array: Lists are immutable, elements cannot change Lists are recursive, arrays are flat both are homogeneous, all elements of the same type All List s in scala are constructed with: Empty list Nil construction op :: (pronunced: cons), x::xs convention in scala: operators ending with : are right-associative . a::b::c is equal to a::(b::c) seen as method calls from the right-hand operand. 1::2::Nil equal to Nil.::(2).::(1) ( :: is prepend operation) operations on lists: head tail isEmpty list in pattern matching: ex. insertion sort of lists def isort(xs: List[Int]): List[Int] = xs match{ case List() => List() case y::ys => insert(y, isort(ys)) } def insert(x:Int, xs:List[Int]): List[Int] = xs match{ case List() => List(x) case y::ys => if(x<=y) x::xs else y::insert(y, ys) }","tags":"notes","title":"[Scala MOOC I] Lec4: Types and Pattern Matching"},{"url":"http://x-wei.github.io/progfun1_lec3_data_abstraction.html","text":"This week, we'll cover traits, and we'll learn how to organize classes into hierarchies. We'll cover the hierarchy of standard Scala types, and see how to organize classes and traits into packages. Finally, we'll touch upon the different sorts of polymorphism in Scala. 3.1 - Class Hierarchies abstract class abstract class IntSet { def incl(x: Int): IntSet def contains(x: Int): Boolean } abstract class: contains members without implementation cannot be created with new class Extensions implement the integer set abstract class with BST 2 types of trees: Empty and NonEmpty class Empty extends IntSet { override def incl ( x : Int ): IntSet = new NonEmpty ( x , new Empty , new Empty ) override def contains ( x : Int ): Boolean = false } class NonEmpty ( elem: Int , left: IntSet , right: IntSet ) extends IntSet { override def incl ( x : Int ): IntSet = if ( x == elem ) this else if ( elem > x ) new NonEmpty ( elem , left . incl ( x ), right )// immutable ! else new NonEmpty ( elem , left , right . incl ( x )) override def contains ( x : Int ): Boolean = if ( elem == x ) true else if ( elem > x ) left . contains ( x ) else right . contains ( x ) } root class of all classes: Object replace concrete defintion of super class: override is mandantory. Object in the previous example, seems only one single Empty set is needed. ⇒ define Empty as singleton object no other Empty instances can be created, object is a value. object Empty extends IntSet { override def incl(x: Int): IntSet = new NonEmpty(x, Empty, Empty) override def contains(x: Int): Boolean = false override def toString = \".\" } Program create standalone scala applicatoins. Each such applications contains an object with a main(args:Array[String]) method exercice: implement union union(other:IntSet): IntSet override def union(other: IntSet): IntSet = left union (right union (other incl elem)) why this recursive call terminates ? every call to union is on a smaller IntSet ! 写成这样就会stackoverflow: other.incl(elem).union(left).union(right) 讨论见: https://www.coursera.org/learn/progfun1/discussions/weeks/3/threads/AzJ-4CLYEeag6wpD-92Rcw (需要点\"view earlier replies\"才有) Dynamic Binding behavior depends on the runtime type of the object. ~ higher-order functions Lecture 3.2 - How Classes Are Organized package // named imports import week3.Rational import week3. { Rational , Hello } // wildcard import import week3._ can import either from a package or from an object automatically imported in scala: ex: scaladoc: http://www.scala-lang.org/files/archive/api/current/#package Traits in scala/java, a class has only one super class (single inheritance). have several supertypes? ⇒ trait! one class can extend many traits (concated by with ) — like java interface traits can contain fileds and concrete methods on the other hand, traits cannot have (value) parameters, but classes can scala class hierchy Any : base class of all AnyVal : primitive types ( Int, Unit, Boolean,... ) AnyRef : (=alias of java.lang.Object ) all reference types ( String, List, Seq, Iterable,... ) Nothing : bottom of the hierchy, is subtype of every type Null : subtype of every reference type, null 's type is Null , not compatible with AnyVal types. exception: throw Exc , the typeof Exc is Nothing exercice: if (true) 1 else false ⇒ type = AnyVal Lecture 3.3 - Polymorphism cons-list immutable linked list implement this in scala: the val in the class definition: defines at the same time parameter and field of a class , equivalent to: type parameters (generic) trait List[T]{ def isEmpty : Boolean def head: T def tail: List[T] } class Cons[T](val head:T, val tail: List[T]) extends List[T] { def isEmpty(): Boolean = false } head and tail are implemented in the parameters(fields), difference between val and def only consist in the initialization (CBN, CBV). class Nil [ T ] extends List [ T ]{ def isEmpty = false def head: Nothing = throw new NoSuchElementException ( \"Nil.head\" ) def tail: Nothing = throw new NoSuchElementException ( \"Nil.tail\" ) } use nothing as return type, and throw an exception. generic functions type parameters can be applied to functions. sometime the type parameter can be deduced by scala compiler. ex. singleton(2); singleton(true) Types and Evaulation type parameters don't affect evaluation. can assume type parameters are removed before the evaluation ( type erasure ). Polymorphism subtyping: instances of subclass can be passed to a base class generics: function/class with type parameters exercice: write a function nth(n: Int, list: List) def nth[T](n: Int, list:List[T]): T = if(list.isEmpty) throw new IndexOutOfBoundsException() else if (n==0) list.head else nth(n-1, list.tail) Programming Assignment: Object-Oriented Sets TweetSet : an abstract class TweetSet with two concrete subclasses, Empty which represents an empty set, and NonEmpty(elem: Tweet, left: TweetSet, right: TweetSet ), which represents a non-empty set as a binary tree rooted at elem. The tweets are indexed by their text bodies: the bodies of all tweets on the left are lexicographically smaller than elem and all bodies of elements on the right are lexicographically greater.","tags":"notes","title":"[Scala MOOC I] Lec3: Data and Abstraction"},{"url":"http://x-wei.github.io/progfun1_lec2_highorder_func.html","text":"This week, we'll learn about functions as first-class values, and higher order functions. We'll also learn about Scala's syntax and how it's formally defined. Finally, we'll learn about methods, classes, and data abstraction through the design of a data structure for rational numbers. 2.1 - Higher-Order Functions higher order functions: functions that takes functions as parameter or returns functions. example: ⇒ put the f as a parameter def sum(f:Int => Int, a: Int, b: Int):Int = { if(a>b) 0 else f(a) + sum(f, a+1, b) } function types A => B is a function that takes A as parameter and returns B . Anonymous functions \"literals\" for functions, syntactic sugar. ex. (x: Int, y: Int) => x+y (x: Int) => x*x use anon functions in previous sum() function: sum(x=>x, 1, 10) sum(x=>x*x, 1, 10) exercice: turn sum() into tailrec fashion. def sum2(f:Int => Int, a: Int, b: Int):Int = { @tailrec def sumTR(a: Int, acc: Int): Int = { if (a > b) acc else sumTR(a + 1, acc + f(a)) } sumTR(a, 0) } (note: using namescoping to eliminate parameters in inner functions) 2.2 - Currying define a function that returns a funtion def sum3(f:Int => Int): (Int, Int)=>Int = { def sumF(a:Int, b:Int):Int = { if(a>b) 0 else f(a) + sum3(f, a+1, b) } sumF } when calling this function: sum3(cube)(1,10) syntactic sugar: shorter version of functions that return functions: def sum3(f:Int => Int)(a:Int, b:Int):Int = { if(a>b) 0 else f(a) + sum3(f)(a+1, b) } question: what is type of sum3 ? → Int => Int => (Int, Int) => Int note: functional types are associated to the right , Int => Int => Int is equivalent to Int => (Int => Int) \"currying\" exercice: def product(f: Int => Int)(a: Int, b: Int): Int = { if (a > b) 1 else a * product(f)(a + 1, b) } product(x=>x)(1,10) def fact(n:Int) = product(x=>x)(1,n) fact(10) def more_general(op: (Int,Int) => Int, default: Int) (f: Int=> Int)(a:Int, b:Int):Int = { if(a>b) default else op(a, more_general(op, default)(f)(a + 1, b)) } more_general((x,y)=>x+y, 0)(x=>x)(1,10) 老师的解法: def map_reduce(f:Int=> Int, op:(Int, Int)=>Int, default:Int) (a:Int, b:Int):Int = { if(a>b) default else op(f(a), map_reduce(f, op, default)(a+1,b)) } def factorial2(n:Int):Int = map_reduce(x=>x, (a,b)=> a*b, 1)(1,n) factorial2(10) 2.3 - Example: Finding Fixed Points find the fix point of a function: x = f(x) val tol = 0.001 def isCloseEnough(x:Double, y:Double):Boolean = math.abs((x-y)/x)<tol def fixedPoint(f: Double=>Double)(firstGuess:Double):Double = { def iterate(guess:Double):Double = { if(isCloseEnough(guess, f(guess))) guess else iterate(f(guess)) } iterate(firstGuess) } fixedPoint(x=>1+x/2)(1) using the fixepoint function for sqrt : sqrt(x) = y such that: x=y*y =y such that y = x/y = fixed point for the function f(y)=x/y def sqrt(x:Double):Double = fixedPoint(y=>x/y)(1) sqrt(2) ⇒ doesn't converge! ⇒ guess oscillates between 1 and 2... average damping: prevent the estimate from varying to much. ⇒ by taking the average of successive values def sqrt2(x: Double): Double = fixedPoint(y => (y + x / y) / 2)(1) abstract this damping technique: def avgDamping(f:Double=> Double)(x:Double):Double = (f(x)+x)/2 def sqrt2(x: Double): Double = fixedPoint(avgDamping(y=>x/y))(1) summary: The highest level of abstraction is not always the best, but it is important to know the techniques of abstraction, so as to use them when appropriate. 2.4 - Scala Syntax Summary notations(EBNF): | denotes an alternative [...] an option (0 or 1) {...} a repetition (0 or more) Types Expressions Definitions 2.5 - Functions and Data example. rational numbers (x/y) define a class: class Rational ( x : Int , y:Int ){ def numer = x def denom = y } val x = new Rational ( 1 , 2 ) x . numer x . denom this definition creates both a class and the constructor. now implement arithmetic: def add(that: Rational) = new Rational(numer * that.denom + denom * that.numer, that.denom * denom) def neg = new Rational(-numer, denom) def sub(that: Rational) = add(that.neg) override def toString = numer + \"/\" + denom 2.6 - More Fun With Rationals simplify the rationals at construction : add private members: private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) private val g = gcd(x, y) def numer = x/g def denom = y/g other options: replace g with gcd(x,y) turn numer and denom into val add less and max function: def less(that:Rational) = this.numer*that.denom < this.denom*that.numer def max(that:Rational) = if(this.less(that)) that else this preconditions ex: avoid divide by 0. require(y!=0, \"denominator must be non zero\") → java.lang.IllegalArgumentException: requirement failed: denominator must be non zero require is a test to perform when the class is initialized. similar: assert() constructor in scala a class implicitly introduces a primary constructor: takes parameters of the class executes all statements in the class body to add another constructor: def this(x:Int) = this(x,1) exercice: override def toString = { val g = gcd(numer, denom) numer/g + \"/\" + denom/g } 2.7 - Evaluation and Operators evaluation for class/object extend the substitution model to classes and objects examples: operator overloading infix ops any method with one parameter can be used as an infix operator. scala identifiers can bu symbolic: ⇒ change names to +, <, -, use in this way: x + y x < y x max y x - y - z unitary ops now change the neg method: prefix operator, and might be confused with the sub( - ) ⇒ it's name is special: unary_- def unary_- = new Rational(-numer, denom) precedence of ops the precedence of an op is defined by its first letter order (by increasing precedence): quite the same as in java Programming Assignment: Functional Sets Mathematically, we call the function which takes an integer as argument and which returns a boolean indicating whether the given integer belongs to a set, the characteristic function of the set. For example, we can characterize the set of negative integers by the characteristic function (x: Int) => x < 0. Therefore, we choose to represent a set by its characterisitc function and define a type alias for this representation: type Set = Int => Boolean def contains(s: Set, elem: Int): Boolean = s(elem)","tags":"notes","title":"[Scala MOOC I] Lec2: Higher Order Functions"},{"url":"http://x-wei.github.io/progfun1_lec1_fun_and_eval.html","text":"In this week, we'll learn the difference between functional imperative programming. We step through the basics of Scala; covering expressions, evaluation, conditionals, functions, and recursion 1.1 - Programming Paradigms imperative programming: modify mutable variables using assignments control structures: if-else, loops, break, continue, return, etc. ~~~> Von Neumann computer: conceptualize data structures word-by-word. ⇒ need higher level abstractions (theories). theory doesn't describe mutations! ex. theories of polynomials To implement high-level concepts following their math theories, there's no place for mutation . Functional programming functions in FP language are 1st-class citizens. 1.2 - Elements of Programming in a language: primitive expressions operators to combin expressions abstract expressions: introduce a name for an expression primitive types in scala are written capatilized: REPL interactive shell: write expressions and respond its value Evaluation evaluation: expression → value evaluation stops onces it results in a value. evaluate non-primitive expressions: take the left-most operator take (left and right) operands apply the operator to operands evaluate names: replace the name with the rhs of its definition. definitions can have parameters. ex. variable/return types are after their names, seperated by a : def power(x: Double, y: Int): Double = ... evaluate a parameterized function: This scheme is called \" substition model \" — formalized in the lambda calculus. This can be applied as long as there is no side effects (ex. modify variable c++ ) . Termination Q: does every expression evaluates to a value? NO, ex. def loop: Int = loop change evaluation strategy do not reduce argument values before rewrite function application. call-by-name and call-by-value the above evaluation schemes. call-by-value: reduce argument values before rewrite function application. advantage: it evaluates every function argument only once. call-by-name: do not reduce arg values advantage: a function argument is not evaluated if the corresponding parameter is unused in the evaluation of the function body. ex. 1.3 - Evaluation Strategies and Termination CBV and CBN: ex. def first(x: Int, y: Int) = x first(1, loop) in scala: normally use call-by-value (more efficient than call-by-name because it avoid repeated computation of CBN). but: if a => before a parameter type , that parameter is called by-name . ex. def constOne(x: Int, y: => Int) = 1 constOne(1+2, loop) constOne(loop, 1) 1.4 - Conditionals and Value Definitions conditional expressions the if-else expression the if-else is an expression, not statement. ex. def abs(x: Int): Int = if (x>=0) x else -x the x>=0 is a boolean expression, sometimes called predicates . CBN and CBV for definition def form is by name, its rhs is evaluated at each use val is by value, evaluted at the definiton ex. scala> def loop:Boolean = loop loop: Boolean scala> def x = loop x: Boolean scala> val x = loop Execution interrupted by signal. exercice: implement and(x,y) scala> def and(x:Boolean, y: => Boolean) = if(x) y else false and: (x: Boolean, y: => Boolean)Boolean scala> and(false, loop) res3: Boolean = false 1.5 - Example: square roots with Newton's method def sqrt(x: Double): Double = ... need helper (recursive) functions. note: in scala the return type of recursive functions must be given. def improve(guess: Double, x: Double): Double = (x / guess + guess) / 2.0 def isGoodGuess(guess: Double, x: Double): Boolean = math.abs((guess * guess - x)/x)<0.001 def sqrtItr(guess: Double, x: Double): Double = { if (isGoodGuess(guess, x)) guess else sqrtItr(improve(guess, x), x) } def sqrt(x:Double): Double = sqrtItr(1, x) 1.6 - Blocks and Lexical Scope nested functions put aux functions inside a function. def sqrt(x:Double): Double = { def improve(guess: Double, x: Double): Double = (x / guess + guess) / 2.0 def isGoodGuess(guess: Double, x: Double): Boolean = math.abs((guess * guess - x)/x)<0.001 def sqrtItr(guess: Double, x: Double): Double = { if (isGoodGuess(guess, x)) guess else sqrtItr(improve(guess, x), x) } sqrtItr(1, x) } blocks and visibility blocks: delimited by braces {...} The last element of a block is an expression that defines its value. Blocks are themselves expressions; a block may appear everywhere an expression can . ex. val x = 0 def f(y: Int) = y +1 val result = { val x = f(3); x * x } + x // result = 16 ex. eliminate the parameter x of helper functions inside the sqrt function def sqrt(x:Double): Double = { def improve(guess: Double): Double = (x / guess + guess) / 2.0 def isGoodGuess(guess: Double): Boolean = math.abs((guess * guess - x)/x)<0.001 def sqrtItr(guess: Double): Double = { if (isGoodGuess(guess)) guess else sqrtItr(improve(guess)) } sqrtItr(1) } about multiline expressions: put the operator in the first line to avoid ambguity. 1.7 - Tail Recursion review: evaluation of function application. ex1. gcd def gcd(a:Int, b:Int): Int= if(b==0) a else gcd(b, a%b) ex2. factorial def factorial(n:Int): Int = if(n==0) 1 else n*factorial(n-1) difference from the gcd case: in gcd , the expression is reduced to a gcd() in factorial , add one more element to the expression in each step ! tail recursion if a function calls itself as its last action , then the function's stack can be reused. ⇒ tail recursion functions are iterative process. In general, if the last action of a function consists of calling a function (which may be the same) , one stack frame would be sufficient for both functions. Such calls are called tail-calls . tail-recursive function: more efficient can avoid stackoverflow might not be as clear as a non-tailrec version add @tailrec annotation to the function, if its not tailrec, an error will be shown. @tailrec def gcd(a: Int, b: Int): Int = ... exercice: turn factorial into tailrec fashion. → idea: use an accumulator (partial result) acc . def factorialTR(n:Int):Int = { @tailrec def fact(n:Int, acc:Int):Int = { if(n==0) acc else fact(n-1, n*acc) } fact(n, 1) } Programming Assignment: Recursion 三道递归的练习题... /** * Exercise 1 */ def pascal(c: Int, r: Int): Int = { if (c == 0 || c == r) 1 else pascal(c, r - 1) + pascal(c - 1, r - 1) } /** * Exercise 2 */ def balance(chars: List[Char]): Boolean = { def balance_rec(chars: List[Char], acc: Int): Boolean = { if (acc<0) false else if (chars.isEmpty) acc == 0 else if (chars.head == '(') balance_rec(chars.tail, acc+1) else if (chars.head == ')') balance_rec(chars.tail, acc-1) else balance_rec(chars.tail, acc) } balance_rec(chars, 0) } /** * Exercise 3 */ def countChange(money: Int, coins: List[Int]): Int = { if(money==0 ) 1 else if(coins.isEmpty || money<0) 0 else countChange(money, coins.tail) + countChange(money-coins.head, coins) }","tags":"notes","title":"[Scala MOOC I] Lec1: Functions & Evaluation"},{"url":"http://x-wei.github.io/progfun1_lec0_setup.html","text":"Get up and running with Scala on your computer. Complete an example assignment to familiarize yourself with our unique way of submitting assignments. Tool setup IntelliJ use worksheet as a better REPL SBT navigate to the directory of the assignment you are working on, then start sbt . (when first running sbt , will take 5~10 minutes to download files...) REPL type console to enter scala REPL, hit ctrl-d to exit REPL. Compile / run / test compile : The compile task will compile the source code of the assignment which is located in the directory src/main/scala . test : The directory src/test/scala contains unit tests for the project. In order to run these tests in sbt, you can use the test command. run : If your project has an object with a main method (or an object extending the trait App), then you can run the code in sbt easily by typing run. In case sbt finds multiple main methods, it will ask you which one you'd like to execute. submit submitting assignments in sbt: submit your@email.com YourSubmissionPassWord Scala tutorial Classes, Traits, Objects and Packages Classes Classes in Scala are very similar to classes in Java. They are templates containing fields and methods. Like in Java, classes can be instantiated using the new construct, there can be many \"instances\" (or \"objects\") of the same class. In Scala there exists a special kind of class named case classes . You will learn about case classes during the course. Classes in Scala cannot have static members . You can use objects (see below) to achieve similar functionality as with static members in Java. Traits Traits are like interfaces in Java, but they can also contain concrete members , i.e. method implementations or field definitions. Objects Object in Scala are like classes, but for every object definition there is only one single instance. It is not possible to create instances of objects using new , instead you can just access the members (methods or fields) of an object using its name. Packages Adding a statement such as package foo.bar at the top of a file makes the code in a file part of the package foo.bar. You can then do import foo.bar._ to make everything from package foo.bar available in your code. The content of a package can be scattered across many files. If you define a class MyClass in package foo.bar, you can import that specific class (and not anything else from that package) with import foo.bar.MyClass. In Scala, everything can be imported, not only class names . So for instance if you have an object baz in package foo.bar, then import foo.bar.baz._ would import all the members of that object. Hello, World! in Scala In Scala, the main or entry point method is defined in an object . An object can be made executable by either adding extending the type App or by adding a method def main(args: Array[String]) . Here are two ways to define a program which outputs \"Hello, World!\" in Scala: object HelloWorld extends App { println(\"Hello, World!\") } or: object HelloWorld { def main(args: Array[String]) { println(\"Hello, World!\") } } Source Files, Classfiles and the JVM Scala source code is stored in text files with the extension .scala . Typically Scala programmers create one source file for each class, or one source file for a class hierarchy: In fact, Scala allows multiple classes and objects to be defined in the same source file . The name of a Scala source file can be chosen freely , but it is recommended to use the name of a class which is defined in that file. Package hierarchies should be reflected in directory structure: a source file defining class C in package foo.bar should be stored in a subdirectory as foo/bar/C.scala. Scala does not really enforce this convention, but some tools such as the Scala IDE for eclipse might have problems otherwise. The scala compiler compiles .scala source files to .class files, like the Java compiler. Classfiles are binary files containing machine code for the Java Virtual Machine. In order to run a Scala program, the JVM has to know the directory where classfiles are stored. This parameter is called the \"classpath\". If you are using eclipse or sbt to compile and run your Scala code, you don't need to do any of the above manually - these tools take care of invoking the Scala compiler and the JVM with the correct arguments. Scala Style Guide style checker: http://www.scalastyle.org/ (in IntelliJ: You can enable scalastyle in Intellij by selecting Settings->Editor->Inspections, then searching for Scala style inspections.) Avoid Casts and Type Tests : Never use isInstanceOf or asInstanceOf - there's always a better solution. Indentation Line Length and Whitespace Use local Values to simplify complex Expressions Choose meaningful Names for Methods and Values Common Subexpressions *Don't Copy-Paste Code!: factor out common parts into separate methods instead of copying code around. * Scala doesn't require Semicolons Don't submit Code with \"print\" Statements: the final code should be free of debugging statements. Avoid using Return : often don't need to use explicit returns. Avoid mutable local Variables : You can often rewrite code that uses mutable local variables to code with helper functions that take accumulators. Eliminate redundant \"If\" Expressions Cheatsheet https://github.com/lampepfl/progfun-wiki/blob/gh-pages/CheatSheet.md Example Assignment implementation implement max and sum method for List[Int] . trick: use recursion. def sum(xs: List[Int]): Int = { if (xs.isEmpty) 0 else xs.head + sum(xs.tail) } def max(xs: List[Int]): Int = { if (xs.isEmpty) throw new java.util.NoSuchElementException() max(xs, Int.MinValue) } def max(xs: List[Int], m: Int): Int = { if (xs.isEmpty) m else if (xs.head > m) max(xs.tail, xs.head) else max(xs.tail, m) } ScalaTest A test suite is simply a collection of individual tests for some specific component of a program. A test suite is created by defining a class which extends the type org.scalatest.FunSuite . When running ScalaTest, it will automatically find this class and execute all of its tests. You have two options for running this test suite: - Start the sbt console and run the \" test \" command - Right-click this file in eclipse and chose \"Run As\" - \"JUnit Test\" Tests are written using the test operator which takes two arguments: A description of the test. This description has to be unique, no two tests can have the same description. The test body, a piece of Scala code that implements the test The most common way to implement a test body is using the method assert which tests that its argument evaluates to true . So one of the simplest successful tests is the following: test(\"one plus one is two\")(assert(1 + 1 == 2)) In Scala, it is allowed to pass an argument to a method using the block syntax, i.e. { argument } instead of parentheses (argument) . This allows tests to be written in a more readable manner: test(\"one plus one is three?\") { assert(1 + 1 == 3) // This assertion fails! Go ahead and fix it. } One problem with the previous (failing) test is that ScalaTest will only tell you that a test failed, but it will not tell you what was the reason for the failure. The output looks like this: {{{ [info] - one plus one is three? *** FAILED *** }}} This situation can be improved by using a special equality operator === instead of == (this is only possible in ScalaTest). So if you run the next test, ScalaTest will show the following output: {{{ [info] - details why one plus one is not three *** FAILED *** [info] 2 did not equal 3 (ListsSuite.scala:67) }}} We recommend to always use the === equality operator when writing tests. In order to test the exceptional behavior of a methods, ScalaTest offers the intercept operation. In the following example, we test the fact that the method intNotZero throws an IllegalArgumentException if its argument is 0 . def intNotZero(x: Int): Int = { if (x == 0) throw new IllegalArgumentException(\"zero is not allowed\") else x } test(\"intNotZero throws an exception if its argument is 0\") { intercept[IllegalArgumentException] { intNotZero(0) } } It is allowed to have multiple assert statements inside one test, however it is recommended to write an individual test statement for every tested aspect of a method. test(\"sum of a few numbers\") { assert(sum(List(1,2,0)) === 3) } test(\"sum of empty list\"){ assert(sum(List())===0) } test(\"sum of negative numbers\"){ assert(sum(List(-1,-1,-1)) === -3) } test(\"max of a few numbers\") { assert(max(List(3, 7, 2)) === 7) } test(\"max of empty list throws NoSuchElementException\"){ intercept[NoSuchElementException]{ max(List()) } }","tags":"notes","title":"[Scala MOOC I] Lec0: Getting Started"},{"url":"http://x-wei.github.io/dlMOOC_L4.html","text":"problems with text: often very rare word is important, e.g. retinopathy ambiguity: e.g. cat and kitty → need a lot of labeled data ⇒ not realistic. ⇒ unsupervised learning similar words appear in similar context. embedding: map words to small vectors measure the closeness by cosine distance: word2vec initial: random vector → train model to predict nearby word. pb: too many words in dictionary → softmax too slow ⇒ random sample the non-target words tSNE dimension reduction (not PCA) that preserves the neighborhood structure (close vector → close in 2d as well). RNN treat varaible length sequences of words. use the current word (Xi) and the last prediction as input. backprop for RNN apply highly correlated derivatives to W → not good for SGD. pb if we use highly correlated updates: grad either explod or it disappear quickly. fix grad-exploding: clip grad-vanishing: memory loss in RNN ⇒ LSTM LSTM in RNN: replace the NN by a LSTM cell represent the system with memory by a diagram with logical gates: change the decision variables to continous: a logistic regression in each gate: controls when to remember and when to forget things. http://blog.csdn.net/dark_scope/article/details/47056361 regularization for LSTM: L2 regularization: OK dropout: OK when used for input/output (X and Y), but NOT use to the recurrent in/out. beam search beam search is for generating sequences by RNN. Greedy approach: at each step, sample from the predicted distribution of the RNN. smarter approach: predict more steps and pick the seq with largest proba. pb with this: the number of possible seq grows exponentially ⇒ just keep the few most promising seqs → \" Beam search\" seq to seq RNN: model to map vaiable length seq to fix-length vectors. Beam search: sequence generation (map fix-length vectors to seq) concat them together: seq to seq system e.g. translation, speech recognation, image captionning","tags":"notes","title":"(DeepLearning MOOC) Lesson 4: Deep Models for Text and Sequences"},{"url":"http://x-wei.github.io/dlMOOC_L3.html","text":"statistical invariance → weight sharing e.g. image colors, translation invariance... convnet is NNs that share their weights across space. convolution: slide a small patch of NN over the image to produce a new \"image\" convnet forms a pyramid, each \"stack of pincake\" get larger depth and smaller area. convolutional lingo def. patch (kernel) small NN that slides over the image. def. depth number of pincakes in stack. def. feature map each \"pincake\" in stack. def. stride nb of pixels that you shift each time you move your filter. e.g. stride=1 → output almost the same size as the input; stride=2 → output about half size def. padding the way you treat the edge of image. valid padding : don't go pass the edge same padding : go off the image and pad with 0s (output size=input size) once got \"deep and narrow\" representation by convolution, connect to a normal (regular) fully-conncected NN. pooling better way to reduce the spatial extend (i.e. size) of the feature map. simple convnet: use large stride to reduce the feature map size. ⇒ aggressive pooling : use small stride (ex. stride=1), then take convolutions in neighbourhood and combine them . max pooling average pooling 1x1 convolution classic convolution = linear classifier over a small patch of image add a 1x1 convolution in the middle ⇒ a mini-dnn over the patch. cheap: not convolution, just matrix multiplication. inception module between each layers, just do both pooling and 1x1 conv, and 3x3 and 5x5.. conv, and concatenate them together. benefit: total number of parameters is small, yet performance better.","tags":"notes","title":"(DeepLearning MOOC) Lesson 3: Convolutional Neural Networks"},{"url":"http://x-wei.github.io/dlMOOC_L2.html","text":"Linear models matrix multiplication: fast with GPU numerically stable cannot cocatenate linear units → equivalent to one big matrix... ⇒ add non-linear units in between rectified linear units (RELU) chain rule: efficient computationally back propagation easy to compute the gradient as long as the function Y(X) is made of simple blocks with simple deritivates. most deep-learning framework can do it automatically for you. N.B. The backprop block takes 2x memory/compute wrt the forward prop blocks. first neural network: RELU units between linear classifiers: Tensor flow tensors define computations, and they are nodes in a computation graph . To actually run the optimization, use sessions ... define a computation graph: batch_size = 128 num_hidden = 1024 graph = tf.Graph() with graph.as_default(): # Input data. For the training data, we use a placeholder that will be fed # at run time with a training minibatch. tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables for linear layer 1 W1 = tf.Variable( tf.truncated_normal([image_size * image_size, num_hidden])) b1 = tf.Variable(tf.zeros([num_hidden])) # Hidden RELU input computation y1 = tf.matmul(tf_train_dataset, W1) + b1 # Hidden RELU output computation X1 = tf.nn.relu(y1) # Variables for linear layer 2 W2 = tf.Variable( tf.truncated_normal([num_hidden, num_labels]))#W2 b2 = tf.Variable(tf.zeros([num_labels])) #b2 # logit (y2) output logits = tf.matmul(X1, W2) + b2 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) def getlogits(X): y1 = tf.matmul(X, W1) + b1 X1 = tf.nn.relu(y1) return tf.matmul(X1, W2) + b2 # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( getlogits(tf_valid_dataset) ) test_prediction = tf.nn.softmax( getlogits(tf_test_dataset)) run sgd optimization: num_steps = 3001 with tf.Session(graph=graph) as session: tf.initialize_all_variables().run() print(\"Initialized\") for step in range(num_steps): # Pick an offset within the training data, which has been randomized. # Note: we could use better randomization across epochs. offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # Generate a minibatch. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # Prepare a dictionary telling the session where to feed the minibatch. # The key of the dictionary is the placeholder node of the graph to be fed, # and the value is the numpy array to feed to it. feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) Regularization early termination: stop when cannot improve in validation performance. L2 regularization: adding L2 norm of Dropout def. activation is the output of last layer that flows into the next layer. dropout: randomly set half of activations to 0 . rational: forcing your model to learn reduadant representations (consus over an ensemble of nns...)... N.B. for evaluation no longer dropout, ye = average of activations, trick to let ye=E(yt) , in training, multiply the remaining activations by 2.","tags":"notes","title":"(DeepLearning MOOC) Lesson 2: Deep Neural Networks"},{"url":"http://x-wei.github.io/dlMOOC_L1.html","text":"这是udacity上deeplearning的笔记, 做得非常粗糙, 而且这门课也只是介绍性质的... https://www.udacity.com/course/deep-learning--ud730 Softmax function socres yi ⇒ probabilities pi property: smaller scores ⇒ less certain about result Onehot encoding Cross entropy measure how well the probability vector S corresponds to the label vector L . ⇒ cross entropy D(S,L) ( D>=0, the smaller the better) N.B. D(S,L) is not symmetric (never log 0 ) recap (\"multinominal logistic classificaton\"): Minimizing cross entropy take avg D as loss function: ⇒ optimization, for example, by grad-desc: for the moment, take the optimizer as black box. two practical problems: how to feed img pixels to classifiers how to initialize the optimization numerical stability adding very small values to very large values will introduce a lot of errors ! ex. >>> a = 1e9 >>> for _ in xrange(1000000): ... a += 1e-6 >>> a - 1e9 0.95367431640625 ⇒ the result is not 1... ⇒ normalize input ! ⇒ 0 mean, 1 variance this make optimizers easier to find optimum. normalization for images: weight initialization draw init w/b from a Gaussian(0, sigma) , sigma → magtitude of initial output. small sigma means small outputs → uncertain about result. ⇒ take small sigma for initialization recap: ⇒ feed this loss fcn to the optimizer training, validation and test dataset rule of thumb (30) : a change that affects 30 examples in the validation set is statically significant. ⇒ in most cases use >30000 samples in validation set → changes in 0.1% is significant. SGD rule of thumb: computing grad(L) takes 3x time than computing loss fcn L . → pb for scaling.. SGD is the only fast enough model in practice. tricks to help SGD: normalize data (0 mean, uni-var) randomly initialize weights momentum learning rate decay Momentum SGD: many small steps in random directions → general direction is more accurate. ⇒ keep a running average of the gradients Learning rate decay take smaller and smaller steps (alpha decays) e.g. alpha decays exponentially... parameter tuning how quickly you learning != how well you train.. balck magics in deep learning: Adagrad variant of SGD, implicitly decays momentum and learning rate. recap:","tags":"notes","title":"(DeepLearning MOOC) Lesson 1: From Machine Learning to Deep Learning"},{"url":"http://x-wei.github.io/codejam-python-tools.html","text":"总结一下用python撸codejam时常用的一些库, 并且给一些简单的例子. 发现用python撸codejam非常合适: codejam的时间要求不严格(4/8分钟), 而且程序只要本地运行. 正好可以使用python简洁的语法和丰富的函数库. collections py自带的一些好用的数据结构... https://docs.python.org/2/library/collections.html from collections import Counter, deque, defaultdict itertools 主要是用来穷举的时候它里面一些函数很好用... https://docs.python.org/2/library/itertools.html >>> from itertools import product , combinations >>> a = 'ABCD' ; b = 'EFG' >>> for p in product ( a , b ): print p ... ( 'A' , 'E' ) ( 'A' , 'F' ) ( 'A' , 'G' ) ( 'B' , 'E' ) ( 'B' , 'F' ) ( 'B' , 'G' ) ( 'C' , 'E' ) ( 'C' , 'F' ) ( 'C' , 'G' ) ( 'D' , 'E' ) ( 'D' , 'F' ) ( 'D' , 'G' ) >>> for c in combinations ( a , 2 ): print c ... ( 'A' , 'B' ) ( 'A' , 'C' ) ( 'A' , 'D' ) ( 'B' , 'C' ) ( 'B' , 'D' ) ( 'C' , 'D' ) >>> for p in permutations ( b , 2 ): print p ... ( 'E' , 'F' ) ( 'E' , 'G' ) ( 'F' , 'E' ) ( 'F' , 'G' ) ( 'G' , 'E' ) ( 'G' , 'F' ) bitmap 聪明一点的穷举需要用bitmap... 实测可以加速十倍... use bitmap for combinations (2&#94;N possibilities) (N elements, each element 2 choices) for mask in xrange(1<<N): ... set/clean Kth bit set: bm |= 1<<k clean: bm &= ~(1<<k) count nb of 1s in a bitmap bin(bm).count('1') networkx 常用的图论算法都在里面了. nx最棒的是 任何hashable的object都可以用来作为节点的index , 再想想用C++的bgl, 简直蛋疼... https://networkx.readthedocs.io/en/stable/ constructing graph >>> import networkx as nx >>> G = nx . DiGraph () # use `Graph` for undired graph, `MultiGraph` for dup-edges >>> G . add_node ( 'a' ) # any hashable obj can be used as node index >>> G . add_edge ( 1 , 2 ) # missing nodes will be automatically added >>> G . add_edge ( 1 , 3 ) # if G is undired(`Graph`), 1-->3 and 3-->1 will be added >>> G . nodes () [ 'a' , 1 , 2 , 3 ] >>> G . edges () [( 1 , 2 ), ( 1 , 3 )] >>> G . add_edge ( 1 , 2 ); G . add_node ( 'a' ) # nx ignores duplicate adding edges/nodes >>> G . nodes () [ 'a' , 1 , 2 , 3 ] >>> G . edges () [( 1 , 2 ), ( 1 , 3 )] >>> G [ 1 ] # outgoing edges from a node { 2 : {}, 3 : {}} >>> G [ 1 ][ 2 ][ 'color' ] = 'blue' # easily add edge properties >>> G [ 1 ] { 2 : { 'color' : 'blue' }, 3 : {}} >>> G . add_edge ( 1 , 2 , capacity = 1 ) # this is another way to add property >>> G . edge { 'a' : {}, 1 : { 2 : { 'color' : 'blue' , 'capacity' : 1 }, 3 : {}}, 2 : {}, 3 : {}} >>> G . node [ 'a' ][ 'cat' ] = 'string node' # can also be: G.add_node('a', cat='string node') >>> G . node { 'a' : { 'cat' : 'string node' }, 1 : {}, 2 : {}, 3 : {}} DiGraph: topo-sort, cycle-detection, strongly connected component http://networkx.readthedocs.io/en/stable/reference/algorithms.shortest_paths.html >>> import networkx as nx >>> G = nx . DiGraph () >>> G . add_edge ( 1 , 2 ); G . add_edge ( 1 , 3 ); G . add_edge ( 'a' , 'b' ) >>> list ( nx . strongly_connected_components ( G ) ) [ set ([ 'b' ]), set ([ 'a' ]), set ([ 2 ]), set ([ 3 ]), set ([ 1 ])] >>> nx . topological_sort ( G ) [ 1 , 2 , 3 , 'a' , 'b' ] >>> G . add_edge ( 2 , 3 ); G . add_edge ( 3 , 1 ); G . add_edge ( 'b' , 'a' ) >>> list ( nx . simple_cycles ( G ) ) [[ 1 , 3 ], [ 1 , 2 , 3 ], [ 'a' , 'b' ]] >>> list ( nx . strongly_connected_components ( G ) ) [ set ([ 'a' , 'b' ]), set ([ 1 , 2 , 3 ])] >>> G . add_edge ( 3 , 4 ); G . add_edge ( 4 , 'a' ) >>> nx . shortest_path ( G , 1 , 'a' ) [ 1 , 3 , 4 , 'a' ] >>> G . add_edge ( 1 , 3 , weight = 2 ); G . add_edge ( 1 , 2 , weight = 3 ) >>> nx . shortest_path ( G , 1 , 'a' ) [ 1 , 3 , 4 , 'a' ] >>> nx . shortest_path_length ( G , 1 , 'a' ) 3 >>> nx . shortest_path_length ( G , 1 , 'a' , 'weight' ) # set attribut edge 'weight' as weight, (if not present, weight=1 ) 4 Undirected Graph: connected component, MST http://networkx.readthedocs.io/en/networkx-1.11/reference/generated/networkx.algorithms.mst.minimum_spanning_tree.html#networkx.algorithms.mst.minimum_spanning_tree >>> G = nx.Graph() >>> G.add_edge(1,2); G.add_edge(1,3); G.add_edge('a','b') >>> list( nx.connected_components(G) ) [set(['a', 'b']), set([1, 2, 3])] >>> G.add_edge(2,3) >>> mst = nx.minimum_spanning_tree(G) # returns a new graph >>> mst.edges() [('a', 'b'), (1, 2), (1, 3)] >>> G.add_edge(1,3,weight=2) # mst takes attribut 'weight', if no present, weight=1 >>> nx.minimum_spanning_tree(G).edges() [('a', 'b'), (1, 2), (2, 3)] maxflow http://networkx.readthedocs.io/en/networkx-1.11/reference/algorithms.flow.html >>> import networkx as nx >>> G = nx . DiGraph () >>> G . add_edge ( 'x' , 'a' , capacity = 3.0 ) >>> G . add_edge ( 'x' , 'b' , capacity = 1.0 ) >>> G . add_edge ( 'a' , 'c' , capacity = 3.0 ) >>> G . add_edge ( 'b' , 'c' , capacity = 5.0 ) >>> G . add_edge ( 'b' , 'd' , capacity = 4.0 ) >>> G . add_edge ( 'd' , 'e' , capacity = 2.0 ) >>> G . add_edge ( 'c' , 'y' , capacity = 2.0 ) >>> G . add_edge ( 'e' , 'y' , capacity = 3.0 ) >>> flow_value , flow_dict = nx . maximum_flow ( G , 'x' , 'y' ) >>> flow_value 3.0 >>> print ( flow_dict [ 'x' ][ 'b' ]) 1.0 maximum matching NB: maxi mum matching != maxim al matching... there are maximum-matching functions for general undir graph ( max_weight_matching ) and for bipartitie graph ( maximum_matching ), the one for bipartite graph is faster, the general one takes O(V**3). http://networkx.readthedocs.io/en/stable/reference/generated/networkx.algorithms.matching.max_weight_matching.html?highlight=maximum_matching >>> G = nx.Graph() >>> G.add_edges_from([(1,2),(2,3),(3,4),(4,5)]) >>> mate = nx.max_weight_matching(G, maxcardinality=True)#mate[v] == w if node v is matched to node w. >>> mate {2: 3, 3: 2, 4: 5, 5: 4} >>> nx.is_bipartite(G) True >>> mate=nx.bipartite.maximum_matching(G) >>> mate {1: 2, 2: 1, 3: 4, 4: 3} and there are vertex cover algorithms as well...... pulp 线性规划的库, 供了非常好用的接口来构造LP问题, 增加约束或者定义objective只要用 prob+=[expression] 就好了, 基本上看看例子就能上手. 面对选择问题的时候线性规划是不错的方法 -- 如果计算速度可以足够快的话... https://pythonhosted.org/PuLP/pulp.html >>> from pulp import * >>> x = LpVariable ( \"x\" , 0 , 3 ) >>> y = LpVariable ( \"y\" , 0 , 1 , 'Integer' ) # var category can be integer >>> prob = LpProblem ( \"myProblem\" , LpMinimize ) >>> prob += x + y <= 2 # add constraint >>> prob += - 4 * x + y # add objective >>> status = prob . solve () # solve using default solver >>> status = prob . solve ( GLPK ( msg = 0 )) # or use glpk solver >>> LpStatus [ status ] 'Optimal' >>> value ( prob . objective ) # see objective value - 8.0 >>> value ( x ) # see variable value 2.0 关于nx和pulp的应用可以参考 上篇文章 .","tags":"tech","title":"codejam常用(python)解题工具"},{"url":"http://x-wei.github.io/codejam-2015-r2pbC.html","text":"昨天做的一道codejam题目, 这个题目的三种解法都非常有代表性, 特此一记. 题目链接在这里: https://code.google.com/codejam/contest/8234486/dashboard#s=p2 Elliot's parents speak French and English to him at home. He has heard a lot of words, but it isn't always clear to him which word comes from which language! Elliot knows one sentence that he's sure is English and one sentence that he's sure is French, and some other sentences that could be either English or French. If a word appears in an English sentence, it must be a word in English. If a word appears in a French sentence, it must be a word in French. Considering all the sentences that Elliot has heard, what is the minimum possible number of words that he's heard that must be words in both English and French? Input The first line of the input gives the number of test cases, T. T test cases follow. Each starts with a single line containing an integer N. N lines follow, each of which contains a series of space-separated \"words\". Each \"word\" is made up only of lowercase characters a-z. The first of those N lines is a \"sentence\" in English, and the second is a \"sentence\" in French. The rest could be \"sentences\" in either English or French. (Note that the \"words\" and \"sentences\" are not guaranteed to be valid in any real language.) Output For each test case, output one line containing \"Case #x: y\", where x is the test case number (starting from 1) and y is the minimum number of words that Elliot has heard that must be words in both English and French. Limits 1 ≤ T ≤ 25. Each word will contain no more than 10 characters. The two \"known\" sentences will contain no more than 1000 words each. The \"unknown\" sentences will contain no more than 10 words each. Small dataset 2 ≤ N ≤ 20. Large dataset 2 ≤ N ≤ 200. 我的codejam程序模板长这样: def readval(typ=int): return typ( raw_input() ) def readvals(typ=int): return map( typ, raw_input().split() ) def testcase(cas): print 'Case #%d: %d' % ( cas, res ) if __name__=='__main__': T = int(raw_input()) for i in xrange(T): testcase(i+1) 法1: 穷举 bruteforce (for small testcase) 在small set(N=20) 中, 对于那18个未知语言的句子, 每句可能是英语或法语. 那么穷举所有可能性, 然后选择双语单词最少的即可. 2&#94;18约等于几十万 , 按说python还是可以handle的 (2&#94;10=1k, 2&#94; 20=1M, python每秒大约能循环几百万次). naive bruteforce 生成所有的可能性组合, python的 itertools 包里提供了 combinations 函数, combinations(iterable, r) 返回所有在iterable中大小为r的子集(的迭代器): >>> from itertools import combinations >>> for c in combinations ( 'ABCD' , 2 ): print c ( 'A' , 'B' ) ( 'A' , 'C' ) ( 'A' , 'D' ) ( 'B' , 'C' ) ( 'B' , 'D' ) ( 'C' , 'D' ) 用 combination 函数即可实现枚举每句话是英语还是法语的功能. 当每句话是英语还是法语已经确定以后, 可以用集合取交集的方法( set1.intersection(set2) )得到两种语言一共有多少个重复的单词. 程序长这样: def testcase(cas): # not fast enough... N = readval() En = set( readvals(str) ) Fr = set( readvals(str) ) if N==2: print 'Case #%d: %d' % ( cas, len(En.intersection(Fr)) ) return scent = [] for i in xrange(N-2): scent.append( set(readvals(str)) ) def partition(engsubset): allEn, allFr = En.copy(), Fr.copy() for i in xrange(N-2): if i in engsubset: allEn.update(scent[i]) else: allFr.update(scent[i]) return len( allEn.intersection(allFr) ) possibleres = [] for l in xrange(N-2): for engsubset in combinations(xrange(N-2), l): possibleres.append(partition(engsubset)) res = min(possibleres) print 'Case #%d: %d' % ( cas, res ) 然而, 下载了small testcase以后运行程序, 速度还是不够快, 大约要十分钟才有结果 — 而codejam的提交时间限制是4分钟啊... 程序每次检查2&#94;18种可能性, 但是由于每次检查都要进行集合的union和intersection操作(这种操作的效率并不高, 甚至C++里也是一样), 这个操作太耗费时间了所以不行... bruteforce using bitmap 需要更加聪明的穷举方法, 自然的想法就是用bitmap(或者叫bitvector?). 之前的两个基本操作都可以用bitmap完成: 枚举各个句子的语言种类: 如果每个句子用一位来表示的话(1代表英语, 0代表法语), 那么用N位的bitmap即可表示一种情形. 这个bitmap只需从0增加到2&#94;N-1, 就把2&#94;N个可能性都遍历了. (实际上是2&#94;N-2个可能性, 因为前两个句子已经确定语言了). 两种语言的词汇表进行union/intersection: 假设共有K个不同的单词, 那么用一个K位的bitmap即可表示一种语言包含了哪些单词. 然后集合的union和intersection即可表示为OR和AND的逻辑运算. bit manipulation蛮subtle的, 不过习惯了就好... 另外python的integer可以任意长度, 不用像C/java那样考虑bitmap位数大于64的情况, 还是非常方便的. 代码如下: def testcase(cas): # use bitmap instead of set to speedup for small case !! N = readval() scent = []; allwords = set() for i in xrange(N): scent.append( readval(str) ) allwords.update( scent[i].split() ) words = sorted(allwords) # if K distinct words in total, each sentence can be reprensented as a K-bit bitmap bitmaps = [] for i in xrange(N): #construct bitmaps bm = 0 for wd in scent[i].split(): bm |= ( 1<< words.index(wd) ) bitmaps.append(bm) res = 1e10 # look for all combinations for i in xrange(1<<(N-2)): en = bitmaps[0]; fr = bitmaps[1] for k in xrange(N-2): if (1<<k) & i > 0: en |= bitmaps[k+2] else: fr |= bitmaps[k+2] res = min( res, bin(en&fr).count('1') ) print 'Case #%d: %d' % ( cas, res ) 以上代码在我电脑上执行small的时候大约花费1分钟, 比最开始用set的速度提高了十倍. 当然, 对于large的case这种解法肯定就超时了... 法2: 最大流 maxflow 这个方法也是官方 analysis 里提供的答案. 如果将所有句子 S 以及所有单词 w 看作节点, 每个句子的节点 S 于它包含的单词的节点 w 相连, 问题转化成了从节点 S1 到节点 S2 的vertex cut问题 (选取最小的节点集合, 将该集合的节点去掉以后S1和S2不再联通). 然后这个问题又可以转为边的min cut问题: 只需要把每个单词节点w分成左右两个节点 w1 和 w2 , 并且这样添加边(假设w在句子S中): w1-->w2 , capacity=1 S-->w1 , capacity=INF w2-->S , capacity=INF 在这样构造的图里计算maxflow即可得到mincut, 也就是题目的答案... 在实现这个解法的时候用到了networkx这个包, 它提供了比bgl好用100倍的接口(虽然也比bgl慢差不多100倍 ==...). 任何hashable的object都可以用来作为节点的index , 所以写起来非常舒服, 15行搞定: def testcase_maxflow ( ind ): # using maxflow ! INF = 1e10 import networkx as nx G = nx . DiGraph () N = readval () for i in xrange ( N ): words = readvals ( str ) si = 'sent- %d ' % i G . add_node ( si ) for wd in words : G . add_edge ( wd + '_l' , wd + '_r' , capacity = 1 ) G . add_edge ( si , wd + '_l' , capacity = INF ) G . add_edge ( wd + '_r' , si , capacity = INF ) flow_value , flow_dict = nx . maximum_flow ( G , 'sent-0' , 'sent-1' ) print 'Case # %d : %d ' % ( ind , int ( flow_value ) ) 用这个代码即可轻松通过large case... maxflow的建模还真是艺术...orz 法3: 线性规划 (integer) linear programming 写完上面那个解法以后, 我又看了看 大神 的解法, 发现他居然用的是 pulp (python线性规划的包), 于是自己想了一下, 这个问题确实可以用线性规划来建模 ! 首先说一句pulp这个包, 它提供了非常好用的接口来构造LP问题, 增加约束或者定义objective只要用 prob+=[expression] 就好了, 可以说比AMPL好用不少... 下面是一个简单的例子: >>> from pulp import * >>> x = LpVariable ( \"x\" , 0 , 3 ) >>> y = LpVariable ( \"y\" , 0 , 1 , 'Integer' ) # var category can be integer >>> prob = LpProblem ( \"myProblem\" , LpMinimize ) >>> prob += x + y <= 2 # add constraint >>> prob += - 4 * x + y # add objective >>> status = prob . solve () # solve using default solver >>> status = prob . solve ( GLPK ( msg = 0 )) # or use glpk solver >>> LpStatus [ status ] 'Optimal' >>> value ( prob . objective ) # see objective value - 8.0 >>> value ( x ) # see variable value 2.0 OK, 本题的建模过程如下: 对于每一个句子 S , 定义binary的LP变量 Se , Se=1表示句子S为英语, =0表示为法语. 对于每个单词 w , 定义两个binary变量 we , wf , 表示单词w是英(法)语单词. 然后, 再定义变量 wef , 它表示单词 w 既是英语单词又是法语单词. 所以有这个逻辑关系: wef = we AND wf 那么这种逻辑关系如何用线性约束描述呢? 可以这样: wef >= we+wf-1 非常巧妙吧... ( 这篇文章 总结了各种逻辑关系用线性规划的描述方式, 写的得非常详细. ) 要最小化的目标函数就是 sum(wef) 了, 约束除了刚才那个 wef >= we+wf-1 以外, 还要表达单词和句子之间的关系: 如果一个句子为英(法)语, 那么句子里的每一个单词都为英(法)语. 这是一个 se==>we 的逻辑关系, 用线性约束表达为: we>=se . 所以整个模型是: Minimize sum(wef) st: we >= se wf >= (1-se) wef >= we+wf-1 Se[0]==1, Se[1]==0 用pulp编写的代码如下: def testcase(ind):# formulate it as linear programming N = readval() sentc = [] allwords = set() for i in xrange(N): sentc.append( set(readvals(str)) ) allwords.update( sentc[-1] ) words = sorted( allwords ) M = len(words) wordsindex = {words[i]:i for i in xrange(M)} # mapping a word to its index pb = LpProblem('Bilingual', LpMinimize) # LP variables Se = [ LpVariable('Se_'+str(i), cat='Binary') for i in xrange(N) ] # Se[i] = indicator(scentence i is english) we = [ LpVariable('we_'+str(j), cat='Binary') for j in xrange(M) ] # we[j] = indicator(word j is english) wf = [ LpVariable('wf_'+str(j), cat='Binary') for j in xrange(M) ] # wf[j] = indicator(word j is french) wef = [ LpVariable('wef_'+str(j), cat='Binary') for j in xrange(M) ] # wef[j] = indicator(word j is BOTH en and fr) pb += sum( wef ) pb += Se[0]==1 pb += Se[1]==0 for i in xrange(N): si = sentc[i] for wd in si: j = wordsindex[wd] pb += we[j] >= Se[i] pb += wf[j] >= (1-Se[i]) for j in xrange(M): pb += wef[j] >= we[j]+wf[j]-1 # # wef[i] = we[i] && wf[i] #~ pb.solve( GLPK(msg=0) ) pb.solve( ) res = int( value(pb.objective) ) print 'Case #%d: %d' % ( ind, res ) 以上代码在small上运行约1分钟, large约3分钟. 另外我发现 另一个大神 代码里没有用Binary/Integer的lp变量, 也就是说他用的是连续的线性规划! 当我把变量的定义改成: Se = [ LpVariable('Se_'+str(i), 0, 1) for i in xrange(N) ] # Se[i] = indicator(scentence i is english) we = [ LpVariable('we_'+str(j), 0) for j in xrange(M) ] # we[j] = indicator(word j is english) wf = [ LpVariable('wf_'+str(j), 0) for j in xrange(M) ] # wf[j] = indicator(word j is french) wef = [ LpVariable('wef_'+str(j), 0) for j in xrange(M) ] # wef[j] = indicator(word j is BOTH en and fr) 这样以后, 得到的结果还是正确的!! (不过代码运行时间没有显著的提高). 所以是这个模型中的矩阵满足 totally unimodular 性质? 不过这个模型似乎并不满足wikipedia里写的那个充分条件?...@@ 这里 有一篇真•大神的文章是讲如何识别TU矩阵的 (tl;dr......), 那么是不是各种solver内部已经有了自动判断TU的代码, 所以这两种程序的运行时间差不多?... 总而言之这道题目还是非常有意思的, 三种解法都很有代表性, 这里的技巧估计在codejam里会经常用到...","tags":"tech","title":"codejam2015-round2-pbC 的三种解法"},{"url":"http://x-wei.github.io/java-use-weka.html","text":"之前一直用weka的GUI界面做机器学习的任务, 感觉这个软件虽然界面丑, 不过确实是快速开展机器学期的利器. 关于GUI的weka使用以后有时间再写. 今天这篇记录一下最近使用的java版本的weka. 1. Include jars into project weka官网的下载链接里选择linux版本的weka压缩包即可, 下载以后找到weka.jar文件, 在工程里将其include一下就可以使用了(btw, 现在开始放弃eclipse, 进入IDEA的怀抱了...). weka的文档在解压缩的文件里有, 另外在线文档在: http://weka.sourceforge.net/doc.stable-3-8/ about libsvm... 关于libsvm需要有一点特别指出. weka自带的算法里是不包含libsvm的 (有个类似的SMO, 不过还是libsvm久经考验啊...), 需要使用weka的package manager安装. 打开package manager是在weka主界面的菜单里: 在package manager里搜索到libsvm安装即可. 然后(linux下)在主目录可以看到有个wekafiles文件夹, wekafiles/packages/LibSVM/ 目录下就是libsvm的内容. 需要指出的一点是, 要使用libsvm的话, 需要同时引用两个jar文件 , 而且都叫libsvm.jar!! 这两个jar, 一个叫 LibSVM.jar , 在 wekafiles/packages/LibSVM/ 下, 另一个叫 libsvm.jar , 在 wekafiles/packages/LibSVM/lib/ 下...orz 如果只include第一个jar的话, 就会报错: \"java.lang.Exception: libsvm classes not in CLASSPATH! \". 2. terminology 首先统一一下各种东西的叫法... Instances : 就是dataset, 比如training set或者test set, Instances实际上就是一个Instance的集合 Instance : 就是一个数据点了 Attribute : 一个数据点有一些attribute (别处一般叫做feature), 其中有一个attribute其实是label(可以为missing) Classifier : weka里的Classifer其实是也包含了regressor或者cluster... 后面都称之为model Evaluation : 给定一个model和一个dataset, 给出evaluation的数据, 类似GUI界面给出的那些内容 3. 在程序里构建数据 这也是为什么要在java里用weka的原因: 如果数据可以直接以csv或者arff文件的方式得到, 那么直接在GUI界面下就可以搞了... 新建Attribute http://weka.sourceforge.net/doc.stable-3-8/weka/core/Attribute.html 新建numeric的attribue只要简单的在构造函数里传入一个attribute的名字即可: // Create numeric attributes \"length\" and \"weight\" Attribute length = new Attribute(\"length\"); Attribute weight = new Attribute(\"weight\"); 新建离散(normial)的attribue则需要一个list乘放所有可能的数值: // Create list to hold nominal values \"first\", \"second\", \"third\" List my_nominal_values = new ArrayList(3); my_nominal_values.add(\"first\"); my_nominal_values.add(\"second\"); my_nominal_values.add(\"third\"); // Create nominal attribute \"position\" Attribute position = new Attribute(\"position\", my_nominal_values); 新建Instances(dataset) http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instances.html Instances 实际上就是一个 Instance 的集合, Instances可以类比为pandas里面的DataFrame, 然后每个instance相当于一行. 另外 Instances 比 Instance 多的就是Attribute信息(类比为pandas里DataFrame的表头head). Instances 的构造函数有两种, 一种是直接在arff文件里读取, 这个后面再说. 另一种构造函数是在java函数里构建Instance时用的, 它构造一个空的Instance集合, 构造函数提供dataset的名字, attribute的集合(arraylist)以及初始的capacity: Instances(String name, ArrayList<Attribute> attInfo, int capacity) 其中的第二个参数attInfo其实就相当于是表头信息了, 它是一个Attribute的ArrayList. ArrayList<Attribute> atts = new ArrayList<Attribute>(); atts.add(length); atts.add(weight); atts.add(position); Instances adataset = new Instances(\"aDataSet\", atts, 10); (上面代码里的 length , weight 和 position 都是前面声明的Attribute对象) Instances还可以指定哪一列对应的是class label (单个Instance则不能 — 因为单个Instance并没有表头信息attInfo): void setClassIndex(int classIndex) 新建Instance http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instance.html Instance 是一个接口而不是一个类, 一半常用的是 DenseInstance 类(它又继承自 AbstractInstance 抽象类) 这里有一个坑: 一定要指定Instance所属的DataSet(既它属于哪一个Instances对象)再使用setValue函数, 否则在调用setValue的时候可能会有问题!!! void setDataset(Instances instances) 构造函数里只需要提供这个instance的attribute数量即可. 然后使用 setValue 函数可以给每个attribue指定数值. setValue函数的第一个参数接收一个Attribue对象, 第二个参数就是这个attribue的数值(double或者string). // Create empty instance with three attribute values Instance inst = new DenseInstance(3); instance.setDataset( adataset); // before calling setValue, should first set the Dataset!`` // Set instance's values for the attributes \"length\", \"weight\", and \"position\" inst.setValue(length, 5.3); inst.setValue(weight, 300); inst.setValue(position, \"first\"); 4. 在程序里训练model 这里只做classification的例子好了. http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/Classifier.html Classifier是一个Interface, 可以在文档里看到有很多类都实现了这个interface, 主要是三个常用的函数: void buildClassifier(Instances data) : 用data数据进行训练 double classifyInstance(Instance instance) : 预测一个Instance的label double[] distributionForInstance(Instance instance) :对于每一个可能的类, 给一个probability, 返回一个double数组 所以程序里训练model只需要调用 buildClassifier() 函数即可. 5. 从文件读入数据和model 之前讲的是在程序里得到数据, 在程序里储存结果的方法, 而如果可以保存数据到文件的话, 在GUI界面下调试模型应该更加放方便. 预先已经得到了数据的话, 可以先把数据保存问arff格式, 然后用GUI的weka训练和调试参数. 当得到满意的结果以后可以在GUI界面里选择保存训练好的模型(一个.model文件): 然后, weka提供了非常方便的方法, 直接从arff文件里得到 Instances 对象, 从model文件里得到 Classifier 对象: (参考链接: https://weka.wikispaces.com/Serialization 以及 https://weka.wikispaces.com/Use+Weka+in+your+Java+code ) Classifier clf = (Classifier) weka.core.SerializationHelper.read(\"/some/where/j48.model\"); Instances testset = new Instances(new BufferedReader(new FileReader(\"/some/where/test.arff\")));`` 6. 输出Evaluation统计 当得到了训练好的模型 clf 以及要使用的测试数据 testset 以后, 可以在testset上测试模型, 并输出结果的统计数据. 这些是靠 Evaluation 类完成的. http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/evaluation/Evaluation.html Evaluation的构造函数里提供的Instances应该为training set, 这个训练集的作用是\"to get some header information and prior class distribution information\", 如果构造时给的是testing set的话, 应该调用 useNoPriors() 函数一下. 构造了evalation对象以后, 只要使用 evaluateModel(Classifier classifier, Instances data) 函数即可, 第一个参数为训练好了的模型( clf ), 第二个参数为要用来测试的数据( testset ). 然后可以输出统计信息, 就像在wekaGUI界面一样, 主要靠 toSummaryString() 和 toMatrixString() 两个函数. Instances trainInstances = ... instances got from somewhere Instances testInstances = ... instances got from somewhere Classifier scheme = ... scheme got from somewhere Evaluation evaluation = new Evaluation(trainInstances); evaluation.evaluateModel(scheme, testInstances); System.out.println(evaluation.toSummaryString()); System.out.println(evaluation.toMatrixString()); 7. 输出预测结果 对于每一个Instance, 只需要调用Classifier的 classifyInstance(Instance instance) 或者 distributionForInstance(Instance instance) 函数, 即可得到预测结果... Voila, 大概就是这样, weka这个工具还是蛮好用的(只要能忍受它界面的丑), 而且也算没有太多的坑...","tags":"soft","title":"在java程序里使用weka进行机器学习"},{"url":"http://x-wei.github.io/algoII_week6_3_intractability.html","text":"1. Introduction to Intractability recall model of computation: DFA a univeral model of computation: turing machine → no more powerful model of computation. Turing machine can compute any function that can be computed by a physically harnessable process of the natural world. bottom line: turing machine is a simple and universal model of computation. Q. which algos are useful in practice ? useful in practice = polynomial time for all inputs def. a pb is intractable if it cannot be solved in polynomial time. 2 pbs that can be proved to require exp time: Given a constant-size programme, does it halt in <=K steps ? Given a N*N chess board position, can the first player force a win ? Bad news: very few pbs can be proved to require exp time... 2. Search Problems Four fundamental problems: LSLOVE Given a system of linear equations, find a solution var: real numbers → guassian elimination LP Given a system of linear inequaties, find a solution. (not necessarily find the opt) var: real numbers ILP Given a system of linear inequaties, find a 0-1 solution. var: 0 or 1 SAT Given a system of boolean equations , find a binary solution. Which ones of the 4 foundamental pbs have poly-time solutions? LSLOVE: Gaussian elimination works in O(n3) LP: Ellipsoid works in poly-time (simplex also poly-time in practice ..) ILP, SAT: No poly-time algorithm known (or believed to exist) ! All 4 pbs are examples of search problems. Search pb : given an instance I , find a solution S / report there's no solution. requirement : able to efficiently (poly-time) check that S is a solution. (that's the case for the above 4 fundamental pbs) another example: FACTOR : given a n-bit integer, find a nontrival factor. (given a solution, simply need to long-divide to check...) 3. P vs. NP def. NP is the class of all search pbs. (ie. solution be checked efficiently) NB: classical definition limits to yes-no pbs... Significance: NP pbs are what scientists and engineers aspire to compute feasibly . examples: def. P is the class of search pbs that are solvable in poly-time. (What scientists and engineers do compute feasibly .) examples: Nondeterminism Nondeterminism machine can guess the solution (donot exist in natural world..). → NFA tries to simulate such a machine... Ex. int[] a = new int[N]; ・ Java: initializes entries to 0 . ・ Nondeterministic machine: initializes entries to the solution! NP: Search problems solvable in poly time on a nondeterministic Turing machine . Extended Church-Turing thesis: P: Search pbs solvable in poly time in natural world . do we have non-determinism in natural world? ---> natural computers ? ex. STEINER tree: set of segments connecting given N points. use soap → doesn't really work... another example for P/NP: automating creativity being creative VS appreciating creativity The central question: does P=NP? (can you alway avoid brute-force searching and do better?) Millennium prize by Clay instute. (among all ways of earning 1M dollars, this might be the most complicated way... @_@...) 4. Classifying Problems classify pbs like classifying elements into perodic table. key pb: satisfiablity SAT. given a sys of boolean eq, find a solution. exhaustive search: try 2&#94;n possible solutions. conjecture: no poly-time algo for SAT (ie. intractable) Assumption : assume the intractability for SAT. Tool: reduction def. pb X reduces to pb Y: we can solve pb X with the algo for pb Y. if SAT poly-reduces to pb Y ⇒ pb Y in (probably) intractable. SAT poly-reduces to ILP (all SAT pb can be reduced to 3SAT) ⇒ can be converted to an ILP pb: for each eq, introduce a var Ci: 5. NP-Completeness def. an NP pb is NP-complete if all pbs in NP poly-reduces to it. prop. SAT id NP-complete. any pb in NP poly-reduces to SAT (reverse direction as last lecture) pf sketch: convert non-dertiministic turing machine notation to SAT notation... cor. poly time algo for SAT iff P=NP... ⇒ there pbs are equivalent ! summary: ==... 6. Coping with Intractability exploit intractability cryptography ecopoits the hardness of FACTOR pb Can factor an n-bit integer in n 3 steps on a \" quantum computer .\" Coping with intractability relax one of desired features... special cases Develop a heuristic, and hope it produces a good solution. no guarantee ex. TSP Approximation algorithm. Find solution of provably good quality. Halmiton path remark: Euler path (each edge once) easy, Halmiton path (each vertex once) NPC... dfs solution for Halmiton path: public class Halmiton{ private boolean[] marked; private int count=0; // nb of Halmiton paths public Halmiton(Graph G){ marked = new boolean[G.V()]; for (int v=0; v<G.V(); v++) dfs(G,1,1); } private void dfs(Graph G, int v, int depth){ if(depth==G.V()) count++; marked[v]=true; for(int w: G.adj(v)) if(marked[w]==false) dfs(G, w, depth+1); marked[v]=flase; // backtrack } }","tags":"notes","title":"[Algorithms II] Week 6-3 Intractability"},{"url":"http://x-wei.github.io/algoII_week6_2_LP.html","text":"simplex algo: top 10 algo of the 20th century (ever?). what is linear programming: a general problem-solving model that works for: shortest-path, maxflow, MST, matching, assignment, ... 1. Brewer-'s Problem toy example: choose products to maximize profit. ... feasible region : a convex polygon. ⇒ optimum solution appears at an extreme point. standard form of LP n non-neg variables (j=1..n) m linear euqations (i=1..m) input: a_ij, c_j, b_i output: x_j to convert inequality to equality (as in the standard form above) : add slack var! def. convex set for any a and b in set ⇒ 1/2(a+b) is also in set. extreme point: def. extreme point is a point in set that cannot be written as 1/2(a+b) with a b distinct. extreme point property: if there exists an potimal solution, then there exists one that is an extreme point. nb of extreme point is finite but this nb can be exponential greedy property: extreme point is optimal iff no better adj extreme points. 2. Simplex Algorithm algo. simplex start at some point pivot from one extreme point to an adj one (never decrease the obj fcn) repeat until optimal We're using the \"basis\" and \"pivoting\" to solve LP. def. basis (基变量) is a subset (size=m) of the n variables. vars in basis are always non-zero... basic feasible solution: set n-m non-basis vars to 0 solve for remaining m vars (with m constraints) if unique and feasible (matrix invertable) algo: initial basic-feasible-solution: start slack vars as basis. choose a non-basic var as pivot , add it into basis, take some basis var out ex. pick B as pivot var using constraint 2 (2nd equation): why picking var B? → its obj coeff is positive why pivot on 2nd constraint (5A+15B+Sc=480)? → RHS > 0 (preserves feasibility) minimum ratio rule: min(480/15, 160/4, 1190/20) stop when no obj-coeff is positive 3. Simplex Implementations encode standard LP formulation into java 2d array: public class Simplex{ private double[][] a; private int m,n; public Simplex(double[][] A, double[] b, double[] c){ m = b.length; n = c.length; a = new double[m+1][n+m+1]; for(int i=0; i<m; i++) for(int j=0; j<n; j++) a[i][j] = A[i][j]; for(int j=n; j<m+n; j++) a[j-n][j] = 1; for(int j=0; j<m; j++) a[j][n+m] = b[j]; for(int j=0; j<n; j++) a[m][j] = c[j]; } } simplex algo: just transform initial 2d array into final solution. choosing pivot variable (find entering column) Bland's rule. find the first column whose obj-coeff is positive. private int bland(){ for(int q=0; q<m+n; q++) if(a[m][q]>0) return q; return -1; } choosing pivot constraint (find entering row) minimum ratio rule (if a tie, choose first row). private int minRatioRule(int q){ int p = -1; for(int i=0; i<m; i++){ if (a[i][q]<=0) continue; else if (p==-1) p=i; else if (a[i][m+n]/a[i][q] < a[p][m+n]/a[p][q]) p=i; } return p; } do the pivot (column q, row p) like Guassian elimination: make var q disappear on each row (except for row p); on row p: make var q's coeff become 1. public void pivot(int p, int q){ for(int i=0; i<m; i++) for(int j=0; j<m+n; j++) if (i!=p && j!=q) a[i][j] -= a[p][j]*a[i][q]/a[p][q]; for(int i=0; i<m; i++) if(i!=p) a[i][q] = 0; for(int j=0; j<m+n; j++) if(j!=q) a[p][j] /= a[p][q]; a[p][q] = 1; } so the simplex algo is: public void solve(){ while(true){ int q = bland(); if(q==-1) break; // optimal if -1 int p = minRatioRule(q); if(p==-1) break; // unbounded if -1 pivot(p,q); } } final solution is just in the array: remarkable property in typical applications , simplex terminates after at most 2(m+n) pivots. — whereas nb of extreme points is exp in n !! ie. LINEAR time in practice!! other pivot rules: degeneracy when choosing new basis, still stay in the same extreme point... → might cause cycling → bland's rule guarantees finite number of pivots further improvement: Best practice. Don't implement it yourself...... (AMPL是个好东西...) 算法的力量: 4. Linear Programming Reductions reduction to std form (equalities) Minimization problem: max -1*obj ineq constraints: add slack var unbounded var X: replace with X=X0-X1, X0>=0, X1>=0 modeling of LP identify variables define constraints define objective fcn convert to std form maxflow by LP variables: x_uv = flow on edge uv constraints: capacity, flow conservation obj: net flow to t can use LP to solve mincost maxflow easily... max cardinality bipartite matching by LP input: bipartite graph goal: max cardinatlity matching (set of vertex-disjoint edges) can be reduced to maxflow (见algolab...) var: x_ij = indicator of person i assigned to job j (0<=x_ij<=1) constraints: vertex-disjoint obj: sum of all x_ij non-trival: cause this is an INTEGER LP... Th (Von Neumann) (and Poincare?..) if all RHS=1 ⇒ all extreme points of the polyhedron have integer coord . and many others... the profound question: Is there a universal problem-solving model ? → P/NP... \"For the time being, the closest thing that we have to universal problem-solving model is LP \"","tags":"notes","title":"[Algorithms II] Week 6-2 Linear Programming"},{"url":"http://x-wei.github.io/imooc_py_oop.html","text":"http://www.imooc.com/learn/317 模块和包 包 : 文件夹 (可以有多级), 且包含 __init__.py 文件(每层都要有) 模块 : py文件 代码分开放在多个py文件( 模块 名=文件名). 同名变量互不影响. 模块名冲突: 把同名模块放在不同 包 中. 导入模块 from math import log from logging import log as logger 引用时: 使用完整的路径(包+模块名). ex. p1.util.f() 动态导入模块 try : from cStringIO import StringIO except ImportError : from StringIO import StringIO 上述代码先尝试从cStringIO导入，如果失败了（比如cStringIO没有被安装），再尝试从StringIO导入。这样，如果cStringIO模块存在，则我们将获得更快的运行速度，如果cStringIO不存在，则顶多代码运行速度会变慢，但不会影响代码的正常执行。 使用__future__ Python的新版本会引入新的功能，但是，实际上这些功能在上一个老版本中就已经存在了。要\"试用\"某一新的特性，就可以通过导入__future__模块的某些功能来实现。 ex. 在Python 2.7中引入3.x的除法规则，导入__future__的division： >>> from __future__ import division >>> print 10 / 3 3.3333333333333335 安装第三方模块 模块管理工具: easy_install pip (推荐) 查找第三方模块: https://pypi.python.org/pypi 面向对象编程基础 OOP: 数据的封装 初始化实例属性 当创建实例时， __init__() 方法被自动调用, 第一个参数必须是 self（也可以用别的名字，但建议使用习惯用法, 第一个参数self被Python解释器作为实例的引用），后续参数则可以自由指定，和定义函数没有任何区别。 相应地，创建实例时，就必须要提供除 self 以外的参数. 用 setattr 让 __init__ 接受任意的kw参数: setattr(object, name, value) This is the counterpart of getattr(). The arguments are an object, a string and an arbitrary value. The string may name an existing attribute or a new attribute. The function assigns the value to the attribute, provided the object allows it. For example, setattr(x, 'foobar', 123) is equivalent to x.foobar = 123. class Person ( object ): def __init__ ( self , name , gender , birth , ** kw ): self . name = name self . gender = gender self . birth = birth for k , v in kw . iteritems (): setattr ( self , k , v ) 访问限制 Python对属性权限的控制是通过 属性名 来实现的. 如果一个属性由双下划线开头( __ )，该属性就无法被外部访问。 但是，如果一个属性以\" __xxx__ \"的形式定义，那它又可以被外部访问了，以\" __xxx__ \"定义的属性在Python的类中被称为特殊属性有很多预定义的特殊属性可以使用，通常我们不要把普通属性用\" xxx \"定义。 以单下划线开头的属性\" _xxx \"虽然也可以被外部访问，但是，按照习惯，他们不应该被外部访问。 创建类属性 绑定在一个实例上的属性不会影响其他实例，但是，类本身也是一个对象，如果在类上绑定一个属性，则所有实例都可以访问类的属性，并且，所有实例访问的类属性都是同一个！也就是说，实例属性每个实例各自拥有，互相独立，而 类属性有且只有一份 。 定义类属性可以直接在 class 中定义： class Person ( object ): address = 'Earth' def __init__ ( self , name ): self . name = name 因为类属性是直接绑定在类上的，所以，访问类属性不需要创建实例，就可以直接访问. 对一个实例调用类的属性也是可以访问的，所有实例都可以访问到它所属的类的属性. print Person.address print p1.address 类属性和实例属性名字冲突怎么办 当实例属性和类属性重名时，实例属性优先级高，它将屏蔽掉对类属性的访问。 可见，千万 不要在实例上修改类属性 ，它实际上并没有修改类属性，而是给实例绑定了一个实例属性。 定义实例方法 实例的方法就是在类中定义的函数，它的 第一个参数永远是 self ，指向调用该方法的实例本身，其他参数和一个普通函数是完全一样的. 在实例方法内部，可以访问所有实例属性，这样，如果外部需要访问私有属性，可以通过方法调用获得，这种数据封装的形式除了能保护内部数据一致性外，还可以简化外部调用的难度。 我们在 class 中定义的实例方法其实也是属性，它实际上是一个函数对象. 因为方法也是一个属性，所以，它也可以动态地添加到实例上，只是需要用 types.MethodType() 把一个函数变为一个方法... 定义类方法 和属性类似，方法也分实例方法和类方法。 在class中定义的全部是实例方法，实例方法第一个参数 self 是实例本身。 要在class中定义类方法，需要这么写： class Person ( object ): count = 0 @classmethod def how_many ( cls ): return cls . count def __init__ ( self , name ): self . name = name Person . count = Person . count + 1 print Person . how_many () p1 = Person ( 'Bob' ) print Person . how_many () 通过标记一个 @classmethod ，该方法将绑定到 Person 类上，而非类的实例。类方法的第一个参数将传入类本身，通常将参数名命名为 cls ，上面的 cls.count 实际上相当于 Person.count。 类的继承 代码复用 python的继承: 总是从某个类继承(最上层是 object ) 不要忘记 super.__init__ 调用 super(SubCls, self)将返回当前类继承的父类, 注意self参数已在super()中传入，在__init__()中将隐式传递，不需要写出（也不能写）。 def init (self, args): super(SubCls, self). init (args) pass 判断类型 函数 isinstance() 可以判断一个变量的类型，既可以用在Python内置的数据类型如str、list、dict，也可以用在我们自定义的类，它们本质上都是数据类型。 >>> isinstance(p, Person) True # p是Person类型 >>> isinstance(p, Student) False # p不是Student类型 >>> isinstance(p, Teacher) False # p不是Teacher类型 >>> isinstance(s, Person) True # s是Person类型 在一条继承链上，一个实例可以看成它本身的类型，也可以看成它父类的类型。 多态 调用 s.whoAmI()总是先查找它自身的定义，如果没有定义，则顺着继承链向上查找，直到在某个父类中找到为止。 由于Python是动态语言，所以，传递给函数 who_am_i(x)的参数 x 不一定是 Person 或 Person 的子类型。任何数据类型的实例都可以，只要它有一个whoAmI()的方法即可： class Book ( object ): def whoAmI ( self ): return 'I am a book' 这是动态语言和静态语言（例如Java）最大的差别之一。动态语言调用实例方法，不检查类型， 只要方法存在，参数正确，就可以调用 。 多重继承 除了从一个父类继承外，Python允许 从多个父类继承 ，称为多重继承。 class A ( object ): def __init__ ( self , a ): print 'init A...' self . a = a class B ( A ): def __init__ ( self , a ): super ( B , self ). __init__ ( a ) print 'init B...' class C ( A ): def __init__ ( self , a ): super ( C , self ). __init__ ( a ) print 'init C...' class D ( B , C ): def __init__ ( self , a ): super ( D , self ). __init__ ( a ) print 'init D...' D 同时继承自 B 和 C，也就是 D 拥有了 A、B、C 的全部功能。多重继承通过 super()调用__init__()方法时，A 虽然被继承了两次，但 __init__() 只调用一次： >>> d = D('d') init A... init C... init B... init D... 获取对象信息 首先可以用 type() 函数获取变量的类型，它返回一个 Type 对象： >>> type(123) <type 'int'> >>> s = Student('Bob', 'Male', 88) >>> type(s) <class '__main__.Student'> 其次，可以用 dir() 函数获取变量的所有属性： >>> dir(123) # 整数也有很多属性... ['__abs__', '__add__', '__and__', '__class__', '__cmp__', ...] >>> dir(s) ['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'gender', 'name', 'score', 'whoAmI'] dir() 返回的属性是字符串列表，如果已知一个属性名称，要获取或者设置对象的属性，就需要用 getattr() 和 setattr() 函数了： >>> getattr(s, 'name') # 获取name属性 'Bob' >>> setattr(s, 'name', 'Adam') # 设置新的name属性 >>> s.name 'Adam' >>> getattr(s, 'age') # 获取age属性，但是属性不存在，报错： Traceback (most recent call last): File \"<stdin>\", line 1, in <module> AttributeError: 'Student' object has no attribute 'age' >>> getattr(s, 'age', 20) # 获取age属性，如果属性不存在，就返回默认值20： 20 class Person(object): def __init__(self, name, gender, **kw): for k,v in kw.iteritems(): setattr(self, k, v) p = Person('Bob', 'Male', age=18, course='Python') print p.age print p.course 定制类 特殊方法 又叫 \"魔术方法\" 定义在class中 不需要直接调用: py的函数或操作符会自动调用 ex. 任何数据类型的实例都有 __str__() 特殊方法. pothon的特殊方法: __str__ : 用于print __len__ : 用于len __cmp__ : 用于比较 cmp /排序 sorted str 和 repr 实现特殊方法 __str__() 可以在print的时候打印合适的字符串, 如果直接在命令行敲变量名则不会: >>> p = Person('Bob', 'male') >>> print p (Person: Bob, male) >>> p <main.Person object at 0x10c941890> 因为 Python 定义了 __str__() 和 __repr__() 两种方法， __str__() 用于显示给用户，而 __repr__() 用于显示给开发人员。 偷懒定义 __repr__ : __repr__ = __str__ cmp __cmp__ 用实例自身self和传入的实例 s 进行比较，如果 self 应该排在前面，就返回 -1，如果 s 应该排在前面，就返回1，如果两者相当，返回 0。 class Student ( object ): def __init__ ( self , name , score ): self . name = name self . score = score def __str__ ( self ): return '(%s: %s)' % ( self . name . lower (), self . score ) __repr__ = __str__ def __cmp__ ( self , s ): if self . score !=s . score: return - ( self . score - s . score ) else: return cmp ( self . name , s . name ) len 如果一个类表现得像一个list，要获取有多少个元素，就得用 len() 函数。 要让 len() 函数工作正常，类必须提供一个特殊方法 __len__() ，它返回元素的个数。 数学运算 如果要让Rational类(有理数)进行 + 运算，需要正确实现 __add__ ： class Rational ( object ): def __init__ ( self , p , q): self.p = p self.q = q p、q 都是整数，表示有理数 p/q。 class Rational ( object ): def __init__ ( self , p , q): self.p = p self.q = q def __add__(self, r) : return Rational ( self . p * r . q + self.q * r.p, self.q * r.q) def __sub__(self, r): return Rational(self.p * r.q - self.q * r.p, self.q * r.q) def __mul__(self, r): return Rational(self.p * r.p, self.q * r.q) def __div__(self, r): return Rational(self.p * r.q, self.q * r.p) def __str__(self): d = 1 for i in xrange(2,min(self.p, self.q)+ 1 ): if self . p %i == 0 and self . q%i==0: d = i return '% s/%s' % (self.p/d, self.q/ d ) __repr__ = __str__ 类型转换 要让 int() 函数对于Rational类正常工作，只需要实现特殊方法 __int__() : 同理，要让 float() 函数正常工作，只需要实现特殊方法 __float__() 。 @property class Student ( object ): def __init__ ( self , name , score ): self . name = name self . __score = score def get_score ( self ): return self . __score def set_score ( self , score ): if score < 0 or score > 100 : raise ValueError ( 'invalid score' ) self . __score = score 使用 get/set 方法来封装对一个属性封装. 但是写 s.get_score() 和 s.set_score() 没有直接写 s.score 来得直接。 可以用装饰器函数把 get/set 方法\"装饰\"成属性调用： class Student ( object ): def __init__ ( self , name , score ): self . name = name self . __score = score @property def score ( self ): return self . __score @score . setter def score ( self , score ): if score < 0 or score > 100 : raise ValueError ( 'invalid score' ) self . __score = score 第一个score(self)是get方法，用 @property 装饰，第二个score(self, score)是set方法，用 @score.setter 装饰， @score.setter 是前一个 @property 装饰后的副产品。对 score 赋值实际调用的是 set方法。 slots 由于Python是动态语言，任何实例在运行期都可以动态地添加属性。 如果要限制添加的属性，例如，Student类只允许添加 name、gender和score 这3个属性，就可以利用Python的一个特殊的 __slots__ 来实现。 顾名思义， __slots__ 是指一个类允许的属性列表 (所以是类属性)： class Student ( object ): __slots__ = ( 'name' , 'gender' , 'score' ) def __init__ ( self , name , gender , score ): self . name = name self . gender = gender self . score = score >>> s = Student ( 'Bob' , 'male' , 59 ) >>> s . name = 'Tim' # OK >>> s . score = 99 # OK >>> s . grade = 'A' Traceback ( most recent call last ): ... AttributeError: 'Student' object has no attribute 'grade' __slots__ 的目的是限制当前类所能拥有的属性，如果不需要添加任意动态的属性，使用 __slots__ 也能节省内存。 call 在Python中，函数其实是一个对象： >>> f = abs >>> f.__name__ 'abs' >>> f(-123) 123 由于 f 可以被调用，所以，f 被称为可调用对象。 所有的函数都是可调用对象。 一个类实例也可以变成一个可调用对象，只需要实现一个特殊方法 __call__() 。 把 Person 类变成一个可调用对象： class Person ( object ): def __init__ ( self , name , gender ): self . name = name self . gender = gender def __call__ ( self , friend ): print 'My name is %s...' % self . name print 'My friend is %s...' % friend 现在可以对 Person 实例直接调用： >>> p = Person('Bob', 'male') >>> p('Tim') My name is Bob... My friend is Tim... 单看 p('Tim') 你无法确定 p 是一个函数还是一个类实例，所以， 在Python中，函数也是对象，对象和函数的区别并不显著 。","tags":"notes","title":"[python进阶课程] 面向对象编程"},{"url":"http://x-wei.github.io/algoII_week6_1_reductions.html","text":"Goal: classify problems according to computational requirements. bad new: for huge number of pbs we don't know... 1. Introduction to Reductions shifing gears: from individual problems to problem-solving models. from linear/quard to polynomial/exponential pbs from implementation details to conceptual framwork suppose we could (not) solve pb X efficiently ⇒ what else pbs could (not) we solve efficiently ? def. reduction Pb X reduces to pb Y if you can use an algo that solves Y to solve X. for an instance of pb X → transform it into an instance of pb Y → translate the solution for Y to solution for X. ex1. finding median can reduce to sorting... cost = NlogN+1 ex1. element distinctness can reduce to sorting... cost = NlogN + N 2. Designing Algorithms algo design: by reduction to problems that we know how to solve (sorting/shortest path/flow/...) ex1. convex hull reduces to sorting Gram scan algo... (discussed in algo-I course) cost = NlogN + N algo. Gram scan pick a point with smallest y-coord sort all points by polar angle wrt the picked point consider points in this order, discard points that creates clockwise turn ex2. undirected shortest path (nonneg weights) reduces to directed shortest path cost: ElogV + E algo. replace each undir-edge by 2 dir-edge... 3. Establishing Lower Bounds goal: prove that a pb requires (at least) a certain nb of steps. ex. any compare-based sorting requires NlogN compares. log(N!) = NlogN Bad news: very hard to estibalish lower bounds. Good new: can spread the lower bound NlogN by reducing to sorting (if cost of reduction is small). def. linear-time reduction pb X linear-time reduces to pb Y if X can be solved with: 1. linear nb of op for reduction 2. constant nb of calles to Y ex. almost all reductions we've seen so far... ex. proof of lower bound for convex hull prop. sorting linear-time reduces to convex hull (注意这次是反向的! ) pf. for an instance of sorting: x1 ... xn ⇒ convert to convex hull instance: (x1, x1&#94;2), ... , (xn, xn&#94;2) ⇒ implication: all (ccw-based) convex hull algo cannot be easier than NlgN ! (otherwise sorting would be easier..) lesson: Establishing lower bounds through reduction is an important tool in guiding algorithm design efforts. 4. Classifying Problems prove that pb X and pb Y have the same complexity: show X linear-time reduces to Y show Y linear-time reduces to X conclude that X Y have the same complexity (even if we don't know what it is) ex. sorting and convex hull... 一个囧囧的脑洞: ex. integer arithmetic reductions: integer multiplication integer multiplication: of two N-bit integers. Its complexity (unknown) is denoted as M(N) brute force: N&#94;2 ops → so M(N) = Omega(N2) many other integer ops can reduce to integer multiplication: what is M(N)? ex. linear-algebra reductions: matrix multiplication compute product of 2 N*N matrices. Its complexity (unknown) is denoted as MM(N) brute force: N&#94;3 operations that can reduce to matrix-multiplication: what is MM(N)? summary","tags":"notes","title":"[Algorithms II] Week 6-1 Reductions"},{"url":"http://x-wei.github.io/imooc_py_functional.html","text":"http://www.imooc.com/learn/317 函数式编程: 更抽象, 更脱离指令(计算机), 更贴近计算(数学). 不需要变量 (python允许有变量, 所以python非纯函数式) 高阶函数 闭包: 返回函数 匿名函数 高阶函数 变量可以指向函数 f=abs; f(-10) 函数名: 就是指向函数的变量 abs=len 高阶函数: 接收函数作为参数的函数 def add(x,y,f): return f(x)+f(y) add(-5, 9, abs) map() map() 是 Python 内置的高阶函数，它接收一个函数 f 和一个 list，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回。map()函数不改变原有的 list，而是返回一个新的 list。 def format_name(s): return s.title() print map(format_name, ['adam', 'LISA', 'barT']) reduce() reduce() 函数也是Python内置的一个高阶函数。reduce()函数接收的参数和 map()类似，一个函数 f，一个list，但行为和 map()不同，reduce()传入的函数 f 必须接收两个参数，reduce()对list的每个元素反复调用函数f，并返回最终结果值。 reduce(function, iterable[, initializer]) Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. If the optional initializer is present, it is placed before the items of the iterable in the calculation, and serves as a default when the iterable is empty. If initializer is not given and iterable contains only one item, the first item is returned. filter() filter()函数接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。 自定义sorted() sorted()也是一个高阶函数，它可以接收一个比较函数 cmp 来实现自定义排序，比较函数的定义是，传入两个待比较的元素 x, y，如果 x 应该排在 y 的前面，返回 -1，如果 x 应该排在 y 的后面，返回 1。如果 x 和 y 相等，返回 0。 返回函数 在函数内部定义一个函数 然后返回这个内部定义的函数. 返回函数可以把一些计算延迟执行 def calc_sum(lst): def lazy_sum(): return sum(lst) return lazy_sum 调用 calc_sum() 并没有计算出结果，而是返回函数: >>> f = calc_sum([1, 2, 3, 4]) >>> f <function lazy_sum at 0x1037bfaa0> 对返回的函数进行调用时，才计算出结果: >>> f() 10 闭包 函数 f 内部定义的函数 g 无法被外部访问 → 可以防止其他代码调用 g . def calc_sum(lst): def lazy_sum(): return sum(lst) return lazy_sum 注意: 发现没法把 lazy_sum 移到 calc_sum 的外部，因为它 引用了 calc_sum 的参数 lst 。 像这种 内层函数引用了外层函数的变量（参数也算变量），然后返回内层函数 的情况，称为闭包（Closure）。 闭包的特点是返回的函数还引用了外层函数的局部变量，所以，要正确使用闭包，就要 确保引用的局部变量在函数返回后不能变 。 ex: # 希望一次返回3个函数，分别计算1x1,2x2,3x3: def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fs f1, f2, f3 = count() 以为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果全部都是 9 ! 原因就是当count()函数返回了3个函数时，这3个函数所引用的变量 i 的值已经变成了3。 函数只在执行时才去获取外层参数i , 由于f1、f2、f3并没有被调用，所以，此时他们并未计算 i*i，当 f1 被调用时i已经变为3... 上面的正确写法是: def count(): fs = [] for i in range(1, 4): def f(j=i): return j*j fs.append(f) return fs f1, f2, f3 = count() print f1(), f2(), f3() 因此，返回函数不要引用任何循环变量，或者后续会发生变化的变量。 匿名函数 Python中，对匿名函数提供了有限支持。 关键字 lambda 表示匿名函数，冒号前面的 x 表示函数参数。匿名函数有个限制，就是只能有一个表达式，不写return，返回值就是该表达式的结果。 map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]) myabs = lambda x: -x if x < 0 else x >>> myabs(-1) 1 装饰器 问题: 定义了函数, 想在运行时增加函数功能同时不改动函数代码. ex. 希望函数调用时打印调用日志 ⇒ 方法: 高阶函数: 接收要修改的函数, 进行包装后返回包装过的新函数. def new_f(f): def fn(x): print 'call %s()' % f.__name__ return f(x) return fn 函数 new_fn 就是所谓装饰器函数. python的@语法可以简化装饰器调用: (注意: 右边代码, 原本未装饰的f1函数已经被彻底隐藏了. ) 优点: 极大简化代码. 无参数decorator 上面例子里面的 new_fn 函数只能装饰接收一个参数x的函数, 想要处理接收任意参数的函数 ⇒ 利用Python的 *args 和 **kw def log(f): def fn(*args, kw): print 'call %s() in %s'%( f. name , time.ctime() ) return f(*args, kw) return fn 带参数decorator 接上面的log函数, 如果有的函数非常重要，希望打印出'[INFO] call xxx()...'，有的函数不太重要，希望打印出'[DEBUG] call xxx()...'，这时，log函数本身就需要传入'INFO'或'DEBUG'这样的参数，类似这样： @log('DEBUG') def my_func(): pass 把上面的定义翻译成高阶函数的调用，就是： my_func = log('DEBUG')(my_func) 再展开一下： log_decorator = log('DEBUG') my_func = log_decorator(my_func) 相当于: log_decorator = log('DEBUG') @log_decorator def my_func(): pass 所以，带参数的log函数 首先 返回一个decorator函数 ，再让这个decorator函数接收my_func并返回新函数 ： def log(prefix): def log_decorator(f): def wrapper(*args, **kw): print '[%s] %s()...' % (prefix, f.__name__) return f(*args, **kw) return wrapper return log_decorator @log('DEBUG') def test(): pass print test() 这里用到了闭包: 最里层wrapper函数(即修饰过个函数)用到了prefix参数. 完善decorator 上面的decorator会修改函数名: 在没有decorator的情况下，打印函数名： def f1(x): pass print f1.__name__ ⇒ 输出： f1 有decorator的情况下，再打印函数名： def log(f): def wrapper(*args, **kw): print 'call...' return f(*args, **kw) return wrapper @log def f2(x): pass print f2.__name__ ⇒ 输出： wrapper 这对于那些依赖函数名的代码就会失效。decorator还改变了函数的 __doc__ 等其它属性。如果要让调用者看不出一个函数经过了@decorator的\"改造\"，就需要 把原函数的一些属性复制到新函数中 ： def log(f): def wrapper(*args, **kw): print 'call...' return f(*args, **kw) wrapper.__name__ = f.__name__ wrapper.__doc__ = f.__doc__ return wrapper 这样写很不方便, Python内置的 functools 可以用来自动化完成这个\"复制\"的任务： import functools def log ( f ): @functools.wraps ( f ) def wrapper ( * args , ** kw ): print 'call...' return f ( * args , ** kw ) return wrapper functools.wraps(f) 是一个装饰器函数, 目的是为了把最后返回的函数再次装饰(复制f的属性进去)... 所以对于带参数的装饰器, 应该在最里面返回的wrapper函数前加上 @functools.wraps(f) import time , functools def performance ( unit ): def perf_decorator ( f ): @functools.wraps ( f ) def wrapper ( * args , ** kw ): print 'call %s () in %s %s ' % ( f . __name__ , time . time (), unit ) #closure return f ( * args , ** kw ) return wrapper return perf_decorator @performance ( 'ms' ) def factorial ( n ): return reduce ( lambda x , y : x * y , range ( 1 , n + 1 )) print factorial . __name__ 偏函数 假设要转换大量的二进制字符串，每次都传入 int(x, base=2) 非常麻烦，于是，我们想到，可以定义一个int2()的函数，默认把base=2传进去： def int2(x, base=2): return int(x, base) functools.partial 可以把一个参数多的函数变成一个参数少的新函数，少的参数需要在创建时指定默认值，这样，新函数调用的难度就降低了。 functools.partial(func[,*args][, **keywords]) Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords. import functools int2 = functools . partial ( int , base = 2 ) >>> int2 ( '1000000' ) 64 >>> int2 ( '1010101' ) 85 sorted_ignore_case = functools . partial ( sorted , key = lambda s : s . lower ()) print sorted_ignore_case ([ 'bob' , 'about' , 'Zoo' , 'Credit' ])","tags":"notes","title":"[python进阶课程] 函数式编程"},{"url":"http://x-wei.github.io/cpp-demo-snippets.html","text":"总结了一下C++ STL里面用的比较频繁的一些代码片段. (地址: https://github.com/X-Wei/cpp-demo-snippets/tree/master/STL ) cpp文档: http://en.cppreference.com/w/cpp 常用的library主要有: <algorithm>, <vector>, <queue>, <set>, <map>, <cmath> 另外一个常见的cpp文件开头版本是: #include <iostream> #include <vector> #include <algorithm> using namespace std ; #define forloop(i,lo,hi) for(int i = (lo); i <= (hi); i++) #define rep(i,N) forloop(i,0,(int)N-1) algorithm copy() OutputIt copy( InputIt first, InputIt last, OutputIt d_first ); Copies the elements in the range, defined by [first, last) , to another range beginning at d_first . 注意如果要放入的container大小不够, 最后一个参数要用 back_inserter . ex. int a[] = {1,2,3,4,5}; vector<int> v(5); copy(a, a+5, v.begin()); vector<int> v2; // if v2 needs to increase capacity, need to use back_inserter copy(v.begin(), v.end(), back_inserter(v2)); sort() void sort( RandomIt first, RandomIt last, [Compare comp]); Sorts the elements in the range [first, last) in ascending order. int a[] = {3,1,5,0,8,9}; vector<int> v(&a[0], &a[0]+6); sort(a, a+6); sort(v.begin(), v.end()); 如果想要降序排列, 可以直接reverse一下: reverse(v.begin(), v.end()); 如果自定义比较函数的话, 可以自己写一个cmp函数(内容和重载的小于运算符相同), 然后把函数名放在第三个参数: bool my_cmp(const pair<int, int> &lhs, const pair<int, int> &rhs) { return (lhs.first < rhs.first) || (lhs.first==rhs.first && lhs.second < rhs.second); } //... sort(vp.begin(), vp.end(), my_cmp); 或者把元素封装为class/struct, 然后重载它的小于号 < 运算符: class MyFooClass { public : int x , y ; MyFooClass ( int xx , int yy ){ x = xx ; y = yy ; } bool operator < ( const MyFooClass & other ) const { return ( x< other . x ) || ( x== other . x && y < other . y ) ; } }; struct MyFooStruct { int x , y ; MyFooStruct ( int xx , int yy ){ x = xx ; y = yy ; } bool operator < ( const MyFooStruct & other ) const { return ( x< other . x ) || ( x== other . x && y < other . y ) ; } }; vector vector 相当于java里面的 ArrayList . 主要操作: push_back() , pop_back() (所以可以当作stack使用). 另外iterator操作也很常用(set, map等同理): for(vector<int>::iterator it=v1.begin(); it!=v1.end(); it++) cout << *it << \" \"; pair 就是一个first一个second. 另外有 make_pair 函数可以构造pair. pair<string, int> p1(\"pair1\", 1); pair<string, int> p2 = make_pair(\"pair2\", 2); 另外两个尖括号再一起时一定要中间加空格, 否则就是位操作运算符了! pair<string, pair<int, double> > p3 = make_pair(\"pair3\", make_pair(3,3.33)); queue 包含普通队列和优先队列(pq) queue bool empty() const; reference front(); void push( const value_type& value ); void pop(); priority_queue (pq的实现也可以用 <algorithm> 里的 make_heap , pop_heap , push_heap 等方法. priority_queue 其实就是algorithm里面函数的封装...) 和queue的API区别是, pq查看队首的函数叫 top 而不是 front . bool empty() const; const_reference top() const; void push( const value_type& value ); void pop(); 使用自定义的cmp方法, 可以把cmp的内容作为括号运算符 () 的重载放入一个struct作为第三个类型参数, 第二个是container, 一般用vector即可. struct MyCmpStruct{ bool operator()(const pair<int, int> &lhs, const pair<int, int> &rhs){ return return (lhs.first < rhs.first) || (lhs.first==rhs.first && lhs.second < rhs.second); } }; priority_queue<pair<int,int>, vector<pair<int,int> >, MyCmpStruct> ppq; 另一种方法是自定义元素的struct/class, 然后重载小于 < 运算符为cmp (同sort). set/map C++里面的set/map是用红黑树实现的, 所以key类型需要支持比较运算. set/map也支持iterator操作( begin() , end() ), 而且由于是BST, 顺序自然是排好了的. set std : :pair < iterator , bool > insert ( const value_type & value ); void erase ( iterator pos ); size_type count ( const Key & key ) const ; iterator find ( const Key & key ); insert 和 erase 的参数可以是iterator或者value. count 的返回值为1或0 find 如果没找到则返回 s.end() map map的value_type是一个 pair<K,V> , 所以遍历是这样: for(map<string,int>::iterator it=m.begin(); it!=m.end(); it++) cout << it->first << \":\" << it->second << \", \"; insertion: 用 [] 或者 insert 函数 map<string,int> m; m[\"aa\"] = 3; m.insert( make_pair(\"dd\", 6) ); 注意 insert 函数如果key已经存在的话value不会改变! 但是用 [] 的话则可以. erase/find/count : 同set.","tags":"tech","title":"C++ STL小结&代码片段"},{"url":"http://x-wei.github.io/algoII_week5_2.html","text":"1. Introduction to Data Compression pb: reduce the size of a file, to save space/time for storing/transmitting. applications: generic file compression(gzip), multimedia (mp3), communication(skype). From binary data B , ⇒ generate a compressed representation C(B) . lossless compression : get exactly B from C(B) compression ratio : |C(B)|/|B| (||means number of bits ) Tools extension to stdio libraries: read/write bits public class BinaryStdIn{ boolean readBoolean(); // read 1 bit and return as a boolean char readChar(); // read 8 bits and return a char char readChar(int r); // read r (<=8) bits and return a char // similar method for int/long/double boolean isEmpty(); // is bitstream empty? void close(); } public class BinaryStdOut{ void write(boolean b); // write 1 bit void write(char c); // write 8 bits void write(char c, int r); // write r (least-significant) bits of c // similar method for int/long/double void close(); // might add some byte alignment bits } usage example: store a date 12/31/1999 universal date compression? prop . NO algorithm can compress every bitstring. pf. by contradiction: repeatedly compress the bitstring ⇒ bit length goes to 0. 2. Run-Length Coding one simple type of redundancy in bitstream: long runs of repeated bits. ⇒ use 4-bit counts to represent alternating 1s and 0s. question1: how many bits to represent counts ? ⇒ pick 8 bits (just tradeoff) question2: what if run length is bigger than max length(2&#94;8)? ⇒ just add length=0 of the alternating bit... application: JPEG... code: public class RunLength{ private final static int R = 256; // max run length private final static int lgR = 8; public static void compress(){ boolean bit, oldbit = false; int l=0; while(!BinaryStdIn.isEmpty()){ bit = BinaryStdIn.readBoolean(); if(bit!=old){ oldbit = bit; BinaryStdOut.write(l,lgR); l = 0; } else if(l==R-1){ BinaryStdOut.write(l,lgR); BinaryStdOut.write(0,lgR); l = 0; } l ++; } BinaryStdOut.write(l,lgR); BinaryStdOut.close(); } public static void expand(){ boolean bit = false; while(!BinaryStdIn.isEmpty()){ int l = BinaryStdIn.readInt(lgR); for(int i=0;i<l;i++) BinaryStdOut.write(bit); bit != bit; } BinaryStdOut.close(); } } 3. Huffman Compression classique... idea: variable length code . ex. Morse code, more freq chars use less chars. → pb: ambiguity? one code is the prefix of another... → need to use prefix-free code ⇒ use a trie to represent the prefix-free code, in bitstream, use a binary trie: Huffman Trie Node define trie nodes having frequences: public static class Node implements Comparable<Node>{ private char ch; // only used for leaves, null for internal nodes private int freq; private final Node left, right; // left-->0, right-->1 public Node(char ch, int freq, Node left, Node right){//...} public boolean isLeaf(){ return left==null && right==null;} public int compareTo(Node that){ return this.freq-that.freq;} } implementation expansion in the bitstream, we first put the (serialized) trie in the head. public void expand{ Node root = getTrie(); // int N = BinaryStdIn.readInt(); // number of chars in string for(int i=0; i<N; i++){ Node x = root; while (!x.isLeaf()){ if(BinaryStdIn.readBoolean()) x = x.right; else x = x.left; } BinaryStdOut.write(x.ch); } BinaryStdOut.close(); } output trie ie. to serialize a trie. ⇒ use preorder traversal : 0 for internal nodes, and 1 for leaf (followed by the corresponding char) recursive method: private static void writeTrie(Node x){ if(x==null) return; if(x.isLeaf()){ BinaryStdOut.write(true); // leaf node BinaryStdOut.write(x.ch); // followed by the char } else { BinaryStdOut.write(false); // internal node writeTrie(x.left); writeTrie(x.right); } } read trie Reconstruct the trie from the serialized bitstring. preorder(x) = x+preorder(x.left)+preorder(x.right) ⇒ recursive method... 这个递归函数还是蛮有代表性的, 值得看. private static Node readTrie(){ if(BinaryStdOut.readBoolean()) { char ch = BinaryStdOut.readChar(); return new Node(ch, 0, null, null); } else { Node left = readTrie(); Node right = readTrie(); return new Node('\\0', 0, left, right); // subtle recursion } } Huffman algorithm pb: how to find the best prefixless code? Shannon-Fano algo: partition symbols S into 2 subsets: S1, S0, with roughly equal freq code in S1 start with 1 and in S0 start with 0 recur on S1 and S0 [Huffman algo] (1950) count letter freq in input build a node for each char select the 2 tries with minimum weight(freq), merge them, and put it back repeat until we get only 1 trie application: jpeg, pdf, mp3, ... implementation private static Node buildTrie(int[] freq){ MinPQ<Node> pq = new MinPQ<Node>(); for(char i=0; i<R; i++) pq.insert( new Node(i, freq[i], null, null) ); while(pq.size()>1){ Node x = pq.pop(), y = pq.pop(); pq.insert( new Node('\\0',x.freq+y.freq, x, y); } return pq.top(); } prop . Huffman algorithm produces the optimal prefixless code. running time: N + RlgR 4. LZW Compression idea: adaptive model: learn and update the model as you read the text. decoder will do the same thing. build a table of not only mapping chars to codes, but also mapping >=2-char combinations to codes(of fixed width). when encoding strings, look for the longest prefix that is in our table. LZW compression LZW compression algo create a symbol table mapping string keys to W-bit code, initialize as mapping only chars to codes find longest prefix s of the unscanned input write out the corresponding code add s+c into the symbol table, where c is the next char in input Use a trie for representing the code table → because support longest prefix match. implementation public void compress(String input){ TST<Integer> st = new TST<Integer>(); // a trie symbol table for(char i=0; i<R; i++) st.put(\"\"+(char)i, i); int code = R+1; // R is used as \"stop\" while(input.length()>0){ String s = st.longestPrefixOf(input); BinaryStdOut.write(st.get(s), W); int l = s.length(); if(l<input.length()) st.put(s+input.charAt(l), code++); input = input.substring(l); } BinaryStdOut.write(R, W); // write \"stop\" BinaryStdOut.close(); } LZW expansion no need to get the codeword table , the input is just the compressed bitstring. maintain a (reverse)table mapping int(code) to strings. as we decode the string, we add new entries to our table. to represent the table: just an array. note when add new entries: add key = s+c , where s is the last decoded string, c is the first char of the currently decoded string. example: when decoded ABR, s = BR, currently decoded string = ABR, so we add BRA. tricky case : ABABABA compression problem in expansion when reading the \"83\": → need to be able to update the table when encountering a code not yet defined... Summary Huffman: fixed-length symbols, with variable-length codes. LZW: variable-length symbols, with fixed-length codes. theoretical limit: 还剩下一周的内容, 可能要等到二月底考完以后有时间再看了, 现在先复习备考...","tags":"notes","title":"[Algorithms II] Week 5-2 Data Compression"},{"url":"http://x-wei.github.io/2015-summary.html","text":"I. 工作 学业 三月初结束了在X的最后几门考试, X的课从来都不简单, 但我真的很享受, 尤其是那些数学课. (不过刚考完就sb了: 陪伴我两年的杯子丢了...) Polytechnique, 从憧憬变成回忆. X的各种经历, 三天三夜也说不完. 然后九月来到ETH, 课程很有意思, project比较多. 第一个学期说实话有点应付, 所以现在要好好复习备考...orz 实习 在MEC实习了五个月, 这里的工作环境真是宽松. 一宽松我有点担心没有什么进步, 不过后来看这一段时间我的进步还是很大的. 实习期间做的是音乐分类工作, 实践了一些NLP的流程, 试用了不少算法, 以及读了一些paper... 有幸和神童Aranud(X11 major)一起工作, 感叹智商不够, 只能努力来凑. 然后顺利通过了实习的答辩, 算是给X的三年划上了圆满的句号 — 到2017年remise再见了! 面试 这是最后三个月的主旋律, 今年算起来经历了好多面试: AL: 三轮电面, 深感ML方面的知识不够扎实, 不过居然过了, 看来他们真的很缺人... GG: 在充分的准备之后经历了两轮电面五轮现场, 拿下dream offer, 感觉很幸运... 非常感谢我的recruter, Marta. JS: 一场电面加三场现场面试, 被X校友的问题难倒了...orz 他们家面试很有难度, 继续仰望了. Jump: 一轮电面和三轮现场, 这个比较奇葩, 面试了一个quant的职位. 问题主要是proba和数学, 目测基本没戏, 不过就算体验生活了, 还顺便去伦敦旅游了一圈... II. 学习 读书 实习的时候开始, 每天上下班在tram上就可以看不少书, 后来上下学在公交车上也看, 这样积攒下来还是读了不少: 黑客与画家 : 涉及了黑客文化, 社会制度, 创业, 编程语言等各个方面, 启发不小. 把时间当作朋友 : 鸡汤再品, 读到一句\" 越是有意义的事情越要慢慢做 \", 于是耐心撸了四五个月的算法. 觉得有这么一句点醒人的话, 这本书就没白读. 啊哈算法 : 高中计算机竞赛水平的读物, 但是我读得津津有味... 读完后对于一些经典算法的理解加深了不少, 期待续集... 第一行代码 (看了一半): 据说是安卓开发最好的入门书, 看了前面几章感觉一些以前不清楚的概念理解深一点了, 可惜实习后期忙着写报告就没有继续看下去... algorithms, 4th ed (看了一半不到): 配合普林斯顿那门MOOC看的, 老爷子讲的太好了, 非常复杂的算法到了由他慢慢道来发现并没有很难. 书中的各种插图太赞, 可以作为各种算法的参考手册. 有必要好好读一遍. 我是一只IT小小鸟 : 又一本鸡汤, 这本书是09年出版的, 后悔自己没提早几年读到这本书... 怪诞心理学 : 里面很多实验设计的很有意思, 我们的决策很多都不是理性的, 这本书让我从新审视日常生活中的一些决策. 编程珠玑 : 神书, 里面的文章和后面的练习题让人脑洞大开, 关于内存占用的一些奇怪的优化现在看来可能不是很重要. 值得二刷. Cracking The Coding Interview, 6th ed : 在收到G现场面试通知的那天得到了这本书, 感觉像游戏玩到一个地方拿到了道具. 这本书的第六版比以前丰富了不少内容, 面试前花两星期匆匆过了一遍 感觉很有帮助. 拖延心理学 : 感觉这本书不能算是鸡汤, 真的很推荐, 后面关于父母育儿以及xx综合症的章节略无聊可以不看. 关于固定心态和成长心态的内容让我很受启发. 很推荐这本书, 毕竟 除了强健身体以外 没有什么事情比优化大脑更重要了 . 公开课 今年没有像去年一样猛刷公开课... Introduction to Big Data with Apache Spark edx上一门比较简单的课, 只要跟着看跟着做就可以了, 第一次接触spark, 和实习用的cosmos SCOPE一比感觉好心累... kivy crash course 严格来说不算公开课, 就是实习期间学习kivy的时候在youtube上找到的一系列视频. 每节只有十分钟, 循序渐进, 不错的kivy上手教程. 希望明年有时间再来鼓捣一下它.. Algorithms, part I 目前为止CS方面看到过的最好的一门课. 实习完了以后在家撸的, 边看视频边记笔记, 每周要花七八个小时, 收获实在太大了. 讲课的老爷子是红黑树的发明人, 在他的讲解下, 像红黑树(包括第二部分的KMP和regexp)等复杂算法变得特别好理解了! 这门课我只看了视频还没有做每周的assignment, 所以有可能以后再刷一遍... Algorithms, part II 上一门课的第二部分, 九月份开始看的, 现在还差一周半的视频没看完. 这门课实在太赞了, (自以为)刷完内功直接上一个档次. blog 今年在写作方面是一个突破, 数了一下, 居然写了42篇! 而且这四十多篇基本没有太水的文章. 我认为知识分为\"术\"和\"道\". \"术\"是一些实用的技能, 只是涉及一个功能怎么实现, 或者一个特定的工具怎么使用而已; \"道\"则是更基本的知识, 算法知识就是\"道\", 是永远不会过时的东西. 2015年我的大部分文章都是关于算法的, 这和以前一些文章只是记录实现xx功能需要xx命令/工具相比, 算是很大的进步. 今年新开了一个板块\"notes\", 看公开课的时候用zim记笔记, 整理了以后发到网上. 相当于是我的一个在线版笔记了, 这些文章记录的比较随意, 可能别人看不太明白, 不过对于我确实很好的参考资料. 以后看MOOC也希望继续这个模式. 另外最后几个月写了一些经典算法的总结, 这个系列还没有写完, 我的希望是能做一个基础算法和数据结构的 地图 , 把常见的算法都用自己的语言描述&点评一下. 刘未鹏说过\" 书写是最好的思考 \", 深以为然. 另外在2015年最后一天, 博客的访问ip数终于超过了8k, 有点小激动. 新技能 LP 二月份和我的cousin, Manu, 一起做了INF580(programmation par contraintes)的 projet . 这个projet感觉是在X三年里最得意的一个. 我们建立了非常漂亮的线性规划(LP)模型, 描述了一个类似欧拉回路的问题. 但是事情并没有一帆风顺, 我们一开始没有考虑integrality的问题, 但是最后一刻峰回路转, 稍微变通一下以后居然变得可解了! 最后的结果完爆当年的ens冠军. 不过, \"当你手上有个锤子, 看什么东西都像钉子\". 2015年的hashcode预赛, 我们翘掉cérémonie参赛, 还想用LP, 结果计算超时了就... 后来发现只要用贪心算法就可以妥妥的了... anyway, 从此兵器库里多了LP这个重型武器. 盲打 以前看到Carlos在练习打字, 后来实习的时候也尝试不看键盘打字. 首先是需要使用标准指法, 我以前打字几乎不用小指和无名指, 所以一开始改到标准指法分分钟想砸键盘... 坚持了两三周以后渐渐适应了就. 这个网站练打字很不错: http://www.keybr.com/ 现在非常享受在(外接)键盘上啪啪啪打字, 简直爽... 现在打字最快能到60wpm. 另外, 把键盘capslock映射到ctrl, 简直就是手指的福音啊. android dev 实习的时候, 由于每天上班很晚, 有段时间我一大早就去CTU的自习室看一个小时的\"第一行代码\", 边看边写一下书里的实例. 实习的后期帮一帮医学startup的同学们做了个app, 类似4*4的candy crush. 虽然代码写的很dirty, 但是看到程序运行还是蛮有成就感... kivy kivy是一个python的GUI程序框架, 实习期间玩了一下. 感觉它的设计非常喜欢, 比安卓的要优雅不少. 先用scrapy爬了点一个的文章, 然后做了个简单的 文章阅读的应用 , 可惜后来尝试打包为安卓程序一直没成功... anyway... TeXmacs 这个神器今年写了两个报告, 实习报告都使用它写的(我觉得要是用tex我就根本写不完报告了要), 基本掌握了它的用法. 用了TeXmacs再看tex系的工具, 有一种信息化时代看石器时代的感脚. 后来写了篇 博客 专门介绍TeXmacs的使用, 希望更多人了解这个工具. 撸算法 2015年做的最有意义的一件事, 包括两百多道leetcode (一共有76天在做leetcode), algo公开课, CTCI, 编程珠玑等几本书. 成果不错. 有意义的事情就值得慢慢做, 16年要继续撸算法, 慢慢来不着急. III. 生活 签证&房子 实习期间4A的结果出来了, 下决定去ETH以后, 就开始准备签证的各种材料. 我以前在这种事情上一直拖延, 但是今年做的很好. 有的事情是不能拖延的, 切记. 后来去伦敦的签证也是, 办得很顺利. 八九月份找房子也是一件大事, 请假专门去苏寸跑了一趟, 而且很幸运遇到了交大学长租给我们性价比超高的房子. 这些事情其实蛮消耗精力, 而且并不能够完全掌控. 但是作为我自己, 只求做最充分的准备了. 运动 plank 今年最得意的一件事情, 就是坚持平板支撑两百多天. 2015年1月1日, 我下载了app尝试了一下平板支撑, 撑了3分钟整. 但是坚持每天都抽出来几分钟, 一开始每天都努力比昨天多撑5到10秒, 渐渐的可以听完一首歌了. 到了20天的时候就可以勉强撑10分钟. 后来我每天边看平板上的动画片边支撑, 渐渐可以撑半集时间, 后来是一整集的时间了. 而且我规定要看动画片只能撑着看, 所以即使某天累了也有动力撑一下. 就这样2015年我看完了数码宝贝第一季(54集)以及灌篮高手的一半. 手机里装了一个叫\"种子习惯\"的app, 2015年我只有这件事情坚持的最久. 虽然是一件每天只要5分钟就可以的事情, 但是坚持每天都做, 总会有点收获吧. 来句鸡汤: \" 慢慢来, 时间和节奏可以战胜一切\" . 其他 实习期间在CTU每天下班在屋里基本会做一点运动, 除了plank以外, 还会做做俯卧撑和深蹲. 另外买了健身神器: 弹力绳, 有了它基本上在屋里就可以锻炼所有的部位了. 跑步不是很多, 心情好了会去CTU或者park跑一圈, 基本属于三天打鱼两天晒网的节奏. 在CTU记得有一天早晨刚起来, 从窗户里看到李qf同学已经出门跑步了. 吃完早饭又看到他跑步回来: 大约跑了四十分钟. 李同学提到他一般六点多起来, 出去跑一圈回来洗个澡然后吃个饭去上班. 不禁对李同学肃然起敬, 学习的榜样... 后来到了苏村, 空气环境很好, 但是跑步也不是很勤, 基本一周也就跑个两三次, 而且每次只跑十分钟... 这时的跑步更多是作为一种心乱的时候的调节方式. 跑步时什么也不想, 只关心脚下的路以及呼吸的节奏, 跑完出一身汗, 洗个澡, 感觉又精力满满了... 记录 每天晚上都会写一个excel文件, 记录一天的得失以及每天的事件, 这件事情今年坚持的特别好, 几乎没有连续好几天不记录的情况. 这样我可以知道过去的一年的每一天我都做了什么. 自以为我做的表格挺不错的, 继续坚持记录, 另外希望有朝一日把它做成个app. IV. 娱乐 音乐 由于实习期间的工作就是和音乐相关, 近水楼台得到了xbox音乐一年的免费, 于是大量的听老鹰... 同事都吐槽我了... 这一年发现(&再发现)比较喜欢的歌大概有: Bitter Creek (Eagles): 结尾的斑鸠琴简直要上瘾 King of Hollywood (Eagles): 只是觉得好听 Ol'55 (Eagles): as always Train Leaves Here This Morning (Eagles): as always Center of The Universe (Eagles) Closer (Travis) Rocky Mountain Way (Eagles): 这首歌的现场版单曲循环... 歌词是nonsense, 但是Joe Walsh的吉他燃爆了, 以前觉得他用那个talbox就想跳过, 现在只觉得爽... butterfly (数码宝贝主题曲) 直到世界尽头 Hotel California (Eagles): as always, 但是今年在youtube上发现一个后来Don Felder的独奏版本, 又要单曲循环了: 纪录片 History of the Eagles 老鹰乐队官方纪录片, 每次坐三个小时以上的火车就会拿出来看. 刷过三四遍了已经. 里面有几句话印象很深: \"Shit don't float\" \"Perfection is not an accendent\" \"Time passes, and things change\" Cosmos A SpaceTime Odyssey 一开始是飞机上看到的, 感觉很震撼, 后来下载了全集. 人不过是宇宙中一粒尘埃上的细菌... Tous sur Orbite 法语的天文科普片, 52集对应一年的52周. 动画做的太赞了. 地球就是我们的宇宙飞船, 每次仰望天空的时候我就想到这句话. 动画片 这些都是平板支撑的时候看的... 数码宝贝(一) 感觉这部动画, 无论是故事性, 思想性, 美术, 配乐, 甚至是歌词, 都实在是太完美了... 八个孩子我都非常喜欢(一般我不喜欢主角的光环, 但是八神太一例外). 这部动画, 即使十三年以后再看还是那么好看. 太多的经典的镜头了, 最难忘的就是最后一集的飞帽杀, 帽子飞起来在天上的时候一下子戳了泪点. 灌篮高手 正在看, 看了一半. 有些桥段笑死了... 不过, 好像樱木花道作为主角一开场就有高出别人一截的体能, 这一点是我非常不喜欢的. 现在我有一种信念, 就是在任何方面的出类拔萃背后都是有着超过常人的付出. 库里之所以变态准, 难道不是他在不断联系各种高难度动作下的投篮么? 鸡汤一句: 以大多数人的努力程度, 根本用不着拼天赋. 电影 由于周末比较闲, 看了不少电影, 而且还为了葡萄账户专门注册了个交大校友邮箱... 看的几部印象深刻的电影有: 黑客帝国 亲爱的 ExMachina 奇妙世纪: 其实是一系列微电影 滚蛋吧肿瘤君: 满分&收藏 谍影重重三部曲: 伯恩帅爆了 头脑特工队: 脑洞NB 心迷宫: 剧情NB 印象不深刻的有: 前目的地 捉妖记 终结者5 碟中谍5 煎饼侠 侏罗纪世纪 十二公民 明日世界 黑猫警长之翡翠之星 还有各种网剧/综艺: 万万没想到, 报告老板, 暴走大事件, 脑残师兄, 跑男... (周末真是闲的蛋疼...) 旅游 年初, 玩了荷比卢德四国五日游 夏天参观了巴黎一个CERN的展览 跟新欧洲的15欧团去Dieppe海边吹了一天风 年末在学弟带领下去意大利浪了八天... 作为一个比较宅的人旅游了这么多地方已经不容易了... 2016 Cannot expect more from 2015, 对我来说这一年几乎没什么遗憾的. 新的一年, 希望家人健康平安. 至于我自己, 希望能在体力, 脑力和意志力上能更进一步. 继续锻炼继续学习, 继续读书继续写作继续记录. Bonne année !","tags":"misc","title":"2015年终总结"},{"url":"http://x-wei.github.io/algoII_week5_1.html","text":"1. Regular Expressions pb: pattern matching. regular expression Is a notation to specify a set of strings. basic operations: concatenation or closure: \"0 or more appearances of chars\" parentheses additional operations ( added for convinence ): ex. [A-C]+ is equivalent to (A|B|C)(A|B|C)* . 吐槽名句: 2. REs and NFAs duality between RE and DFA: RE: to decribe a set of strings. DFA: machine to ecognize whether a string is in a given set. [Kleene's therom] For any DFA, there exists a RE that describes the same set of strings; For any RE, there exists a DFA that recognizes the same set of strings. first attempt of pattern matching (Ken Tompson) same as KMP — no backup. basic plan: construct the DFA simulate the DFA with text bad news: DFA may have exponential nb of states. ⇒ change to NFA (nondeterministic finite automaton). NFA put RE into parentheses every char as a state (start=0, success=M) — 这里和之前的DFA很不一样: 之前是每个transition(edge)关联一个char, 这里是每个状态(node)关联一个char. epsilon-transition (red links below): change of machine state without scanning text match-transition (black links below): change state, but also have to scan next char in text, match transition is added after each alphabetic char success (accept) if any sequence of transitions (after scanning all text) end at state-M. 亦可理解为, DFA是每一条边对应一个可能的(字母表内的)char, 而NFA只有match-transition对应于pattern里的(alphabetic) char, 其他epsilon transition的边对应空字符串(也就是epsilon string). example: is \"AAAABD\" a match ? → yes. (和上一节substring的插图进行一下比对, 还是有很大不同) pb: non-determinism How to determine whether a string is a match of a NFA (ie. how to select the right sequence of transition) ? ⇒ sysematically consider all possible transition sequences. 3. NFA Simulation state names: 0 to M. (M+1 states in total, M=length of RE string). match-transitions: store in array re[] (the match transitions are naturally in order of the array). epsilon -transitions: store in a digraph G idea: maintain a set of all state that NFA could be in after reading first i chars in text. at each iteration: check all reachable state wrt the transitions, then update reachable states. algorithm (for the NFA above, 注意为了方便已经加了 必要的括号) [Algo] initial: rs (reachable state)=reachable state from state 0 (left parenthese) using epsilon trantisions consume a char in text : nrs ( new-reachable-states ) = empty set from all reachable state of this character: add next state using the match-transition to nrs add all reachable states (using epsilon transition) form the nrs set to nrs set rs = nrs , and consume the next char in text accept if at the end the state M is in rs concrete example init: when matching A from text: state 2 or 6 using match transition of A, we can get to state 3 or 7 if we add epsilon transitions: so reachable states after reading 1st A are: 2, 3, 4, 7 matching 2nd A from text: state 2 using match transition we can only get to state 3. using epsilon transitions from state 3: ( the only state after matching A is state 2 3 4 ) etc... 或者直接看这张图: reachability All reachable vertices from a set of source vertices → just DFS. ⇒ directly use the API from the digraph section: running time linear to E+V Java implementation API: public class NFA{ private int M; private char[] re; private Digraph G;// digraph of the epsilon-transitions public NFA(String regexp){ M = regexp.length(); re = regexp.toCharArray(); G = buildEpsilonTransitionGraph();// helper function to build the graph G } public boolean matches(String text);// does text match the regexp? private Digraph buildEpsilonTransitionGraph();// private helper function } The function buildEpsilonTransitionGraph() will be attacked in next session, for now we focus on the NFA simulation code — that is, the mathes() method. For simplicity let's assume we have a function reachableVertices(Digraph G, Bag<Integer> sourceSet) and reachableVertices(Digraph G, int source) that gives the reachable states from (a set of) source vertices, including the sources. Or we can directly use the DirectedDFS api as listed above. public boolean matches(String text){ //does text match the regexp? Bag<Integer> rechableStates = reachableVertices(G,0);// init reachable for(char c: text){ Bag<Integer> newRechableStatesBymatch = new Bag<Integer>(); for(int i:rechableStates) if(re[i]==c || re[i]=='.') newRechableStatesBymatch.add(i+1);// match transition rechableStates = reachableVertices(G,newRechableStates);//epsilon transition } return reachableStates.contains(M); } (代码虽然短但是这个过程我理解了好久.. 另外上面的代码有点伪). Analysis prop . the matches() method takes O(MN) time in worst case. pf. N chars in text, each char can go through <= M states (DFS), and in the digraph, no node has >3 degree ⇒ number of edges <= 3M , so the time for each dfs is O(M), in total we have O(MN). 4. NFA Construction → construct the epsilon transition digraph. buiding a NFA from a re (parsing) states in a NFA: one state per char, plus an accept state (state M) alphabet state: chars in alphabet ( A, B, C, D ) → (implicitly) put a match transition to next state metacharacters: ( ) . * | , 5 metacharacters in total ⇒ to deal with the metacharacters: paretheses ( ) simply put a epsilon-transition to the next state closure * 星号前面只可能是字母(包括 . )或者右括号 ) , 所以分两种情况讨论一下, 需要向前看一位, 这里就比较subtle for each * state, add 3 transitions as below: or | or符号肯定在一个括号里面 add 2 epsilon transitions wrt parethese: 以上就是NFA建立G的时候要处理的三种情形, 这三种情形都要知道一个左括号( lp )的位置 ⇒ use a stack ! implementation for alphabetic chars: do one-char lookahead → if next is * , add transitions. for left parenthese ( : add transition to next state, and push to stack for or | : add transition to next state, and push to stack for right parenthese ) : pop the stack to deal with or and lp; and also do lookahead. code is not trival... look carefully: private Digraph buildEpsilonTransitionGraph(){// private helper function Digraph G = new Digraph(M+1); Stack<Integer> stk = new Stack<Integer>(); int lp; for(int i=0; i<M; i++){ if(re[i]=='|' || re[i]=='(') stk.push(i); if(re[i]=='(' || re[i]==')' || re[i]=='*') G.addEdge(i,i+1); else if(re[i]==')'){// need to pop until get a lp int j = stk.pop(); if(re[j]=='|'){ lp = stk.pop(); int or = j; G.addEdge(lp, or+1);// add edge for the `or` case G.addEdge(or, i); } else lp = j; } // do the lookahead: if(re[i+1]=='*'){ if(re[i]==')'){ // case 1 of closure: a rp before `*` G.addEdge(lp, i+1); G.addEdge(i+1, lp); }else{ // case 2 of closure: an alphabetic char before `*` G.addEdge(i, i+1); G.addEdge(i+1, i); } } }// go through each char in re return G; } Analysis prop . building an NFA takes linear time and space in M. pf. for each char, the nb of operations is const. 真不愧是most ingenius algorithm we met in this course...... 5. Regular Expression Applications grep \" G eneralized R egular E xpression P rint\" print out all lines (from stdin) having a substring of an RE. ⇒ equal to adding a .* to the beginning and end of the RE to make a match. public class GREP{ public void main(String[] args){ String re = \".*\"+args[0]+\".*\"; NFA nfa = new NFA(re); while(StdIn.hasNextLine){ String line = StdIn.readLine(); if(nfa.matches(line)) StdOut.println(line); } } } the grep has NM worst case running time — same as brute force substring search — amazing... grep application: crossword puzzles regexp in other languages unix: grep, awk script: python, perl java: String.matches(regexp) Harvesting information goal: print all substrings of input that match an RE. use Pattern and Matcher class in java.util.regexp . first compile the regexp, then build the matcher → so that we can iterate through all matches of the input using find() and group() of the matcher Caveat : performance NOT guaranteed ! → exponential time growth! Not-so-regular expressions \"not rugular\" means Kleene's Th doesn't hold → efficient performance not tractable...... back-reference \\1 matches subexpressions that was matched earliser limitations of regular languages: Summary the substring and regexp are examples of compilers ! (from string to a NFA/DFA/bytecode)","tags":"notes","title":"[Algorithms II] Week 5-1 Regular Expressions"},{"url":"http://x-wei.github.io/heap-summary.html","text":"今天简单介绍一下优先队列(priority queue, 以下简称PQ)这个数据结构的实现. PQ又叫\"堆\"(heap), 但是可能优先队列这个名字更容易记忆它的用途: pq是一种队列, 不过不是先进先出(FIFO), 而是每次出队的元素永远是优先级最高的. logical structure of PQ 个人感觉\"堆\"这个名字大概源于PQ的(逻辑上的)形状吧: PQ是一种树(tree), 准确的说, 是一种二叉树(binary tree), 说得再准确一点, 它是一种 完全二叉树(complete binary tree) : 没错, PQ是一种满足某些条件的完全二叉树. 所谓的\"完全二叉树\", 要满足: 除了最后一层, 所有层都排满(没有非空节点) 最后一层的所有非空节点都排在左边 一张图可以直观说明, 完全二叉树其实就是长得像这样: 一个完全二叉树能被成为PQ的话, 要满足的条件就是: 对于任何一个节点, 它的优先级都大于左右子节点的优先级. 比如下图(圆圈里数字代表优先级): 这样看上去的话, 确实像是\"一堆\"东西的形状(码的还挺整齐的), 而且我们知道在这一堆节点里, 优先级最高的就是最顶上的那个节点了. 我很喜欢这个比喻: 一个heap就像是一个 公司的hirachy结构: 子节点就是下属, 父节点就是上司, 每一个上司的能力都比他的下属强(优先级比子节点搞). 能力最强(优先级最高)的人在最上面. array representation of PQ 对于一个完全二叉树, 没有必要用常规的树结构(使用指针)来表示, 因为如果从上到下走过每层(每层内从左到右)给所有节点编号(根节点的编号为1)的话, 完全二叉树有以下性质: father(i) = i/2 其中father(i)表示编号为i的节点的父节点的下标 leftchild(i) = i*2, rightchild(i) = i*2+1 既然用数组表示的时候, 任何节点的父亲节点和左右子节点都可以轻松得到, 就没有必要使用指针了. 所以只需要一个数组即可表示PQ ! 比如一个int的PQ在内部只要表示为: int pq[] . 另外注意, 上面的公式成立的前提是数组下标从1开始, 实现的时候我们把数组的第0个元素空出来即可. 不难看出一个PQ有以下性质: 高度为lgN 第k层有 2&#94;k 个节点 (root是第0层) 最后一层的节点对应的下标>=N/2 PQ implementation 先写一下代码框架: public class PQ{// maxPQ of integers int pq[]; int MAX_CAPACITY=1000;// if we use ArrayList we do not need MAX_CAPACITY int size;// pq[size] is the index of last element (rightmost node in last level) public PQ(){ pq = new int[MAX_CAPACITY+1]; size = 0; } public boolean isEmpty(){ return size==0; } public int top(){ // get top element assert !isEmpty(); return pq[1]; } public void add(int n); // insert element to PQ --> stay tuned public int poll(); // get and remove top element --> stay tuned } 最关键的两个函数是 add 和 poll , 实现这两个操作的关键在于两个基本操作: siftup() 和 siftdown() . 在PQ的性质被扰乱的时候, 使用这两个操作可以使PQ恢复性质(所谓的\"调整\"). siftup 如果一个新元素到来, 首先将其放在最后(最底层最右边的节点), 如果新来节点的优先级比较高, 可以用siftup将其提升到一个合适的层次. 用公司结构来类比就是新入职的员工由于能力强被提高到了高层, 而提升的办法就是: 和上司交换岗位(也就是和父节点交换位置). 所以这个操作只要不断把该元素与父节点交换, 直到遇到一个优先级大于该节点的父节点或者已经到了最高处停止. 代码很短: private void siftup(){ int i = size;// i is the index of the newly added element for(;i>1 && pq[i/2]<pq[i]; i/=2) swap(pq, i/2, i); } siftdown siftdown的功能和siftup相反: 如果在最高处是一个优先级很低的元素, 需要将其\"下放\". 方法就是把它和子节点里面优先级较高的进行交换. 代码也不长, 不过第一次写可能不太简洁: private void siftdown(){ if(isEmpty()) return;// nothing to sift when empty int i=1;// siftdown root node while(i*2<=size){ // while the node is not in last level int max=pq[i], j=i;// j is the element to swap if (pq[i*2]>max) // left child {j=i*2; max=pq[i*2];} if (i*2+1<=size && pq[i*2+1]>max) // right node {j=i*2+1; max=pq[i*2+1];} if (j==i) return;// stop when node is bigger than both child swap(pq, i, j); i = j; } } implementation of add and poll 有了这两个sift的基本操作, add和poll的操作就简单了. add 加入新元素的时候, 只要先把它放在最后面, 然后调用siftup()函数调整一下pq即可: public void add(int n){ // insert element to PQ assert size+1<MAX_CAPACITY; pq[++size] = n; siftup(); } poll 堆顶元素被拿走以后, 我们可以先把最后一个元素放在顶部, 然后调用siftdown进行调整: public int poll(){ // get and remove top element assert !isEmpty(); int top = pq[1]; pq[1] = pq[size--];// put last element on the top siftdown(); return top; } PQ application 接下来讲一下PQ的应用. heapsort 由于每次出队的都是在剩下元素里面最大(小)的, 所以只要把数组的元素放到一个pq里, 然后依次poll出来, 得到的序列就是排序好了的. 不管是插入还是删除操作, 每次调整的复杂度为log(n) (堆的高度), 所以算法复杂度为 O(NlgN). 实际使用的时候效率比快速排序/合并排序略差, 以后专门写一篇关于排序算法的文章时再聊. heapify 首先前面的siftdown函数很明显可以修改一下加上一个参数变为 siftdown(int i) (i代表要向下调整的元素的下标). 在heapsort里第一步是要建立一个PQ. 最naive的建堆操作就是新建一个空的heap然后不断向里面加入数组 a 里的元素(空间复杂度N, 时间复杂度NlgN). 其实这个操作可以做的更好: 我们 先直接把数组a当作pq[]数组 , 现在显然它不满足PQ性质, 只需要 多次使用siftdown 进行调整即可. 假设一共有h(=lgN)层, 由于最后一层的节点不必调用siftdown, 我们只要从倒数第二层开始调用siftdown即可, 结合前面提到的pq的性质(N/2以后的节点都在最后一层), 写法很简单(简单起见认为a也是把第0个元素空出来好了): public static void heapify(int[] a){ int N = a.length; for(int i=N/2;i<=1;i--){ siftdown(a, i); } } 这个操作的时间复杂度是O(N)的 ! 为什么呢? → 第k层节点有2&#94;k个节点, 这一层的节点向下调整最多会进行h-k步, 所以计算量是一个求和表达式: Sigma( 2&#94;k * (h-k) ) for k=0,...,h-1 (具体见 http://algs4.cs.princeton.edu/24pq/ 里面Q20的答案) top K elements of a stream 问题描述: 一系列数字一个一个到达, 当所有数(假设有N个)都到达以后, 求最大的K个数字. 这是一个经典的PQ应用 ⇒ 使用一个最小堆(minPQ)即可达到这个效果: 新建一个minPQ(最小的数字在最上面, 每次当一个数字n到达时: if minPQ.size<K → minPQ.add(n) if minPQ.size==K → 比较n和minPQ.top(): if n>minPQ.top(): minPQ.poll(); minPQ.add(n) else pass... median of a stream 问题描述: 一系列数字一个一个到达, 求他们的中位数(median)? 这个的解法是使用 两个PQ , 一个最大堆 maxpq 一个最小堆 minpq , 然后 maxpq里存放较 小 的一半数字, minpq里存放较 大 的一半数字: maxpq.top()<=minpq.top() . 并且我们要求: minpq.size <= maxpq.size <= minpq.size+1 ⇒ 这样中位数或者是maxpq.top(), 或者是两个top的平均值了~ 初始时两个pq都是空的, 每次数字n到来时: 先把n放进maxpq里 如果 maxpq.size>minpq.size+1 的话: minpq.add(maxpq.poll()) more about heap k-way heap 这里介绍的PQ其实是binary heap, 即这棵树是一个完全 二 叉树, 但是我们完全可以做成完全 K 叉树, 就是所谓的K-way heap了. 这样的好处是heap的高度会变得更小(从log2(N)变为logK(N)), 不过由于每个分叉变成了K个, 所以siftdown每次循环的操作复杂度也会增加(从2变为K). more \"flexible\" heap 在某些应用中(比如Dijkstra算法), 我们希望在插入后修改某个元素的优先级, 这时候需要对这个标准的heap做一些修改, 需要用一个数组把元素的\"逻辑编号\"和在pq数组里的\"实际编号\"相互转换. 不过这个会比较subtle(其实我已经记不起来了...), 见: http://x-wei.github.io/algoII_week2_2.html (IndexedPQ部分)","tags":"tech","title":"Priority Queue/Heap (优先队列/堆)小结"},{"url":"http://x-wei.github.io/algoII_week4_2.html","text":"1. Introduction to substring search \"most ingenious algorithm we've seen so far\" pb. having two strings, pattern and text , len(pattern)=M << len(text)=N, try to find pattern in text. ex. indexOf method of String in java. 2. Brute-Force Substring Search function signature: public static int search(String pat, String txt); brute-force algo: look for pattern at every position of text . public static int search(String pat, String txt){ int N=txt.length(), M=pat.length(); for(int i=0; i<=N-M; i++){ int j; for(j=0; j<M && pat.charAt(j)==txt.charAt(i+j); j++); if(j==M) return i; } return N;// not found } worst case: when txt/pat are repetitive → MN compares. problem with brute-force: always backup when mismatch. brute-force alternative j := number of matched chars in pattern i := index of the end of matched char in text ⇒ do explicite backup when mismatch by i -= j public static int search(String pat, String txt){ int N=txt.length(), M=pat.length(), i=0, j=0; while(i<N && j<M){ if(pat.charAt(j)==txt.charAt(i)) {j=i++; j++;} else {i=i-j+1; j=0}// <==backup } return j==M ? i-M : N; } challenge: want linear-time guarantee, and want to avoid backup. 3. Knuth-Morris-Pratt \"one of the coolest/trickiest algorithm covered in this course\" intuition suppose pattern = \"BAAAAA\", if we matched 5 chars in pattern and get mismatch on 6th char ⇒ we know the previous 5 chars are \"BAAAA\" → no need to backup the i pointer. KMP algorithm: clever method that always avoid backup ! Deterministic finite state automaton (DFA) finite states ,including start and halt state, indexed by j in the subtring pb for each state: exactly one transition for each char in alphabet ex. states are 0~6, pat=\"ABABAC\", transitions are indexed by chars in alphabet = {A,B,C}, finish if we reach state-6. dfa[c][i] = the next state if we are currently in state-i and encoutered char-c. interpretation of DFA for KMP algo in the DFA after reading txt[i] , the index of state is the number of matched chars in pattern, or length of *longest prefix of pat that is a suffix of txt[0:i]. * need to precompute the dfa[][] array from pattern the pointer i never decrements (thus we can do it in a streaming manner) → if dfa[][] is precomputed , java code is very very simple: public static int search(String pat, String txt, int[][] dfa){ int N=txt.length(), M=pat.length(), i, j=0; for(i=0; i<N && j<M; i++) j = dfa[txt.charAt(i)][j]; return j==M ? i-M : N; } running time: linear. → key pb: how to build dfa efficiently ? DFA construction match transition (easy part) when at state j, for the char c0==pat.charAt(j+1) , just go on matching: dfa[c0][j] = j+1 ex. ( pat=\"ABABAC\" ) mismatch transition (hard part) (for j==0, things are simple: dfa[c][0]=0 for all c!=pat[0] ) at state j (ie. j chars in pattern are matched) , and for c!=pat.charAt(j+1) ⇒ we are in state j: we know the last j chars in input are pat[0...j-1] , and followed by char = c , so the last j+1 chars of input string is: pat[0...j-1]+c ⇒ to compute dfa[c][j]: we can simulate as if we backup , ie. i=i-j+1, j=0 . if we go back to set j=0, and set i = i-j+1, then i is pointing at pat[1] , the text become pat[1...j-1]+c . We then let this string go through our dfa, the state that it achieves is the value of dfa[c][j] . here is a concrete example: pattern = \"ABABAC\" , state j=5 , char c='B ' we know the last 6 chars of the input = pat[0...j-1]+c=\"ABABA\"+\"B\"=\"ABABAB\" if we backup, i will point to pat[1], the string is just pat[1...j-1]+c=\"BABAB\" we use the string \"BABAB\" as input and go through the partially constructed dfa, and see that we will reach state 4 so we know dfa['B'][5]=4 similarly we can get dfa['A'][5]=1 , as indicated below: ( pat=\"ABABAC\" ) one concern: seems this simulation needs j steps ? ⇒ can be changed to be constant time if we maintain a state X := the state of simulating of input=pat[1...j-1] we maintain this state X , then for each mismatched char c, we just need to look at dfa[c][X] . ( pat=\"ABABAC\" ) [Algo] set all matched transitions dfa[c0][j] = j+1 for all c0==pat[j] fill first column (j==0): dfa[c][0]=0 for all c!=pat[0] initialize X=0 (state for empty input string) for j=1 to M: for all c!=pat[0] : set dfa[c][j] = dfa[c][X] (DP here...) update X=dfa[c0][X] ⇒ 注意, 此时X并 不等于 X+1(最开始 dfa[c0][j]=j+1 不适用于此), 为什么? 因为 c0==pat[j] 而不 是pat[X] !! 比如说最开始, j=1的时候X是等于0的!!! (这个弯我饶了好几分钟...) java code (can be written to be more compate): public int[][] constructDFA(String pat){ int R=256;//ASCII code int M=pat.length(); int[][] dfa = new int[R][M]; // 1. fill matched transitions: dfa[pat.charAt(j)][j] = j+1 for(int j=0;j<M;j++) dfa[pat.charAt(j)][j] = j+1; // 2. fill 1st column --> can be ignored as java int default val=0 // 3. fill mismatched transitions int X = 0; for(int j=1;j<M;j++){ for(int c=0;c<R;c++) if(c!=pat.charAt(j)) dfa[c][j] = dfa[c][X]; X = dfa[pat.charAt(j)][X]; } return dfa; } running time and space: O(M*R) . prop. KMP algorithm runs in O(M+N) time, and constructs the dfa in O(M*R) time/space. 这个KMP算法, 我曾经想过好几个小时, 然后最后写出了特别复杂的代码, 虽然可以用但是基本写了就忘掉了. 但是经过老爷子这么一讲, 感觉这次印象深刻了好多. 老爷子NB... 八卦时间: 4. Boyer-Moore Heuristic in practice. i does not necessarily go through all txt chars ⇒ i may skip some chars. intuition for matching: scan chars from right to left (j will decrease when checking) when encoutered a mismatch: we can skip <= M chars (if the char is not in pattern) ex. (pat=\"NEEDLE\") → pb: how to skip? mismatch character heuristic note : the i always points to the beginning of the substring ( txt[i,...,i+M-1] ) to be checked for match. case 1. mismatched char not in pattern easy case → just move i to the right of this char. case 2. mismatched char in pattern heuristic: line up i with the rightmost char in pattern . i += skip where skip length = j - index of rightmost char in pattern note: this does not always help, in the example below, i even backups : to avoid backup, in this case we just increment i by 1 (heuristic doesn't help in this case). implementation use an array right[] as skip table , right[c] is the index of rightmost occurrence of char c (-1 if c not in pat). int[] right = new int[M]; for(int i=0;i<R;i++) right[i] = -1;//value for chars not in pattern for(int j=0;j<M;j++){ right[pat.charAt(j)]=j; } using this table we can implemente the heuristic algorithm: public static int search(String pat, String txt, int[] right){ int N=txt.length(), M=pat.length(); int skip; for(int i=0;i<N-M;i+=skip){ skip = 0; for(int j=M-1;j>=0;j++) if(pat.charAt(j)!=txt.charAt(i+j)){// when mismatch happens skip = Math.max(1,j - right[txt.charAt(i+j)]);// skip if we can, else just increment i by 1 break; } if(skip==0)// if the above for loop finishes without changing skip --> we are done. return i; } return N;// pattern not found } analysis property . the Boyer-Moore heuristic (in practice) takes about N/M (sublinear!) compares to search. 好神奇, 比KMP还要简单的算法, 实际效率这么高... worst-case performance: N*M ... 这一点不如KMP. → can be improved... 5. Rabin-Karp 两个图灵奖的大神发明的算法.. intuition basic idea: modular hashing ex. for strings of numbers compute hash fcn (for number strings is easy: take the string and treat it as a number, then %Q where Q is a big prime number). for a pointer i →corresponds to the substring txt[i, ..., i+M-1] → check hash for match (below: text=3141592653589793, pattern=26535) computing the hash function efficiently let ti be the ith char in txt, the hashcode for substring txt[i,...,i+M-1] is: ⇒ just an M-digit base-R integer modulo Q ! poly(M, R) % Q *. * Honor's method linear time algorithm for evaluating polynomial. recursive equation: poly(i, R) = poly(i-1, R)*R+ti ex. (R=10, M=5) private long hash(String key, int M){ long h=0; for(int i=0;i<M;i++) h = ( h*R + key.charAt(i) ) % Q return h; } if we know x_i, the x_i+1 can be infered: ⇒ x_i+1 can be computed in constant time: ⇒ we precompute R&#94;(M-1) and maintain the hash number, and check for match ! public static int search(String txt, String pat){ int N=txt.length(), M=pat.length(); long pathash = hash(pat, M); int RM = R&#94;(M-1);// <-- pseudo code, store value of R&#94;(M-1) long txthash = hash(txt, M);// txthash will be maintained for(int i=0;i<N-M;i++){ if(txthash==pathash && checkMatch(i,txt,pat)) return i; txthash = ( (txthash - txt.charAt(i)*RM)*R + txt.charAt(i+M) ) % Q; } } 更新txthash的地方可能会有modulo造成的问题... 不过先这样写吧.. for collisions: Monte Carlo vs. Las Vegas analysis Theory : if Q is sufficiently large (~M*N&#94;2), the probability of collision is ~1/N. Practice : choose Q to be sufficiently large, and collision probability is ~1/Q. Summery","tags":"notes","title":"[Algorithms II] Week 4-2 Substring Search"},{"url":"http://x-wei.github.io/quick-sort-and-more.html","text":"今天总结一下非常有用的快速排序(qsort)算法, 以及由此衍生的一些其他相关算法(Knuth shuffle, quick select, 3-way partition). 快速排序的算法可以用三句话描述: [Algo] 选择基准项(pivot element, 一般取第一个元素为pivot) 把数组里所有小于pivot的移动到pivot左边, 大于pivot的移动到右边 ⇒ 此时pivot已经位于最终排序时的正确位置 对pivot左右两个数组分别递归进行快速排序 由以上描述可见, qsort是一个递归算法, 我们可以把它的函数声明写成: void qsort(int[] a, int lo, int hi) , 表示排序a[lo, hi]之间(闭区间)的所有元素. quick partition 由上面描述可以见, qsort最关键的是第二步: 把数组元素以pivot分为两部分. 这个操作就是quick partition. 函数声明为: int partition(int[] a, int lo, int hi) , 该函数返回pivot(即subarray的第一个元素 a[lo] )所在的位置. 如果允许新建一个临时数组的话, 那么这个就不是什么问题, 但是为了节约空间占用, 现在需要直接修改(in-place)使得a[lo] 到, 而且希望可以用尽量少的交换( swap(int[]a, int i, int j) )操作, 就不是很evident了. 这个函数的写法是用两个指针i和j分别从两端向中间走, 如果两个指针指向的元素一个小于pivot一个大于pivot那么就进行交换, 当两个指针碰面的时候结束(最后把pivot和指针元素交换). 请看下面这个萌萌的图(图片来自<<啊哈!算法>>): 选取第一个元素(6)为pivot, 然后j向左走直到遇到一个小于pivot(6)的数停止, i向右走直到遇到一个大于pivot的数停止( 注意要让j先移动 ), 此时二者交换: 只要重复这个过程, 直到i>=j为止, 此时只要最后把pivot和j( 注意是j而不是i )指向的元素交换即可: 所以pivot的位置就是j, 函数返回j即可. java实现: int partition(int[] a, int lo, int hi){ int pivot = a[lo], i=lo, j=hi; while(true){ for(;j>=lo && a[j]>=pivot;j--);// move j to a point where a[j]<pivot for(;i<=hi && a[i]<=pivot;i++);// move i to a point where a[i]>pivot if(i>=j) break;// break if i and j meets swap(a, i++, j--); } swap(a, lo, j);// swap pivot with a[j] return j; } 这里有几点要注意的: 让j先移动 最后pivot要和j交换而不是和i交换: 因为最后放在最左边的应该是一个小于pivot的数嘛 移动的时候别忘了需要加数组下标的边界检查( i<=hi , j>=lo ) partition()的复杂度是线性的 O(n) Knuth shuffle qsort之所以快, 是因为每次都能够按照pivot分为大致同样长度的两个子数组(所以每次子问题的规模除以二), 所以复杂度为 O(NlogN) . 最坏情况下, 如果每次两个子数组中可能有一个长度为0, 那么每次子问题的规模只减少了1, 所以复杂度变成了quadratic O(N2) . 为了防止这种最坏情况的出现, 可以在一切开始之前把数组打乱顺序, 所以这一节讨论快速shuffle的算法. 最经典的就是Knuth的shuffle算法了, 算法很简答, 描述为: for(k=1 to n): 每次把第k个元素和前k个元素中的随机一个元素交换. 代码只有两行: void shuffle(int[] a){ for(int K=0; K<a.length; K++) swap(K, Random.nextInt(K+1)); } 关于算法的正确性, 其实只要证明\"元素i在shuffle后最终位于位置i\"的概率为1/N即可, 不难证明. quick sort 好了 有了以上两个辅助函数就可以写qsort函数了: void qsort(int[] a, int lo, int hi){//recursive helper function if(lo>=hi) return; int p = partition(a, lo, hi); qsort(a, lo, p-1); qsort(a, hi, p+1); } void qsort(int[] a){ shuffle(a); qsort(a, 0, a.length-1); } 其实qsort的主体就是那个partition函数, 单独把partition列出来是因为它不止可以用在排序, 还可以用来做quick select, 见quick select节. quick select 对于一个没有排序的数组, 如何快速找到它的中值(median)? 以上这个问题的答案就在 partition() 函数. 之前说过, partition() 函数的返回值表示pivot在排序好的数组中的位置(rank), 这个消息非常有用: 中值只不过是rank等于长度除以2的元素而已. 为了寻找rank等于k的元素, 我们用partition函数可以每次把问题规模缩小: 如果partition()=p k那么右边subarray不用考虑, 如果数组事先shuffle过了的话, 问题规模每次缩小一半. 定义一个函数, 寻找rank等于k的元素, 代码类似于二分查找: int findKth(int[] a, int k){ shuffle(a); int lo=0, hi=a.length-1; while(lo<hi){ int p = partition(a, lo, hi); if(p==k) return a[k]; else if(p<k) lo=p+1; else hi=p-1; } return a[k]; } 该算法内层循环为O(hi-lo), 每次问题规模减少一半, 所以复杂度为N+N/2+N/4+...+1 = 2N, 复杂度为线性时间! 3-way qsort qsort之前有个bug: 在数组里很多重复元素的时候, 效率会下降为O(N2). 原因是qsort没有好好处理重复元素的问题. 于是Dijkstra提出了一个 3-way partition 的算法: 把数组分为三部分: 左边 [lo, lt) 严格小于pivot, 中间 [lt, gt] 等于pivot, 右边 (gt, hi] 严格大于pivot. 算法初始化 lt=lo, gt=hi, i=lo , 用指针i向右扫描, [i,gt]为未处理到的部分. 算法很subtle, invariant是这样的: a[lo,lt-1] < pivot a[lt, i-1] = pivot a[i,gt] = unseen a[gt+1, hi] > pivot 这个图很有助于写代码: void qsort3way(int[] a, int lo, int hi){ if(hi<=lo) return; int lt=lo, i=lo, gt=hi, pivot=a[lo]; while (i<=gt){// [i,gt] is unseen elements if(a[i]==pivot) //a[lt,i-1] are elements == pivot i++; else if(a[i]>pivot) // a[gt+1, hi] are elements > pivot swap(a, gt--, i); else // a[lo, lt-1] are elements < pivot swap(a, lt++, i++); } qsort3way(a, lo, lt-1); qsort3way(a, gt+1, hi); } 说它很subtle, 除了因为没有那个图我写不出来以外, 还有就是, 在把i和lt交换时, i可以increment (因为我们知道 a[lt]==pivot ), 但是i和gt交换时, i 不能 increment: 因为a[gt]不知道多大, 所以i位置要继续检查. 另外说一句, quicksort的思想在radix sort和trie里面也有闪现, 见我关于这两个主题的MOOC笔记...","tags":"tech","title":"quick sort and more: 快速排序算法总结"},{"url":"http://x-wei.github.io/algoII_week4_1.html","text":"More efficient version of symbol-table where the keys are strings. 1. R-way Tries Two implementations of symbol tables that we've seen: when keys are strings: ( L =string length, N =number of strings, R =radix) for string keys ⇒ do better by avoiding examing the entire key. goal: faster than hashtable, more flexible than BST ! String Symbol Table API public class StringST<V>{// implements ST<String, V> public void put(String key, V val); public V get(String key); public void delete(String key); } R-way tries A trie is a tree where: except the root, each node store characters (instead of string keys) — in fact the chars are stored in links each node has R children store value in node if the node corresponds to the last char in key. example: ( a trie ) search in a trie get() hit if when search ends the node has a non-null value miss if reach a null link or have null value when search ends. insertion in a trie put() follow the links as the chars indicate in the key → if meet null links: create new node → when reach last char in key: set the value of the node Java implementation trie node class: in each node use an array of size R to store links... private static class Node{ private Object val;// because cannot create arries of generic type (array of Node) private Node[] next = new Node[R];// chars are implicitly defined by link index } 另外关于为什么内部类Node声明为static, 参考这里: http://www.geeksforgeeks.org/static-class-in-java/ implementation: use private recursive methods, 和BST的实现类似, 定义一个递归的private函数, 返回插入后的Node, 很有用. public class TrieST<Value> { pirvate final int R = 256; // ASCII chars private Node root = new Node(); private static class Node{ private Object val;// because cannot create arries of generic type (array of Node) private Node[] next = new Node[R];// chars are implicitly defined by link index } public void put(String key, Value val){ this.put(root, key, val, 0);// use private recursive helper method } private Node put(Node x, String key, Value val, int d){ // `d` is the index of char to put // returns the inserted node if (x==null) x = new Node(); if (key.length()==d) x.val = val; else{ char c = key.charAt(d); x.next[c] = put(x.next[c], key, val, d+1); } return x; } public boolean contains(String key){return get(key)!=null;} public Value get(String key){ Node nd = this.get(root, key, 0); if(nd==null) return null; return (Value) x.val; // cast back } private Node get(Node x, String key, int d){ // returns the node that contains val for key if(x==null) return null; if(d==key.length()) return x; char c = key.charAt(d); else return get(x.next[c], key, d+1); } } delete node in trie: find the node and set the val to null if a node has 0 links(leaf) and val==null: delete it and recursivly go up. analysis time: search hit — L nodes examined. serach miss — sublinear in L space: each node has R links (possibly null) — too much memory for large R ! Application interview question: data structure for spell checking. 2. Ternary Search Tries Solution to the memory issue of R-way tries — much fewer null links. ternery=tuple of 3... TST: store chars (and vals) explicitly in nodes each node has 3 children: smaller, larger: TST that starts with char smaller/bigger than its char c. equal: the TST with keys that starts with this char c (所以除了用smaller/larger连接的节点在Rway trie里属于同一层 类似于3-way radix sort(同一篇paper里提出的), 有有点RBTree的意思... searching in TST: quite similar to BST search, will go down (\"equal\" or \"middle\") when the current node matchs current char... Implementation Node class: private class Node{ private Value val; private char c;// store char explicitly private Node left, mid, right; } TST class (again use recursive helper functions) 有一种写BST代码的感觉, 另外由于这次char是显式存在node里, 代码反而更好理解了... public class TST<Value>{ private class Node{...}; private Node root; public void put(String key, Value val){ this.put(root, key, val, 0); } private Node put(Node x, String key, Value val, int d){ char c = key.charAt(d);// char to process if(x==null) x = new Node(c); if(c==x.c) { if(d==key.length()-1) x.val = val; else x.mid = put(x.mid, key, val, d+1);// d+1 means we go down one level } else if (c<x.c) x.left = put(x.left, key, val, d);// do not increment if the current char is not matchd else x.right = put(x.right, key, val, d); return x; } public Value get(String key){ Node nd = this.get(root, key, 0); if(nd==null) return null; return nd.val; } private Node get(Node x, String key, int d){ if(x==null) return null; char c = key.charAt(d); if(x.c==c) { if(d==key.length()-1) return x; else return get(x.mid, key, d+1); } else if(c<x.c) return get(x.left, key, d); else return get(x.right, d); } } Hybrid of TST and Rway trie at root: do R&#94;2 branching other nodes are TSTs analysis space cost: linear in N. time: if keys arrive in rand order... (can use rotation to get worst-case guarantee..) Hashing vs. TST Hashing: need to check entire key no matter hit or miss performance relies on hash functions donot support ordered operations TST: examines just enough chars support ordered operations 3. Character-Based Operations some very useful char-based operations: prefix match wildcard match : use a dot to represent any char longest prefix : find the key that is the longest prefix of a string API public class TrieST<Value>{ // functions decalred before.. Iterable<String> keys; Iterable<String> keysWithPrefix(String s); Iterable<String> keysThatMatch(String s); Iterable<String> longestPrefixOf(String s); } keys(): ordered iteration keys() : just an inorder-traversal of the Rway trie →dfs, + maintain the chars in the path (root to current node). public Iterable<String> keys(){ Queue<String> q = new Queue<String>(); collect(root, \"\", q);// helper fcn return q; } private void collect(Node x, String pathstr, Queue q){ if(x==null) return; if(x.val!=null)// if this is a key q.enqueue(pathstr);// pathstr is the string of chars from root to x for(char c=0;c<R;c++) // dfs collect(x.next[c], pathstr+c, q); } prefix match very useful: ex. autocomplete, search bar, ... implementation⇒ just find the end of that prefix, then call keys() on the subtrie . 之前写的俩helper function这里发挥作用了: public Iterable<String> keysWithPrefix(String prefix){ Node subtrieRoot = get(root, prefix, 0); Queue<String> q = new Queue<String>(); collect(subtrieRoot, prefix, q); return q; } longest prefix ⇒ just do a search and keep track of the longest key that we encounterd. public String longestPrefixOf(String s){ int len = search(root, s, 0, 0); return s.substring(0, len); } private int search(Node x, String s, int d, int len){ if(x==null) return len; if(x.val!=null) len=d;// this is the current longest prefix if(d==s.length()) return length; char c = s.charAt(d); return search(x.next[c], s, d+1, len); } application: T9 texting classique... G面试以及X的TP里都有涉及过... first: generate all string combinations second: call get() on the trie of dictionaries. Other Variants Patricia trie improvement: remove the one-eay branching (put >1 chars in a node). suffix tree patricia tree of the suffix linear time construction ongest repeated substring, longest common substring, longest palindromic substring, substring search, tandem repeats... 好有用!!... Summery","tags":"notes","title":"[Algorithms II] Week 4-1 Tries"},{"url":"http://x-wei.github.io/algoII_week3_2.html","text":"This week: string sort. 1. Strings in Java char data type char in C 8-bit integer, 256 characters, 7-bit ASCII code char in Java 16-bit Unicode String data type String : immutable sequence of characters operations: lengthe, ith char, substring, concatenate implementation: using a char[] , maintain a length and an offset. ⇒ substring methode is O(1) time. StringBuilder data type StringBuilder : mutable data type. implementation: using a resizing char[] array (ArrayList). ⇒ contat in (amortized) constant time, substring in linear time ! ex. reverse a string: linear using StringBuilder, quad using String. form an array of suffixes: quad (time&space) using StringBuilder, linear (time&space) using String. Longest Common Prefix: runs in linear/subinear time ⇒ compareTo() for strings takes (sub)linear time! Alphabet alphabet different for different type of string (ex. binary numbers, DNA, ...) def. Radix R is number of digits in alphabet. 2. Key-Indexed Counting review of compare-based sorting algorithms: lower-bound for compare-based algorithms: ~ NlgN (=Lg(N!)) ⇒ goal: do better by avoiding using compares. Key-indexed counting is an algorithm to sort by character(ex. sort array of string by their 1st character). Assumption: keys are integers between 0 and R-1 (<<N, small integer). ⇒ use keys as array index, to sort an array of N integers between 0 and R-1 . [algo] count freq of each key index (of size R+1 ) using count array, compute a cumulated freq (cumsum of count[] ) the cumsum[] array stores the index range of each key index: index range of key-i in sorted array is [cumsum[i], cumsum[i+1]] then get the sorted array by going through the array and using cumsum[] array ⇒ public void keyIndexCounting(int[] a, int R){// entries in a[] are in range [0,R-1] int N = a.length; int[] count = new int[R+1]; // count[i] = freq of key i-1, count[0] = 0 for(int ai:a) count[ai+1]++; int[] cumsum = count; //cumsum = cumulated freq for(int i=0;i<R;i++) cumsum[i+1] += cumsum[i]; // the ranges of key i in sorted array should be [cumsum[i], cumsum[i+1]] int[] aux = new int[N]; for(int ai:a) aux[cumsum[ai]++] = ai; a = aux; } analysis running time: linear in time and in space. stable sorting: that's why we need the cumsum and aux array... 3. LSD Radix Sort LSD = least significant digit (for string sorting) (assume strings all of same length) idea: consider chars from right to left sort using dth character as key (using key-indexed counting) important: the key-indexed counting should be stable. analysis time: W * N (W=length of string) correctness: prop . LSD sorting works pf. by induction on i prove that: after pass i, strings are sorted by last i characters. implementation public static LSDsort(String[] a, int W){//W=fixed width int R = 256;//for ASCII chars int N = a.length; String[] aux = new String[N]; for(int d=W-1;d>=0;d--){//W passes //key-indexed counting at digit d int count = new int[R+1]; for(String s:a) count[s.charAt[d]+1]++; for(int i=0;i<N;i++) count[i+1] += count[i];//count = cumsum(count), range of each key for(String s:a) aux[s.charAt[d]++]=s;//put each key in right place for(int i=0;i<n;i++) a[i]=aux[i]; } } if keys are binary numbers → break into bit characters then apply LSD. if not fixed length → some fix.. sort 1 million 32-bit integers? (Google/Obama interview) ⇒ LSD string sort~ 4. MSD Radix Sort most-significant-digit first (from left to right) Idea. partition array into R pieces according to first character (the count[] array is the partition) recursively sort each subarrays variable length: end-of-string are treated as before any char implementation private static void sort(String[] a, String[] aux, int lo, int hi, int d){ if(hi<=lo) return; //sort by dth character int count = new int[R+2];// R+2 as we are taking account into the end-of-strings!! for(int i=lo;i<=hi;i++) count[ a[i].charAt(d)+2 ]++;//charAt(end-of-string)=-1 for(int i=0;i<R;i++) count[i+1] += count[i]; //cumsum for(int i=lo;i<=hi;i++) aux[count[a[i].charAt(d)+1]++] = a[i]; for(int i=lo;i<=hi;i++) a[i] = aux[i-lo];// attention: aux is filled from index 0 instead of lo //recursively sort each subarray (R subarrays in total, ranges stored in count[]) for(int r=0;r<R;r++) sort(a, aux,lo+count[r], lo+count[r+1]-1, d+1); } analysis can recycle aux[] , but not count[] . too slow for small subarrays (if len(subarray)<<R) huge nb of subarrays because of recursion improvement ⇒ cutoff to insertion sort... characteristics of MSD sort: examines just enough chars to sort. → can be sublinear in N. MSD vs. quicksort disadvantages for MSD: random access of memory (cache inefficient) too many instructions in inner loop extra space for count[] extra space for aux[] disadvantages for qsort: NlgN nb of string compares has to rescan many chars for keys with long prefix matches 5. 3-way Radix Quicksort ⇒ combine benefits of qsort and MSD. idea: do 3-way partition by the dth character . less overhead than R-way partitioning for MSD do not re-examine chars equal to the partitioning char Implementation modification of the 3-way qsort. private static sort ( String [] a , int lo , int hi , int d ) { if ( hi <= lo ) return ; int lt = lo , gt = hi , i = lo + 1 ; // use 3 pointers : lt , i , gt char pivot = a [ lo ] . charAt ( d ); while ( i <= gt ) { // invariant : a [ lo , lt ) < pivot , a ( gt , hi ] > pivot , a [ lt , i ] = pivot char c = a [ i ] . charAt [ d ] ; if ( c < pivot ) exch ( a , lt ++, i ++ ); else if ( c > pivot ) exch ( a , i , gt -- ); else : i ++ ; } // 3-way partition sort ( a , lo , lt-1 , d ); if ( pivot >= 0 ) sort ( a , lt , gt , d + 1 );// pivot < 0 means end-of-string sort ( a , gt + 1 , hi , d ); } analysis wrt. qsort: from NlgN string compares to NlgN char compares. avoids re-comparing long common prefixes. wrt. MSD: short inner loop cache friendly in-place 6. Suffix Arrays ( some applications of suffix array) keyword-in-context search given N chars (N huge), preprocess it to enable fast substring search. ⇒ suffix sort generate suffix array (linear time & space ) sort on the suffix ⇒ brings repeated suffixes together longest repeated substring brute force algorithm try all i, j as starts of indices, then compute longest common prefix (LCP) → O(D*N2) where D is length of longest repeated substring. ⇒ use suffix array sorting suffix array will bring repeated substrings together java code: //int lcp(String s1, String s2) defined public static lrs(String s){ int N = s.length; String[] suffix = new String[N]; for(int i=0;i<N;i++) suffix[i] = s.substring(i);//construct suffix array Arrays.sort(suffix);// sort suffix array ==> using 3-way radix sort // one pass to get longest repeated substring (bigest lcp) String lrs = \"\"; for(int i=0;i<N-1;i++){ int lcp = lcp(suffix[i], suffix[i+1]); if(lcp>lrs.length) lrs = suffix[i].substring(0, lcp); } return lrs; } lrs worst-case input : lrs very long (say N/2). ⇒ quadratic for lrs and for sorting. → improvement of lrs for worst-case performance: Manber-Myers algo sketch: phase 0: sort suffix[] on 1st char phase i: given suffix[] is sorted based on first 2&#94;(i-1) chars → create suffix[] sorted on first 2&#94;i chars. each phase: double the nb of chars sorted on. maintain an inverse[] array, to make comparisons constant time. performance: NlgN String sorting summery can have linear sort: use chars as array index sublinear sort: not all date need to be examined 3-way radix qsort is asymptotically optimal","tags":"notes","title":"[Algorithms II] Week 3-2 Radix Sorts"},{"url":"http://x-wei.github.io/TeXmacs_intro.html","text":"今天介绍一个论文写作的神器: TeXmacs ! 0. Why TeXmacs? 一说到\"论文写作神器\"一般大家首先想到的就是LaTeX, 确实LaTeX写出来的数学公式和文章的排版非常漂亮. 但是作为一个几年来用过LaTeX写过几次报告的小白用户, 说句实话我从来都没有喜欢上过LaTeX. 根本的原因大概是: LaTeX的语法是一种标记语言(markup language), 本质上是给机器看而不是给人看的—就像html源代码是为了给浏览器看而不是直接给人看的. 0.0 LaTeX强迫症自测 矩阵A的转置, 你用tex会怎么写? ⇒ 如果你不能容忍直接写成 $A&#94;T$ , 而一定要写成类似 $\\textbf{A}&#94;\\intercal$ 的话... 请直接忽略本文 & 继续用LaTeX, 好走不送...... (另: 强迫症可以去这里看到底怎么打转置: http://tex.stackexchange.com/questions/30619/what-is-the-best-symbol-for-vector-matrix-transpose ) 如果你认为这样的细节不重要, 好好描述数学问题本身才最重要的话, 请继续阅读. 0.1 TeXmacs是什么 简言之, TeXmacs是一个所见即所得的编辑器. 你也许想到了LyX, TeXworks之类的工具, 但是TeXmacs和它们不一样—它并不依赖LaTeX. 他吸收了LaTeX的优点, 即文章的排版非常美观. 同时它又不同于LaTeX, 比如, 在TeXmacs里是 没有 \"源文件\"这个概念的. 另外它还有一些非常实用的特性(尤其是对于数学公式的支持), 可以极大的提高编辑效率. 作者: Joris van der Hoeven ( http://www.TeXmacs.org/joris/main/joris.html , prof@X ) 官网: http://TeXmacs.org/tmweb/home/welcome.en.html 在官网上有两个短视频, 介绍了TeXmacs的简单用法: http://TeXmacs.org/tmweb/home/videos.en.html 我自己感觉写同样的内容, 比如写同样一页纸的数学推导, 用TeXmacs大概比LaTeX快10倍... 另外如果你发现这一页纸的推导中间有一步有错误要修改的话...那么TeXmacs大概要快100倍吧... 最早听到TeXmacs的名字是看到过王垠的一篇 博客 , 后来我惊奇地发现TeXmacs的作者居然是X的一位老师!! 所以更要高举安利TeXmacs的大旗了. btw, 在X三年时间, 除了我自己和被我安利的Manu同学, 并没有见有别人用TeXmacs...唉. 0.2 LaTeX吐槽 正式安利TeXmacs之前, 先这里举一些我不认为LaTeX可以让人高效编辑文章的原因, 如果你认同大部分下面的吐槽, 欢迎继续阅读本文. 重申一遍, markup的语法不是让人看的 : 要是忘记一个大括号或者少了个反斜杠就编译出错找半天 请问每个tex文件开头那十来行usepackage什么的谁能记住?... 我每次要么上网搜要么从以前的tex文件里抄过来... 公式是最痛苦的: 好几行的数学推导, 然后每行的内容又差不多的话, 编辑起来眼快瞎了 想要插入个截图还得先保存为图片文件(&还得费时间起个文件名): 就不能像word那样直接粘贴进去么? 另外图片的大小和位置也要试好几次才知道怎么合适 一编译生成一些乱七八糟的东西: xx.aux什么的, 写篇小文章也得专门建立个文件夹放这些乱七八糟的东西 LaTeX号称可以让人专心文章内容不关心排版, 可实际情况却是我每写一小段就会重新编译生成pdf看看, 因为我生怕少写一个花括号导致又编译不成了 有时为了用一个包只好安装整个texlive-full, 电脑空间瞬间少了1G... 还有什么欢迎补充... 0.3 TeXmacs优点一览 TeXmacs的优点大概是以下几点: 数学编辑功能非常强大, 用了再也不想用LaTeX源码写数学公式 所见即所得, 不必记忆LaTeX各种tag, 有了它完全可以不用学LaTeX 插入图片可以直接嵌入tm文件(其实是把图片序列化为一个很长的字符串), 不再需要为了插图专门建立文件夹 TeXmacs体积很小: 我用的版本解压后只有66MB 可以从tex文件导入或导出到tex文件 0.4 TeXmacs适合什么样的任务? 目前为止我个人使用TeXmacs写过两篇比较正式的报告(其中一篇见 这里 ), 另外用它写过一些课程的简单报告和作业, 还用它进行过一些数学的推导或者记笔记. 个人感觉以下任务都可以用TeXmacs高效完成: 各种正式的学术报告 常见的论文(TeXmacs里面内建了很多conf的文章style可以选择) 课堂记数学笔记(前提是打字速度快, 最好能盲打) 简单公式推导, 新建个tm文件就可以开始了, 不用兴师动众新建个文件夹... 好像也可以用来做ppt (beamer), 个人没有尝试过 0.5 TeXmacs不适合什么样的任务? 我觉得TeXmacs不适合的就是LaTeX强迫症用户的任务... 1. 基本操作 1.1 界面 打开TeXmacs可以看到如下的用户界面: 正文上方有四行菜单栏: 第一行是常见的菜单, 提供了所有功能 第二行是一些常用操作: 打开/保存/复制粘贴/前进后退等 第三行是插入一些常见\" 环境 \"的快捷按钮, 后面会看到. 第四行非常有用, 它叫做\" 上下文菜单 \", 它的内容会随着光标所在\" 环境 \"而变化. 由于现在光标处在最开始位置, 此时上下文菜单是各种文章显示的选项, 可以选择一个style模板, 调整字体, 页面设置以及文章语言等. 在正文下面, 窗口最底下还有一行: 右下角指示当前光标所在处的\" 环境 \", 可以理解为当前光标处在什么tex的tag里面, 例如公式环境/表格环境/斜体字环境等, 在编辑的时候可以注意一下右下角的显示. 另外一般处在某种环境下面的话, 该环境会由矩形边框标出. 1.2 插入标题 点击 上下文菜单 右边的\"Title\"按钮可以加入文章标题, 敲回车可以输入作者名: 注意此时光标处于\"标题\"环境下, 所以 上下文菜单 是关于标题的功能按钮, 点击\"Title\"右边的加号可以加入其他信息: texmac也支持简单的tex标记, 比如要加入日期的话 除了用上面的菜单, 也可以输入 \\date 然后敲两次回车即可: 1.3 插入概述 从\"标题\"环境出来 又回到了整个文章的环境, 上下文菜单 也改变了 此时 上下文菜单 右边是一个abstract按钮, 因为TeXmacs猜测我们在输入标题以后会输入概述, 点击这个按钮即可进入概述环境(见右下角). 1.4 插入章节 从abstract环境出来, 插入一个章节(tex里的 \\section 标记)有三种办法: 办法一: 使用第一行的菜单 办法二: 使用第二行的\"插入章节\"图标 办法三: 直接用tex标记, 输入 \\section 然后回车即可 1.5 选择样式 在最外层环境下的上下文菜单里可以选择文章样式. 1.6 插入列表 在第二行有个按钮可以直接插入列表, 或者也可以输入 \\itemize 然后回车. 光标在列表环境时可以看到上下文菜单的内容也变化了: 这里再介绍一个非常实用的快捷键: ctrl-tab , 作用是改变当前环境的显示样式, 比如在列表环境里按下 ctrl-tab 以后可以变成其他样式(圆点变为了横线). 这个快捷键在公式或者表格环境里也很常用. 按下多次 ctrl-tab 可以在各种样式中循环. 1.7 常见文字格式: 粗体/斜体/等宽/下划线/超链接 粗体和斜体可以像word一样使用 ctrl-b/ctrl-i 做到 等宽可以用第三行的工具栏按钮, 或者输入 \\texttt 加回车. 下划线我没有找到按钮, 不过直接用 \\underline 加回车即可搞定. 超链接使用第三行按钮即可 在hyperlink环境下(注意上下文菜单又变化了)输入两个参数: 链接文本和链接地址, 然后回车即可 1.8 定理/算法等 插入一个定理或者算法, 可以直接用第三行的按钮, 看图: 1.9 打开多个文件/退出TeXmacs 如果打开了多个tm文件, 使用主菜单的Go按钮在文件之间切换: 当前改动没有保存的时候, 标题栏的文件名后面会有一个星号指示. 此时直接点击关闭的话, 会在最下方有个提示框: 2. 数学公式 这是TeXmacs最激动人心的功能, 用它编辑数学公式非常畅快~ 2.1 公式环境 首先为了进入数学公式环境, 可以直接输入 $ 或者 alt-$ , alt-& (对应行内公式/单行公式/多行公式). 这里说一下, 个人感觉英文界面的一些词汇比如formula/equation, table/tabular, 傻傻分不清楚, formula和equation的区别在于, equation是多行公式... 把tm换成中文界面的话这些名字好理解多了: 另外, 切换行内/单行公式也可以用 ctrl-tab 快捷键. 在进入公式环境以后, 注意不止第四行的上下文菜单变成了公式的按钮, 第三行的插入菜单也变为了插入符号的按钮: 2.2 输入数学符号 这是我觉得TeXmacs最爽的地方: 公式环境下(注意是 在公式环境下 ), 形状相似的符号通过tab轮转 . 例子: 字母α和字母a形状很像, 输入字母 a 以后直接tab即可变为α. ⊗怎么输入呢? 它等于 @-x , 同理⊕等于 @-+ , ∞等于 @@ ∈像什么? 只要输入 < 然后多tab几次就看到了 不等号≠怎么输入? 直接连续打 = / 即可 遇到别的不会打的字符, 可以用第三行的按钮, 鼠标悬停可以显示相应快捷键: 2.3 其他数学符号: 上下标/根号/分数/积分/求和/... 输入上标直接打一个 &#94; 即可, 下标是 _ , 和LaTeX里面一样 — 只不过不用看让人眼晕的源码. 符号上标下标(不是右上角而是正上正下): 用 alt-a/alt-b (above/below) 输入根号快捷键是 alt-s 输入分数快捷键是 alt-f (再 ctrl-tab 可以切换分数线的大小) 积分号可以用按钮插入, 或者直接 \\int 回车 求和号同理, 可以直接输入 \\sum 回车 ...LaTeX和这比起来简直就是石器时代! 总结一下: 2.4 公式编号/引用公式 公式加入编号非常简单, 用上下文菜单里的一个\"IV\"按钮. 如果对一个公式进行索引的话, 可以直接用tex里的 label/ref 标签完成. 首先给这个公式加一个label, 只需要输入 \\label , 回车后输入参数(label名字)再回车即可. 然后要引用这个公式时, 只需要输入 \\ref 加刚才指定的label即可: 以后对于图片/表格的引用也是同样道理. 关于多行公式, 先要介绍表格的使用, 请看下节. 3. 表格 像上面这个公式的输入方法, 就是在大括号右边使用一个两行一列的表格. 3.1 表格环境 插入表格只需点击第三行的表格按钮: 那些词汇也傻傻分不清楚, 倒是中文界面说的很清楚: 不过这些表格样式也是可以通过 ctrl-tab 快捷键轮转切换的. 点击以后进入表格环境, 注意上下文菜单的按钮又变了: 3.2 增加/删除行和列 在表格环境的上下文菜单里, 那八个中间是加号或叉号的方向按钮就是向各个方向新建/删除一行/一列的. 或者使用快捷键: win-方向键 可以在当前格子的各个方向新建行/列, win-backspace 则是删除. 3.3 对齐模式 改变表格的对齐模式只要选中相应的格子然后用上下文菜单的按钮点选即可: 3.4 矩阵 输入矩阵的话, 可以 在公式环境下 嵌套 一个表格环境 . 首先输入 alt-$ 进入公式环境, 此时再看表格按钮的时候出现了矩阵选项: 然后只需要像之前表格一样操作即可了, 关于如何输入各个方向的省略号, 只要多tab几次就可以了: 以上我觉得基本够用了, 关于表格的其他的功能可以参考tm的文档... 4. 插图 关于插图我使用最多的只是截图然后直接从剪切板里粘贴而已, 这里只介绍这种最简单最常用操作. TeXmacs实际上远比我这里介绍的强大, 见这里: http://www.TeXmacs.org/tmweb/documents/tutorials/TeXmacs-graphical-plugins.pdf 4.1 图片环境 点击按钮插入图片 4.2 插入截图 截图在clipboard的时候, 只需要在TeXmacs里面粘贴就可以了: 我想要强调的一点是, 用这种方法画图, 可以直接把图片嵌入tm文件里面, 而不用像LaTeX一样在文件夹里放一堆图片. 4.3 画图 TeXmacs也提供了直接画图的功能, 不过我基本没试过, 下图是乱画的: 另外, 关于如何引用图片/表格, 只要用 \\label 和 \\ref 标签即可, 前面公式环境里已经介绍过了. 5. 插入目录/参考文献 5.1 插入目录 首先点击第三行的按钮插入目录: 但是这是的目录是空的, 还需要更新目录, 方法是第一行的菜单: document → update → table of content 然后就看到了目录: 5.2 参考文献 bibtex文件 首先, 为了引入参考文献, 需要建立一个 demo.bib 文件, 里面放的就是参考文献的bibtex代码, 比如: @ARTICLE{lda, author = {David M. Blei and Andrew Y. Ng and Michael I. Jordan and John Lafferty}, title = {Latent Dirichlet Allocation}, journal = {The Journal of Machine Learning Research}, year = {2003} } @article{nmf, author = \"Yehuda Koren and Robert Bell and Chris Volinsky\", title = \"Matrix Faoctorization Techniques for Recommender Systems\", journal = \"IEEE Computer\", pages = \"42--49\", year = \"2009\", } @book{pci, title = \"Programming Collective Intelligence: Building Smart Web 2.0 Applications\", author = \"Toby Segaran\", year = \"2007\", } 注意每一个entry的别名( lda, nmfCF, pci )是我自己取的. 插入参考文献 点击按钮插入参考文献: 然后在最下方的提示框输入样式名(一般就用默认tm-plain)以及bib文件名(和tm文件同一目录下): 在引用文献时可以用tex标签 \\cite 引用文献: TeXmacs不会在最后参考文献里显示没有被引用过的文献, 那些没有显式提到的文章可以用插入invisible citation的方式加入: 注意, 此时需要update 两次 才可以看到效果. 6. 更多 这里只是介绍了最基础最简单的使用, 关于更多的高级TeXmacs使用方法, 这里有几个链接: tm官方的tutorial: http://TeXmacs.org/tmweb/help/manual.en.html 一些其他人做的文章: http://TeXmacs.org/tmweb/help/tutorial.en.html 王垠大神做的思维导图: https://www.mindomo.com/mindmap/b207992c90c046bdbe4053cbdf88b5d5 tm官网有个workshop的视频 里面展示了texmac一些亮瞎眼的功能: http://magix.lix.polytechnique.fr/magix/workshop/workshop-videos.en.html 另外, 这篇文章所做的示例文件可以在这里下载: https://github.com/X-Wei/texmacs-demo 欢迎抛弃LaTeX & 拥抱TeXmacs!","tags":"soft","title":"学术文章写作利器: TeXmacs介绍"},{"url":"http://x-wei.github.io/algoII_week3_1.html","text":"1. Introduction to Maxflow Min-cut pb input: edge-weighted digraph G, each edge e has weight( \"capacity\" ) c[e] >=0, a source vertex s , a target vertex t . def . an st-cut (A,B) is a partition of vertices into 2 disjoint sets A and B, with s in set A and t in set B . def . the capacity of a cut (A,B) is sum of capacities of edges going from A to B (not considering B to A) . ⇒ min-cut pb: find the cut (A,B) with min-capacity. Max-flow pb same input: graph G, source s, target t def. an st-flow is an assignment of values to edges f: e→f[e] such that: capacity constraint: 0<=f[e]<=c[e] for any e; local equilibrium: for any vertex v (other than s or t), inflow=outflow ; def. the value of a flow f is the inflow at t . (assume no ingoing edge to s or outgoing edge to t) ⇒ max-flow pb: find f with max value. remark: max-flow and min-cut are dual problems. 2. Ford-Fulkerson Algorithm def. given a flow f for a graph, an \"augment path\" is an undirected path form s to t , if there exist df>0 ( \"bottleneck capacity\" ) such that: for forward edges e: can augment flow by df (not full: f[e]+df<=c[e] ) for backward edges: can decrease flow by df (not empty: f[e]-df>=0 ) def. residual capacity for forward edge e, residual-cap = c[e]-f[e] for backward edge e, residual-cap = f[e] ⇒ an aug-path is a path where each edge has residual capacity >0 . blocking edges : full forward edge or empty backward edge. → idea: increase flow along augment paths. [algo] start: 0 flow: f[e]=0 for all e. find an augment path (and the corresponding df ) in graph, and change the flows along the path by +/-df . loop until no augment path exists. (ie. all path s→t are blocked either by a full forward edge or an empty backward edge, ie. by an edge with 0 residual capacity) FF is a gernel algorithm: 3. Maxflow-Mincut Theorem def . for a cut (A,B), the net flow across the cut ( netflow(A,B) ) is the sum of flows from A to B minus flows from B to A. [flow-value Lemma] For any flow f and any cut (A,B) ⇒ netflow(A,B) = value(f). pf. induction on the size of set B. base case, when B={t}, by def we have netflow(A,B) = value(f) when moving any vertex v from A to B: * netflow(A, B) augment by flow(A→v)+flow(B→v)=inflow(v) , * netflow(A, B) decrease by flow(v→A)+flow(v→B)=outflow(v) , * by equilibrium of flow, netflow(A',B')=netflow(A,B)=value(f) ex. (A: gray vertices, B: white vertices) [cor] outflow(s)=inflow(t)=value(f) [weak duality] For any flow f and any cut (A,B) , ⇒ value(f) <= capacity(A,B). [Augmenting path Th] A flow f is maxflow iff there is no augment path. [maxflow-mincut Th] value(maxflow) = capacity(mincut). pf. for any flow f , prove the equivalence of the 3 following statements: i. there exists a cut st: capacity(cut) = value(f). ii. f is a maxflow. iii. there is no augmenting path wrt f . [i⇒ii] suppose cut(A,B) st: capacity(A,B)=value(f) ⇒ by weak duality, for any other flow f', vlaue(f')<=capacity(A,B)=value(f) [ii⇒iii] (eqv to prove ~iii⇒~ii) suppose there is an aug-path from s to t, of bottleneck capacity=df, ⇒ by improving f with df, we get a f' > f [iii⇒i] suppose there is no aug-path, ie, all path from s to t are blocked by some full-forward edge or empty backward edge. ⇒ let A:=vertices connected with s by a path with no blocking edges, and B := the rest ( so once we get a maxflow, we can compute the mincut in this way ) → for all edges across A and B, all forward edges are full, all backward edges are empty ⇒ capacity(A,B) = netflow(A,B) = value(f) by flow-value lemma CQFD... 过瘾... 4. Running Time Analysis getting a mincut form maxflow? → easy (as discussed in the pf above) computing an aug-path? → BFS does FF algo always terminate? how many augmentations? → ... integer capacity graphs special case of FF algo : edge capacities are integers between 1 and U. invariant: flow is always integer all along FF algo. [prop] nb of augmentations <= value of maxflow. pf. each augmentation will add flow by >=1. [integrality Th] There exist an integer-valued maxflow. Bad case for FF nb of augmentation == value of maxflow (each time, the path through the middle edge is chosen as aug-path) can be easily avoided ⇒ by using shortest(nb of edges)/fastest(biggest df) path Performance of FF depends on the algo for choosing aug-path: 5. Java Implementation representation of flow graph flow edge: each e= v→w, have flow f[e] and capacity c[e]. flow graph: put e in both v and w's adj-list. flow augmentation (by delta) for forward edge e, f[e] += delta for backward edge e, f[e] -= delta Residual graph Gr def . For a flow f and a graph G , the residual graph Gr is obtained by: for each edge e=v→w , (with c[e] and f[e] ) in G , put in Gr : e1=v→w , with weight= c[e]-f[e] e2=w→v , with weight= f[e] (即两个方向上的weight都为residual capacity) (rmq: Gr is just a weighted digraph, not a flow graph) [prop] Augment path in G is equivalent to a path in Gr ( df of aug-path in G = min edge weight in Gr ) . (但是实现的时候其实不用显式构造Gr, 只需BFS的时候修改一下即可) APIs 这里的API设计的非常合理... 导致每一部分的代码量都不大... NB flow-edge: rmq. both calculate residual-cap and augmentation need to specify a direction , so we need a index v as parameter for these 2 functions. public class FlowEdge{ private final int v, w; private final double capacity; private double flow=0.0; FlowEdge(int v, int w, double cap); int from(); int to(); int other(int v); double capacity(); double flow(); double residualCapTo(int v);// residual capacity void addFlowTo(int v, double delta);// augment residual flow } flow graph: public class FlowNetwork{ private Bag<FlowEdge>[] adj;//use adj-list representation for flow graph FlowNetwork(int V); void addEdge(FlowEdge e); Iterable<FlowEdge> adj(int v);// both incoming and outgoing edges ... } FF algo: use a function hasAugPath() to test termination use a function bottleNeck() to get delta if a augpath is found, use two arrays reached[] and edgeTo[] to get the augpath (find the path backwards ). code: public class FordFulkerson{ private boolean[] reached; //reached[v] indicates if a path s-->v exists in Gr, used in DFS private FlowEdge[] edgeTo;// edgeTo[v] = last edge on the path s-->v private double value=0.0;// value of flow public FordFulkerson(FlowNetwork G, int s, int t){ while(this.hasAugPath(G,s,t)){ double delta = this.bottleNeck(); for(int v=t; v!=s; v=edgeTo[v].other(v)) edgeTo[v].addFlowTo(v, delta); this.value += delta;// each time the flow value augments by delta } } private double bottleNeck(){//bottleneck-cap = min residual flow on the aut-path double bottleneck = 9999999; assert(reached[t]);// the aug-path should exsit for(int v=t; v!=s; v = edgeTo[v].other(v)) bottleneck = Math.min(bottleneck, edgeTo[v].); return bottleneck; } private boolean hasAugPath(FlowNetwork G, int s, int t){ // perform a BFS Queue<Integer> q = new LinkedList<Integer>(); this.reached = new boolean[G.V()]; this.edgeTo = new FlowEdge[G.V()]; q.add(s); while(!q.isEmpty()){ int v = q.deque(); for(FlowEdge e:G.adj(v)){ int w = e.other(v); if(!reached[w] && e.residualCapTo(w)>0){// modified BFS: valid edges are those with residualCap>0 edgeTo[w] = e; reached[w] = true; if(w==t) return true;// t is reached by BFS q.enqueue(w); } } }// BFS while loop return false; } }//class FF 6. Maxflow Applications 关键是建模很巧妙... ex1. bipartite matching pb 二分图的最大匹配问题. (有点像marriage stable问题...但是不一样 因为没有preference order) ⇒ is there a way to match all students to a job? ie. given a bipartite graph, find a perfect matching. modeling add source s and target t all edges from s to students: capacity=1 all edges from companies to t : capacity=1 all edges from student to company: capacity=INF ⇒ find maxflow in the graph when no perfect matching: mincut can explain why in the above case, student 2,4,5 can only be matched to 7,10 ⇒ mincut can help us find such cases! recall: how to get mincut from maxflow mincut = (A,B), where: A:=vertices connected with s by a path with non blocking edges, B := the rest ( blocking edges: full forward edge or empty backward edge on path ) ex. let S=students on s side of mincut (in above case, S={2,4,5} ) let T=companies on s side of mincut (in above case, T ={7,10} ) |S|>|T|, that's why no perfect matching! ex2. baseball elimination (前三列是目前成绩, 后面四列是接下来赛程矩阵) Montreal is mathematically eliminated → easy to see → Philly is mathematically eliminated also ! another case: Detroit is mathematically eliminated ! whether team-4 still has a chance to win? modelling remaining games flow from s to t. use team-pairs ans teams as vertices carefully chosen capacities (see below) ⇒ team 4 could win iff all flow from s are full (ie. all match points can be repartitioned over other teams without depassing team 4's maximum wins ). 总之很巧妙....","tags":"notes","title":"[Algorithms II] Week 3-1 Maximum Flow"},{"url":"http://x-wei.github.io/algoII_week2_2.html","text":"1. Shortest Paths APIs context: directe, weighted graphs . shortest path variants in terms of vertices: source-sink: form one vertex to another single source : from one vertex to all others (considered in this lecture) all pairs constraints on edge weights: nonnegative weights arbitary weights eculidean cycles: no directed cycles no negative cycles APIs for weighted directed edge: public class DirectedEdge{ DirectedEdge(int v, int w, double weight); int from(); int to(); double weight(); } for edge-weighted digraph: public class EdgeWeightedDigraph{ private final Bag<DirectedEdge>[] adj; EdgeWeightedDigraph(int V); void addEdge(DirectedEdge e); Iterable<DirectedEdge> adj(int v); int V(); } for single source shortest-path: public class SP{ SP(EdgeWeightedDigraph G, int s);//s is the source node double distTo(int v);//dist from s to v Iterable<DirectedEdge> pathTo(int v);// shortest path from s to v } 2. Shortest path properties goal: single-source shortest path prop . a shortest-path-tree (SPT) exists. 这个结论以前没见过... 不太evident吧... 没有太想明白 ⇒ consequence: can represent this SPT by 2 vertex-indexted arrays: double distTo[v] = shortest path length s → v DirectedEdge edgeTo[v] = last edge to v in shortest path, edgeTo[s] = null private int distTo[] = new int[V]; private DirectedEdge edgeTo[] = new DirectedEdge[V]; public double distTo(int v){ return this.distTo[v]; } public Iterable<DirectedEdge> pathTo(int v){ Stack<DirectedEdge> path = new Stack<DirectedEdge>(); for(DirectedEdge e = this.edgeTo[v]; e!=null; e = edgeTo[e.from]) path.push(e); return path; } Edge relaxation dynamic prog: distTo[v] = length of known shortest path from s to v distTo[w] = length of known shortest path from s to w edgeTo[w] = last edge in the known shortest path form s to w (consider edges one by one) def . edge e = v-->w relaxes if e.weight+distTo[v] < distTo[w]. → update distTo[w] and edgeTo[w]. private void relax(DirectedEdge e){ int v = e.from(), w = e.to(); if(distTo[w]>distTo[v]+e.weight()){ distTo[w] = distTo[v]+e.weight(); edgeTo[w] = e; } } optimality conditions prop . optimality conditions distTo[] is the solution iff : distTo[s] = 0 distTo[v] is the weight of some path from s to v for any edge e = v → w, distTo[w] <= distTo[v]+e.weight() Generic algo just relax all edges... prop . the above generic algo gives the SPT from s. implementations of the algo: Dijkstra (nonnegative edge) Topological sort (!) (no directed cycles) Bellman-Ford (no negative directed cycles) 3. Dijkstra's Algorithm (non-negative edges) consider vertices in increasing distance from s.* add vertex to the SPT, relax all edges from that vertex. each time: take the closest vertex to s that is not in the SPT (ie. whose dist is not determined yet), add the vertex, and relax all its outgoing edges. prop . Dijkstra works. pf. each edge e = v → w is considered exactly once (when vertex v is added to SPT). distTo[w] <= distTo[v]+e.weight() after relaxing e ineq holds until algo terminates * distTo[w] decrease monotonely, * distTo[v] will not change because each time we choose distTo[v] smallest, and all edges non-negative. * CQFD by optimality condition. implementation Use an IndexMinPQ to store vertices. public class DijkstraSP{ private int dist[]; private DirectedEdge edgeTo[]; private IndexMinPQ<Integer,Double> pq; DijkstraSP(EdgeWeightedDigraph G, int s){ this.dist[] = new int[G.V()]; this.edgeTo[] = new DirectedEdge[G.V()]; this.pq = new IndexMinPQ<Integer,Double>(G.V()); for(int v = 0; v<G.V(); v++) dist[v] = 999999; dist[s] = 0; pq.insert(s, 0.0); while(!pq.isEmpty()){ int v = pq.delMin(); for(DirectedEdge e:G.adj(v)) relax(e);// decrease key or insert vertices to pq } }//constructor() private relax(DirectedEdge e){ int v = e.from(), w = e.to(); if(dist[w]<dist[v]+e.weight()){ dist[w] = dist[v]+e.weight(); edgeTo[w] = e; if(pq.contains(w)) pq.decreaseKey(w, dist[w]); else pq.insert(w, dist[w]); } }//relax() }//class Dijkstra is in fact a Prim algorithm ! Both are algos that compute a spanning tree. in Prim: each time takes the vertex closest to the tree (and is for undirected graph). in Dijkstra: each time takes the vertex closest to source s (and is for directed graph). DFS BFS are also computing spanning tree! ...NB!! complexity V insertions to PQ (each vertex is added to the SPT) V delMin from PQ E decrease key → depends on the PQ implementation. in our implementation, O(ElogV) 4. Edge-Weighted DAGs in a DAG, it's easier to find the shortest path ? ⇒ yes! simple algo: consider vertices in topological order relax outgoing edges form this vertex. correctness prop . topo-sort algo computes SPT in any DAG ( even with negative weights ) in time O(E+V) (linear time!) . pf. each edge is relaxed exactely once. distTo[w] <= distTo[v]+e.weight() after relaxing e ineq holds until the algo terminates because: * distTo[w] decrease monotonely, * distTo[v] will not change because of topo-order, no edge pointing to v after v is relaxed. * CQFD implementation public class AsyclicSP{ AsyclicSP(EdgeWeightedDigraph G, int s){ // init dist[] and edgeTo[]... Topological topo = new Topological(G); for(int v: topo.order()) for(DirectedEdge e:G.adj(v)) relax(e); } } application seam-carving Resizing images non-uniformly without distortion. 好神奇! Grid DAG of pixels, edges are pointed to 3 downward neighbors. 横向缩小: 删掉一个\"seam\" longest path in DAG → just negate all edge weights , as this algo is OK for negative edges. parallel job scheduling 感觉有点运筹的意思, 关键是建立一个DAG, 比较有技巧性: add source and sink split each job to 2 vertices (begin and end) ⇒ use longest path to schedule jobs 5. Negative Weights 只是给所有边增加weight并 不能 解决负权边问题. ⇒ need a different algo. prop. SPT exists iff there is no negative cycles. Bellman-Ford 一句话总结Bellman算法: 每次relax所有的边, 进行V次. ( 因为relax k次以后, s到v的最短路径的长度小于等于k. ) prop . Bellman-Ford works, complexity is O(EV) . pf. after i passes, found shortest path contain at most i edges. code: public class BellmanFordSP{ BellmanFordSP(EdgeWeightedDigraph G, int s){ // init dist[] and edgeTo[]... for(int i=0;i<G.V();i++) for(DirectedEdge e:G.edges()) relax(e); } } improvement if dist[v] is not changed during one pasee → no need to relax incoming edges any more. → much faster in practice. FIFO implementation: maintain a queue of vertices whoses dist is changed. Finding negative cycle add two method to SP: boolean hasNegCycle(); Iterable<DirectedEdge> negCycle(); prop . if a vertex u is updated in phase V, there exists a negative cycle, and can track edgeTo[u] to find the cycle. 原因是最短路径的长度小于V, 如果大于V则说明存在negative cycle. negative cycle application arbitrage detection 建模: graph of currencies, edge weight = exchange weight (complete graph). ⇒ find a cycle whose product of edge is >1. ⇒ take logs to make a shortest-path pb. ⇒ take minus log, then try to find a negative cycle. NB!! Summery","tags":"notes","title":"[Algorithms II] Week 2-2 Shortest Paths"},{"url":"http://x-wei.github.io/algoII_week2_1.html","text":"1. Introduction to MSTs Given: undirected connecte graph G with positive edge weights. def. Spanning tree T is a subgraph of G , that is both tree ( connected, acyclic ) and spanning( all vertices are included ). ⇒ Goal: find a spanning tree with minimum weight sum. 2. Greedy Algorithm assumptions for simplification: edge weights are distinct graph is connected → MST uniquely exists. cut property def. a cut of a graph is a partition of its vertices into 2 non-empty sets. def. a crossing-edge (wrt a cut) is an edge connecting vertex from one set to another. prop. Given any cut, the crossing edges with minimum weight is in the MST . proof. Given a cut. {S1,S2} are the two set of vertices, let e be the min-weighted edge among all crossing-edges. If e is not in the MST → exist another crossing-edge, f , in the MST (otherwise not connected) → adding e to the MST will create a cycle (tree property) → the edge f will be in this cycle → removing f and adding e will give us another spanning tree (!) → this new spanning tree has smaller weight sum ⇒ contradiction, CQFD. Greedy MST algo [ algo ] Greedy MST initialize: all edges not selected (colored gray) find any cut with all crossing-edge gray use this cut and select the min-weighted crossing edge (color the edge as black) repeat V-1 times. prop. the greedy algorithm gets the MST. pf. any selected (black) edges are in the MST (according to the cut property) If we haven't selected V-1 edges → there is always a cut with all crossing-edges gray. (证明algo不会卡死) ( if edge weight not distinct, the proof fails, but can be fixed) efficient implementations : how to choose the cut each time? how to find min-weighted crossing-edge? ⇒ Kruskal & Prim 3. Edge-Weighted Graph API Edge API → Edge abstraction : make Edge comparable. public class Edge implements Comparable<Edge>{ Edge(int v, int w, double weight); int either();// get one of the endpoint of edge (as we are in undirected graph contex here) int other(int v);// get the other endpoint int compareTo(Edge that);// compare by edge weight double weight(); } Edge-weighted Graph API adj-list implementation: Bag<Edge>[] adj; (for undirected graph, each edge appears twice in adj) public class EdgeWeightedGraph{ private final int V; private final Bag<Edge>[] adj; EdgeWeightedGraph(int V){ this.V = V; this.adj = (Bag<Edge>)new Bag[V]; for(int v=0;v<V;v++) adj[v] = new Bag<Edge>(); } void addEdge(Edge e){// use the Edge class instead of directly v and w int v = e.either(), w = e.other(); adj[v].add(e); adj[w].add(e); } Iterable<Edge> adj(int v){//get Edges incident to v return adj[v]; } Iterable<Edge> edges();// get all Edges } (allow self-loops and parallel edges) MST API public class MST{ MST(EdgeWeightedGraph G);//compute the MST Iterable<Edge> edges();// selected edges in the MST double weight();// sum of all edge weights in MST } 4. Kruskal's Algorithm [algo] consider edges in ascending order of weight, add the edge to MST unless it creates a cycle . In the running of Kruskal: we have several small connect components and they merge with each other until we get MST. correctness prop. Kruskal's algo works. pf (idea: proove that Kruskal is a special case of the greedy algorithm, ie. how to select the specific cut) suppose Kruskal's algo selects(colored black) an edge e=v-w → select a cut = vertices connected to v in the (constructing) MST; and the rest vertices. → for this cut, there is no black crossing edges → moreover among all crossing edges of the cut the edge e has the smallest weight!! (by def of Kruskal) CQFD implementation how to test if adding an edge will create a cycle ? DFS from v to w? → O(V) ⇒ Union-Find ! O(lg*V) ☺ (almost constant time) if find(v)==find(w) , then we know adding e will create a cycle. considering edges in order? → use a prority queue. public class KruskalMST extends MST { private Bag < Edge > mst = new Bag < Edge > (); public KruskalMST ( EdgeWeightedGraph G ) { MinPQ < Edge > pq = new MinPQ < Edge > (); // build pq --> can be optimized to O ( n ) if build bottom - up for ( Edge e : G . edges ()) pq . insert ( e ); UF uf = new UF ( G . V ()); // build a UF of V elements while ( ! pq . isEmpty () && mst . size () < G . V () -1 ) { Edge e = pq . delMin (); int v = e . either () , w = e . other ( v ); if ( uf . connecte ( v , w ) ) continue ; uf . union ( v , w ); this . mst . add ( e ); } } public Iterable < Edge > edges () { return this . mst ; } } complexity running time: O(ElogE) 5. Prim's Algorithm since 1930... Idea: start from a vertex and grows the tree T to MST. [algo] Add to the tree T the edge that have exactely one endpoint in T and with minimum weight, repeat V-1 times. In the running of Prim: there is always ONE connnected component . Correctness prop . Prim's algo works. pf. suppose edge e is the min-weighted edge connect a vertex in T with a vertex out of T. → select the cut = vertices in the tree T; vertices out of T → by def, there is no black crossing edge → e is the min-weighed edge by def of Prim. CQFD implementation challenge: how to find such an edge (connect T and other vertex, with min weight) ? ⇒ priority queue \"lazy\" implementation [algo] Maintain a PQ of edges that connect T and the rest vertices. e = pq.delMin(), e = v-w , if v and w are both in T (as edges in pq might become obsolete as T grows) ⇒ just disregard it to maintain the pq: add all incident edges(with other endpoint not in T) of the newly added vertex to pq public class LazyPrimMST{ private Bag<Edge> mst; LazyPrimMST(EdgeWeightedGraph G){ boolean[] marked = new boolean[G.V()]; // vertices in T MinPQ<Edge> pq = new MinPQ<Edge>(); this.mst = new Bag<Edge>(); marked[0] = 0; // add vertex 0 to T for(Edge e:G.adj(0)) pq.insert(e);// add edges to pq while(!pq.isEmpty() && this.mst.size()<G.V()-1){ e = pq.delMin(); int v = e.either(), w = e.other(v); if(marked[v] && marked[w]) continue;//ignore obsolete edges v = marked[v] ? w : v;// v is the newly added vertex marked[v] = true; for(Edge e:G.adj(v)){ if(!marked[e.other(v)]) pq.insert(e); } } } } Running time: O(ElgE) space: O(E) in worst time. \"eager\" implementation Idea: use a PQ of vertices , priority of vertex v := min-weight of edge that connects v to T. [algo] Get from pq the vertex v that is closest to T, add it to T. Update pq -- consider v 's incident edge e=v-w: if w in T → ignore else: if w in pq → add w to pq else → if v-w has smaller weight than the current priority, update w 's priority. repeat till get V-1 edges. key implementation component: a MinPQ that supports priority(key) update. class IndexMinPQ <Key extends Comparable<Key> >{ IndexMinPQ ( int N );// indices of elements: 0 ... N-1 void insert ( int i , Key key ); void decreaseKey ( int i , Key key );// update the key ( priority ) of element-i int delMin (); int size (); } implementation of such a PQ: Use same code as standart PQ (maintain a heap[] array). Elements are always accessed by \"index\", in range 0...N-1. maintain 3 parallel arrays: keys[], pq[], qp[] : keys[i] : is the priority of element i (the element with index=i) pq[i] : is the index of the element in the heap position i (ie. in heap[i] is pq[i]th element ) qp[i] : is heap position of element i ( ⇔ the ith element is in heap[qp[i]] ) to decreaseKey(i,key) : change keys[i] , then call siftup(qp[i]) summery of pq implementations: 6. MST Context unsolved pb: does a linear MST algo exists? (recap: for UF, tarjan has prooved that linear algo doesn't exist — although Nlg*N is fast enough...) @_@... (这个Yao是清华那个Yao吧?) Euclidean MST Given N points in plane, edge weight := Euclidean distance. ( dense graph, E = V2 ) → exploit geomerty, O(NlgN) clustering k-clustering (~ dist-fcn) single-link clustering (def. dist of clusters = dist of 2 closest elements in each cluster) → Kruskal...","tags":"notes","title":"[Algorithms II] Week 2-1 Minimum Spanning Trees"},{"url":"http://x-wei.github.io/algoII_week1_2.html","text":"1. Intro to digraphs Has profound differences wrt undirected graphs. def: digraph edges: have directions vertex: distinguish indeg and outdeg digraph pbs: path/shortest path topological sort: Can you draw a digraph so that all edges point upwards? strong connectivity: Is there a directed path between all pairs of vertices? transit closure PageRank 2. Digraph API public class Digraph{ Digraph(int V); void addEdge(int v, int w);// edge is directed Iterable<Interger> adj(int v);// vertices reached by outgoing edges int V(); Digraph reverse();// <--new methode wrt undirected graph } representation: adj-list, ie. an array of bags. Bag<Integer>[] adj;// prec vertices 3. Digraph Search BFS and DFS can be applied to digraphs. reachability find all vertices reachable from vertex-s. use the same DFS as for undirected graphs. → application: programme control-flow analyse, garbage collection. DFS is the basis for a lot of digraph pbs : 2-satisfiability, Euler path, strongly connected component. multiple source shortest path: ⇒ use DFS but enque all vertices in the set . → application: web crawler(DFS not suitable for crawling) 4. Topological Sort application. precedence schedule, java compiler (cycled inheritance), ... def. topo-order is a permutation of vertices, where for each vertice v→w, w is behind v in the permutation. def. DAG directed acyclic graph. prop. for a digraph, topological order exists iff graph is a DAG. algo: ⇒ use DFS~ reverse DFS postorder def. postorder is the order of the vertices that we have finished (ie. we have visited all reachable vertices from this vertex). implementation 这个以前的blog写过... private boolean[] visited; private Stack<Integer> revPostorder;// stores the vertices in reverse post order private void dfs(Digraph G, int v){ visited[v] = true; for(int w: G.adj(v)) if(!visited[w]) dfs(G, w); //** now we know the vertex v is \"finished\" ** revPostorder.push(v); } public Iterable<Integer> topoOrder(Digraph G){ for(int v=0;v<G.V();v++) if(!visited(v)) dfs(G,v);// visit all cc return revPostorder; } proof prop. reverse post-order of a DAG is in topological order. (这个证明蛮精彩) pf. for any edge v→w , when dfs(v) is called: case 1: dfs(w) is called and returned, so w is done before v in post-order; case 2: dfs(w) is not called, it will be (in)directly get called by dfs(v) , so dfs(w) finishes before dfs(v) ; case 3: dfs(w) is called but NOT returned (ie, w not finished ) → exist path from w to v ⇒ graph is not a DAG! (cycle detection) 5. Strong Components For undirected graphs: connected components can be solved with dfs or UF. def. Strongly-connected v and w are strongly-connected if exist path from v to w and w to v. → is an equivalent relation. def. Strong Component subset of V where each pair are strongly-connected. Goal: compute all strong components( scc ) in a digraph. linear time DFS solution: Tarjan (1972) (developed version: a two-pass linear-time algorithm) Intuition: scc for G is the same for G.reverse(). Kernel DAG : contract each scc into a single vertex. Idea: compute topological-order in the kernel DAG. run DFS, consider vertices in reverse-topo-order [Algo] 1. compute topo-order in G.reverse (just a DFS in the reversed graph) 2. run DFS in original G , visit unmarked vertices in topo-order of G.reverse . (instead of visiting vertices by their index) ⇒ each time we finish a dfs from a vertex, we get a scc! 太精彩了!!! proof: tricky, cf book...(貌似Werner课上讲过..) implementation private int[] scc = new int[V]; // scc[v] is the index of the SCC that v belongs to private int sccCount = 0; private boolean[] visited = new boolean[V]; public getSCC(Digraph G){ // 1. get topo-order in reverse graph Iterable<Integer> topoOrderGR = topoOrder(G.reverse()); // 2. run dfs in original graph, run on vertices using the above topo-order for(int v:topoOrderGR)// <-- only difference from the standard topo-order algo if(!visited[v]) dfs(G, v, sccCount++);//increment sccCount everytime we done a component } private dfs(Digraph G, int v){ // run dfs from v, and all touched vertices are marked in sccId's SCC visited[v] = true; scc[v] = sccCount; for(int w:G.adj(v)) if(!visited[w]){ scc[w] = sccCount; dfs(G,w); } }","tags":"notes","title":"[Algorithms II] Week 1-2 Directed Graphs"},{"url":"http://x-wei.github.io/algoII_week1_1.html","text":"1. Intro to graphs Graph: vertices connected by edges. terminology: path : sequence of vertices connected by edges cycle : path with same starting and ending vertex two vertices are connected : if there is a path between ex of graph problems: path: or connectivity shortest path cycle Euler tour (ouii..) Hamilton tour MST bi-connectivity: is there a vertex whose removal disconnects the graph? planarity isomorphism 2. Graph API graph representation vertex representation: use integers between 0 and V-1 anormalies: self-loop and multiple edges are possible public class Graph{ Graph(int V); void addEdge(int v, int w); Iterable<Integer> adj(int v); int V();// nb of vertices int E();// nb of edges } print all edges: basic functions: static int degree ( Graph g , int v ) { int deg = 0 ; for ( int w : G . adj ( v )) deg ++ ; return deg ; } static int nbOfSelfloops ( Graph g ) { int cnt = 0 ; for ( int v = 0 ; v < G . V (); v ++ ) for ( int w : G . adj ( v )) if ( w == v ) cnt ++ ; return cnt / 2 ; } edge representation set-of-edge implementation: a list of all edges ⇒ can lead to inefficient implementation adj-matrix implementation: maintain a 2d (V*V) boolean array ⇒ space complexity too heavy adj-list implementation: vertex-indexed array, each array entry is a Bag (类似桶bucket) ⇒ sutable for sparse graphs adj-list implementation: private final int V; private Bag<Integer>[] adj; public Graph(int V){ this.V = V; this.adj = (Bag<Integer>[]) new Bag[V];// java cannot create generic array for(int v = 0; v<V; v++) adj[v] = new Bag<Integer>(); } public addEdge(int v, int w){ adj[v].add(w); adj[w].add(v);// if undirected graph } 3. Depth-First Search Tremaux maze exploration: trace back when no unvisited vertices availiable. 动画好看... DFS goal: systematically search through a graph. design pattern : decouple graph data and graph processing. public class Paths{ Path(Graph G, int s);// graph G and source s boolean hasPathTo(int v); Iterable<Integer> pathTo(int v); } algo: 注意每次访问节点以前就将其mark. implementation 用一个boolean数组 visited[] 作为标记 为了找到一条具体的路径(ie, 一系列节点), 维护一个 prev[] 数组, 存放当前节点是从哪个节点走过来的. //public class DFSpaths extends Paths... boolean[] visited = new boolean[V]; int[] prev = new int[V]; public void dfs(int v){ visited[v] = true; for(int w: G.adj()) if(!visited[w]) { prev[w]=v; dfs(w); } } public Iterable<Integer> pathTo(int v){ Stack<Integer> s = new Stack<Integer>(); for(int x = v; x!=s; x = prev[x]) s.push(x); return s; } properties prop . DFS visite all edges in time propotional to the sum of their degrees(ie. nb of edges). 4. Breadth-First Search not recursive algo. maintain a queue, add to queue for all vertices not-marked. implementation use visited[] to mark vertices use a prev[] array to get explicit path use a dist[] array to record the shortest dist from v to source (can use dist to replace visited ) public void bfs(Graph G, int s){ boolean visited[] = new boolean[G.V()]; int prev[] = new int[G.V()]; int dist[] = new int[G.V()]; Queue<Integer> q = new Queue<Integer>(); visited[s] = true; q.push(s); while(!q.isEmpty()){ int v = q.dequeue(); for(int w:G.adj(v)) if(!visited[w]) { prev[w] = v; visited[w] = true; q.enqueue(w); } } } property prop. BFS computes the shortest path from s to all vertices using time propotional to E+V. intuition: BFS examines nodes by increasing distance 5. Connected Components dealing with connectivity(equivalence) queries ⇒ answer in constant time (with preprocessing ). public class CC{ boolean connected(int v, int w); int count();// nb of CCs int id(int v);//id for a CC } ⇒ Union-Find ? ⇒ Use DFS!! def. connected component is a maximal set of connected vertices. algo: for each unmarked vertex, run dfs(with increasing cc id)... after the preprocessing, we can get the array id[] and cc count cnt ... 6. Graph Challenges some typical pbs pb1. bipartite graph Can we divide vertices into 2 subsets, where all edge go from one subset to other. ⇒ can be done with dfs. cf. booksite pb2. cycle detection ⇒ simple using dfs. pb3. Euler cycle Find a cycle that uses all edges exactely once. [Euler] a graph is Eulerian iff all vertices have even degree. ⇒ typical diligent algo students can do. cf. booksite pb4. Hamilton cycle Find cycle that visits each vertex exactly once. ⇒ intractable (typical NP-complete pb) pb5. isomorphism of graphs Are two graphs identical except for vertex names? ⇒ no one knows... pb6. planary graph Lay out a graph in the plane without crossing edges? ⇒ expert level. exists linear time algo based on DFS by Tarjan, but too complicated.","tags":"notes","title":"[Algorithms II] Week 1-1 Undirected Graphs"},{"url":"http://x-wei.github.io/shortest-path-summary.html","text":"weighted graph的最短路径问题有三个非常有名的算法, 分别以三个大牛的名字命名, 今天用尽量简洁的篇幅一一介绍. 简单起见(这回只写伪代码好了), 对于图的定义如下: node index = {1,2,...,n} e[u,v] = distance of edge(u,v); if (u,v) is not an edge, e[u,v]=INF 令N=点的数量, M=边的数量 任意两点最短路径: Floyd-Warshall Floyd算法解决的问题是: 对于 任意 两个节点 s (source)和 t (target), 求s到达t的最短路径. Floyd算法的思想是动态规划: 定义 d(i,j,k) 为点i到点j之间, 只允许借道节点1...k的最短路径 初始化: d(i,j,0)=e[i,j] (即i到j之间不经由其他任何中转节点的最短路径) 更新dij的公式就是: d(i,j,k)=min( d(i,j,k-1), d(i,k-1,k-1)+e[k,j]) 更新n次 每次更新dij的意思就是: 现在从i到j可以经过节点k了, 那么看一下ij之间从k这个点经过的话(i → k → j 这条路)能不能缩短dij. i到j最短路径最终就是: d(i,j,n) 即i到j的路程可以经过1~n中的任何中转节点. 伪代码特别短(其实真代码也一样短....): for all [i,j]: // initialize d[i,j] = e[i,j] for k = 1 ~ n: // relax for k times: for all [i,j]: d[i,j] = min(d[i,j], d[i,k] + e[k,j]) 核心代码只有最后三行... 运行结束后 d[i,j] 就保存着任意i和j之间的最短路径长度. 程序主循环n次, 每次要处理遍历所有的ij组合, 所以复杂度是 O(N&#94;3) . 单源最短路径: Dijkstra Dijkstra算法解决的问题是: 没有负权边的情况下, 从 源节点 s 到其他任意节点 t 的路径长度. 维护一个dist数组, dist[i]就表示(目前为止)s到i的最短距离. 对于每个元素, 标记 是否其dist是否已经确定不再更改(或者说维护两个集合: 一个集合的dist确定, 另一个未确定). Dijkstra算法是一种贪心策略: 每次在未确定最短路径的节点里挑选距离s最近的那个点, 把这个点标记为已经确定dist, 然后对从这个点出发的边进行松弛. 为了标记每个点, 这里用一个bool数组表示: determined[i] 为true表示i的dist已经是最短路程, 为false表示还不确定. 算法如下: 初始化 dist[i] = e[s,i] , determined[i] 全为false 在dist未确定的元素里( determined[i]==false )寻找一个dist最小的节点 u: 标记 u 的dist已经确定 determined[i]=true 用u的所有出边进行松弛: s到i如果经过(u,i)这条边会不会变近? dist[i] = min(dist[i], dist[u]+e[u,i]) 重复循环直到所有的点都确定dist(or 重复N遍即可: 每次只会确定一个新的节点的距离 ) 伪代码: for i in 1~n: dist[i] = e[s,i] determined[i] = false loop N times: u = argmin(dist[i]) among all i that determined[i]==false determined[u] = true // determine one node at each loop for v such that e[u,v]<INF: dist[v] = min( dist[v], dist[u]+e[u,v] ) 以上代码的复杂度为O(N&#94;2), 不过如果用堆来优化寻找最近的u的距离, 复杂度可以变得更低. 有负权边的单源最短路径: Bellman-Ford Dijkstra算法的缺点在于不能处理边长为负数的情况, 而这就是Bellman-Ford算法解决的. Bellman算法也是一种动态规划(动态规划这个东西就是Bellman提出来的): 定义 d(i,k) 为源点s到i 最多经过k条边的 最短距离 初始化: d(i,1)=e[s,i] 每次更新di的公式: for all (u,v): d(v,k)=min( d(v,k-1), d(u,k-1)+e[u,v] ) 更新n-1次 为什么是更新n-1次? 因为s到i的最短路径至多只有n-1条边(即s到i的路径经过了所有n个点). 注意每次更新, 需要把 所有的边 试一遍: 看看用每条边(u,v)能不能松弛dv. 伪代码: for i in 1~n: dist[i] = e[s,i] loop n-1 times: for all (u,v) such that e[u,v]<INF: dist[v] = min( dist[v], dist[u]+e[u,v] ) 核心代码也是最后三行... 太tm精妙了!! 外层循环N-1次, 内层循环M次, 所以代码的复杂度是O(NM). More 以下问题有空再写... negative cycle A*","tags":"tech","title":"最短路径三剑客: Floyd, Dijkstra, Bellman"},{"url":"http://x-wei.github.io/dfs-summary.html","text":"今天总结一下也许是搜索问题里最重要的算法: DFS ! 由于树可以看成是一个graph, 这里还是只写对于graph的DFS算法. Graph类的定义还是用每一个节点保存邻居信息: public class GraphNode{ int val; List<GraphNode> neighbors; } 为了防止重复, 仍然用一个HaseSet记录走过的节点: HasheSet<GraphNode> visited = new HasheSet<GraphNode>(); Recursive DFS 首先写递归版本的DFS, DFS就是一条路走到底, 不撞南墙不回头, 所以递归写起来很自然: 每到一个节点, 标记其已经访问过了, 然后对于邻居里面没有访问的节点继续递归进行DFS. 递归的DFS代码非常简洁: public void DFS(GraphNode nd){ System.out.println(nd.val); visited.add(nd); for(GraphNode next: nd.neighbors){ if( !visited.contains(next) ) DFS(next); } } 虽然这个算法很短, 但是它非常重要, 回溯算法(backtracking)其实就相当于在问题的求解域做一个dfs. 另外拓扑排序也是基于递归dfs进行一点点修改. Non-recursive DFS 非递归版本的dfs同样很重要, 因为毕竟非递归的版本效率高一些, 另外这个算法和bfs非常相似, 只不过把队列queue换成了栈stack而已: public void DFS(GraphNode start){ Stack<GraphNode> s = new Stack<GraphNode>(); q.push(start); visited.add(start); while(!s.empty()){ GraphNode cur = s.pop(); System.out.println(cur.val); for(GraphNode next: cur.children){ if(!visited.contains(next)){ s.push(next); visited.add(next); // mark node as visited when adding to stack! } } }//while } 同样要注意的一点就是在把一个节点入栈的时刻就将其标记为已访问. (for Trees) DFS with depth 和上次 bfs 一样, 对于树而言 , 在dfs搜索的过程中也可以记录该节点所在的depth. 非递归版本的程序就是用一个和上面 s 平行的栈记录深度, 程序和\"BFS with distance\"很像. 递归版本只要在函数签名里加上一个depth的参数即可. 这两个实现都很简单, 就不写了. 注意这个只对于树有意义, 对一个图而言没有depth一说... DFS for binary tree: Preorder traversal dfs另一个有用的性质是: 对于 二叉树 而言, dfs得到的节点顺序正是其前序遍历(preorder traversal)的顺序. 其实前序遍历的定义就相当于是一个递归版本的dfs了: [preorder(node)] = node.val + [preorder(node.left)] + [preorder(node.right)] DFS with path 如果在访问到某一个节点的时候想同时获得到该点的路径, 其实也不麻烦. 对于递归版本的dfs而言, 可以在参数里面用一个List记录到当前节点的路径. 非递归的版本的话... 貌似不是很trival, 需要对stack做好维护, 可能需要一个hashmap什么的... 以后有空了再写写. Cycle Detection 判断一个有向图是否存在回路是一个非常重要的问题, 简单修改dfs就可以做到了. 在递归版本的dfs里, 我们对每一个点改为 三种标记 : 未访问过(0), 正在访问其邻居节点(1), 已经访问完该节点以及所有该节点可以到达的节点(2) . 什么时候会出现回路呢? 就是当前节点v的一个邻居u的状态为1的时候 . 因为该节点状态为1, 即还没有把它以后的节点全部遍历, 所以当前节点v肯定可以从u到达, 而现在又可以从v到达u, 所以构成一个回路. 为了表示一个节点的三种状态, 我们把visited的定义改一下, 定义为一个hashmap: HasheMap<GraphNode, Boolean> visited = new HasheMap<GraphNode, Boolean>(); 节点不在visited表示还未访问过, 节点对应为false表示正在访问, 节点对应为true表示已经访问该节点以及所有可以从它到达的节点. 写一下代码: public void DFS(GraphNode nd){ visited.put(nd, false); // mark as status-1 for(GraphNode next: nd.neighbors){ if( !visited.contains(next) ) DFS(next); else if(visited.get(next)==false) // found cycle System.out.println(\"Cycle detected!!!\"); }// now all touchable nodes from nd are visited visited.put(nd, true); // mark as status-2 } 非递归版本的话貌似也不很容易, 暂时不写了, 以后有空了考虑一下... Topology Sort 这一节(以及上一节)参考这个非常棒的视频: https://class.coursera.org/algo-003/lecture/52 拓扑排序是一个dfs的应用, 所谓拓扑排序是指在一个DAG(有向无回路图)里给每个节点定义一个顺序(v1...vn), 使得按照这个顺序遍历的节点, 每一个节点vi都是之前遍历过的的节点(v1 ~ vi-1)所指向的(或没有任何其他节点指向的). 好像还没说清楚... 拓扑排序的一个应用就是对于各种依赖性(比如学习课程A需要先学习过课程B)组成的图寻找一个节点遍历的顺序使其可行. propositions : 拓扑排序的结果不唯一. 有回路的图不存在拓扑顺序. 如果一个节点没有出边, 那么它可以放在拓扑排序的最后面(没有节点以来它). 如果一个节点没有入边, 那么它可以放在拓扑排序的最后面. 简单修改一下递归的dfs就可以处理拓扑排序: 维护一个计数器 K (初始化为n=所有节点数), 每当一个点已经遍历完毕(所有通过这个点可以到达的点都已经被走过)以后, 就把这个点的顺序设为K, 同时减少K. 就用一个HashMap来为每个节点关联一个序号好了: HasheMap<GraphNode, Integer> order = new HasheMap<GraphNode, Integer>(); public void DFS(GraphNode nd){ for(GraphNode next: nd.neighbors){ if( !visited.contains(next) ) DFS(next); }// all touchable nodes from nd are visited order.put(nd, K--); } 是不是特别简单, 太神奇了! 上面只是对于一个点进行的, 为了给所有点拓扑排序, 只要从一个没有出边的节点出发进行遍历, 一直运行到所有的节点都已经访问过为止.","tags":"tech","title":"深度优先搜索(DFS)小结"},{"url":"http://x-wei.github.io/simhash.html","text":"除了上次介绍的 minhash 方法以外, 还有一种常见的hash方法, 叫做simHash. 这里做简要介绍. 这个hash函数的背景和上次一样, 还是考虑把文本抽象为ngram的集合: 然后相似度依旧是Jaccard similarity: simHash simHash的方法听上去比minHash还要简单: 对一个文档 d 中的每一个term(ngram, shingle) t , 计算其hashcode(比如用java内建的 Object.hashCode() 函数) hash(t) . 把d中所有term的 hash(t) 合成为一个hashcode作为d的hashcode simHash(d) : simHash(d) 的长度与 hash(t) 相同, simHash(d) 的第k个bit的取值为所有 hash(t) 第k个bit的 众数 . 写成数学表达式很吓人, 其实只不过不断在{0,1}和{-1,+1}之间变而已, 总之就是对所有hash(t)的每一位进行统计, 如果1多就放1, 否则就放0... 关于为什么simHash可以满足近邻hash的条件(即两个文档jacccard sim越大, 其simhash相等的可能性越大), 不知道... 不过可以参考这个链接: http://matpalm.com/resemblance/simhash/ simHash VS minHash 下面来比较一下二者的差别. 首先是表示方式: simHash只需要直接拿term的集合即可使用 minHash需要首先建立字典, 然后用一个binary的向量(长度为字典长度)表示一个文档 其次是取值范围: simHash得到的hash范围取决于应用到每个term上的hash函数的范围, simHash与所有term的hash位数相同. minHash的范围等于字典的长度, 如果字典里有M个term那么minHash取值在1到M之间. 但是minHash也有优点: 要生成不同的simHash比较困难, 取决于应用在每个term上的hash函数有多少种. 生成不同的minHash非常容易: 每次shuffle就可以对一篇文章生成不同的minHash. 所以如果我们想要用多个hash来索引一个文章的时候, minHash可以很容易实现.","tags":"notes","title":"Approximate Retrieval(2): simHash"},{"url":"http://x-wei.github.io/bfs-summary.html","text":"今天总结一下广度优先搜索(BFS). BFS是树/图的遍历的常用算法之一, 对于没有边权重的图来说可以计算最短路径. 由于树的BFS只是图的BFS的一种特殊情况, 而且比较简单不需要visited标记, 这里只写一下图的BFS好了. 先定义一个Graph类, 这里在每一个节点保存邻居信息: public class GraphNode{ int val; List<GraphNode> neighbors; } BFS for trees/graphs 图的遍历需要注意不走重复节点, 所以需要一个HashSet(名字叫visited)来保存哪些节点已经访问过了. 需要注意的是, 在把一个节点放进队列queue的时刻就要把它放进visited , 而不是在队列里取出来的时刻再放. public void BFS(GraphNode start){ LinkedList<GraphNode> q = new LinkedList<GraphNode>(); HasheSet<GraphNode> visited = new HasheSet<GraphNode>(); q.push(start); visited.add(start); while(!q.empty()){ GraphNode cur = q.poll(); System.out.println(cur.val); for(GraphNode next: cur.children){ if(!visited.contains(next)){ q.push(next); visited.add(next); // mark node as visited when adding to queue! } } }//while } BFS with distance 在BFS的同时我们可以记录从start节点到当前node的距离, 方法是把一个距离信息同时入队(封装一个 Pair<GraphNode, Integer> ), 或者使用一个与queue 平行 的队列保存距离信息. 在上面的代码中, 加入: //... LinkedList<Integer> distq = new LinkedList<Integer>(); distq.push(0);// distance from start to start //... // in the while(!q.empty()) loop: int d = distq.poll();//get distance from start to current node for(GraphNode next: node.children){ distq.push(d+1);// distance from start to next node //... 对于Tree的情况来说, 这里的dist其实就是当前节点的深度depth. properties 性质1: 每个节点node的distance都是node距离起始点start的最短距离. 性质2: 距离start近的节点(depth浅的节点)一定比距离start远的节点早被访问到. 这是对一个树BFS的时候节点的访问顺序: BFS \"by layer\" 参考上面的性质, 可以一次处理\"一层\"的节点, \"一层\"的意思是指所有节点距离start的距离相同. 代码在while循环里不是一次poll一个节点, 而是一次把queue的内容处理完, 然后换新的queue进入下一次while循环. 代码重新写一下: public void BFS(GraphNode start){ ArrayList<GraphNode> q = new ArrayList<Tree>(); HasheSet<GraphNode> visited = new HasheSet<GraphNode>(); q.push(start); visited.add(start); while(!q.empty()){ ArrayList<GraphNode> newq = new ArrayList<Tree>();// create a new queue for(GraphNode cur: q){// deal with all nodes in the queue System.out.print(cur.val+\", \");// all nodes in q are of the same distance/depth for(GraphNode next: cur.children) if(!visited.contains(next)) { newq.push(next);visited.add(next); } } System.out.println(); q = newq;//replace q with newq }//while } 以上程序每次打印一行, 第i行包括了距start距离为i的所有节点. 由于这样的话每次不必在队首poll出元素(而是依次处理所有queue的元素), 所以可以改用ArrayList. 此时while循环里的不变量是: 所有q里面的节点距离start的距离都相同. complexity 假设一个图有N个节点和M条边, BFS会走遍所有节点, 时间是O(N), 然后由于每个节点会检查所有的出边, 最终所有的边都会被检查过, 时间是O(M), 所以BFS的时间复杂度是 O(N+M) . 队列里面最多可能存放所有节点, 空间复杂度为 O(N) .","tags":"tech","title":"广度优先搜索(BFS)小结"},{"url":"http://x-wei.github.io/algo-ds-mindmap.html","text":"Here is a mindmap of the common algorithms and data structures, it can give an overview of the algorithmic terms. I shall update its content later on. And maybe write some blog entries on some of the items. This mindmap is drawn using xmind .","tags":"tech","title":"Mindmap of algorithms & data structures"},{"url":"http://x-wei.github.io/minhash.html","text":"approximate retrieval (相似搜索)这个问题之前实习的时候就经常遇到: 如何快速在大量数据中如何找出相近的数据. 问题描述: 假设有N个数据, 并且对于他们有一个相似度(或距离)的度量函数 sim(i,j) , 我们的问题就是如何快速找出所有N个点中相似度较大的i和j组合. 乍一看这个问题必须要对所有的(i,j)计算相似度, 但是N&#94;2的复杂度在N太大的情况下是不能够忍受的. kdtree 之前在algo-note里面遇到过 kdtree , 用它可以使得寻找nearest neighbor的复杂度减少到logN. 但是这种情况对于维度低一点(比如二三维)的情况合适, 维度到了成千上万的时候并不是很好的选择, 所以这里不多讨论. simhash 另一个思路是, 使用某个hash函数, 对于每一个数据计算一个哈希值. 这个hash函数要满足: 当i和j的相似度很高的时候, hash(i)和hash(j)的值(很可能)相同. 这次介绍的minHash就是这样的一种方法. Jaccard similarity 明确问题含义, 首先需要定义相似度. 这里主要考虑文本相似度的问题, 假设字典D有M个term(term可以是单词, 也可以是n-gram或叫shingle): 一段文本(document i)可以用binary vectorization变为一个binary的向量: (这里没有用TF或者TFIDF, 只用一个简单的binary向量化, 因为只有binary的时候才适合我们接下来的推导...) 每个document可以看作一些term的 集合 , 集合之间的相似度有一个经典的度量: jaccard similarity. 对集合S1和S2, 他们的相似度定义为: 也很好理解, 重合部分比例越高相似度就越高, 另外jaccard-sim取值在0到1之间. 对于document i和j, 他们的向量形式分别是di和dj. 现在我们希望计算hash(di)和hash(dj), 使得: minHash min hash的思路是这样的, 首先生成一个随机的(1...M)的排序(permutation)π: 然后, 对于每个document d, 都按照这个permutation, 把d的分量从新排列: 然后定义 minHash(di) 为permutation以后的di里的第一个不为0的位置: (上面个公式里d的下标只是代表第i个文本, 并不代表分量... 我应该写上标的..) 所以 minHash() 返回1到M之间的一个数. proof 现在证明一下为什么这样选择minHash函数可以保证两个文本的哈希值相等的概率为他们的jaccard similarity. 对于d2和d2, 我们分别查看π(d1)和π(d2)的每个分量, 这两个数有(11), (10), (01), (00)这四种可能, 分别记每种可能性的出现次数为a,b,c,d: 那么jaccard similarity可以表示为: 再看 minHash() 是如何计算的, 当π(d1)_k和π(d2)_k都为0的时候会继续增加k, 一直到π(d1)_k和π(d2)_k中某一个为1. 那么 minHash(d1)==minHash(d2) 的情况就是二者都为1的情况, 这种情况的可能性为: 这个概率恰好就是jaccard similarity.","tags":"notes","title":"minHash: 一种快速approximate retrieval方法"},{"url":"http://x-wei.github.io/linux-swap.html","text":"当年装mint 17.1的时候心想我的电脑有4GB内存怎么可能不够用, 于是果断没有分一个swap分区, 呵呵... 然后就看到chrome这几年来吃内存越来越严重, 导致我开多几个标签页再开eclipse的话电脑就很有可能直接卡死... 郁闷啊 ! 于是决定给系统增加一个swap分区. 之前给 thealternative 写邮件询问这件事, Sandro Kalbermatter同学热情回复了我告诉我怎么做, 按照他说的果然成功了, 特此一记. step1. 建立swap分区 首先, 用gparted调整磁盘分区, 缩小一个磁盘的大小, 然后用空出来的空间新建一个swap分区. 关于swap分区应该多大, 根据 这个帖子 , 大约是内存的2-3倍, 不过我只是分了和内存一样大的4G空间, 感觉这样应该够用了(吧). 我是把存放文件的500G分区缩小, 这个过程会比较慢, 大概二十多分钟以后才结束: step2. 编辑fstab 建立好了swap分区以后, 打开终端, 输入 sudo blkid 查看所有的磁盘分区. $ sudo blkid /dev/sda1: LABEL = \"FILES\" UUID = \"ddefc0a7-30a1-42fb-a71a-0aebb55cb0b3\" TYPE = \"ext2\" /dev/sda2: UUID = \"952b70b9-c7ee-4adf-9ceb-ac631ed8d7eb\" TYPE = \"swap\" /dev/sdb1: LABEL = \"WIN7\" UUID = \"D268862068860407\" TYPE = \"ntfs\" /dev/sdb5: UUID = \"0ee51625-2fe3-49c8-b388-53076bce7955\" TYPE = \"ext4\" 然后看到了swap分区的UUID以后, 将其复制, 然后编辑fstab文件: sudo leafpad /etc/fstab : UUID=put-the-uud-here none swap defaults 0 0 然后重启, 就可以看到swap分区了! step3. 调整swapness swapness 是一个在0到100之间的数, 当系统的内存剩余不到百分之swapness的时候, 开始使用swap分区. 默认的系统swapness是60, 就是内存占用超过40%的时候就开始使用swap分区了. 使用 cat /proc/sys/vm/swappiness 可以查看当前的swapness. 由于内存读写要比swap分区快很多, 在内存足够的情况下完全没有必要使用swap, 于是修改swapness为20: sudo leafpaf /etc/sysctl.conf 在文件里加入一行: vm.swappiness = 10 重启即可...","tags":"soft","title":"linux系统添加swap分区&调整swapness"},{"url":"http://x-wei.github.io/linreg-bayes.html","text":"几乎所有的ml课都是从线性回归讲起, ETH的课也不例外. 不过这次老师用了贝叶斯的视角讲这个问题, 自从高中接触丁老师讲的线性回归以来 第一次听到一个不同于最小二乘的解读, 感觉很有意思. 又想起来刘未鹏那篇非常棒的 博客 , 于是想记录一下. notation 首先有n个数据点: 其中y是实数, 每个x有d个维度, 为了方便表示截距, 再给x加入一个始终等于1的维度: 例子: y代表房价, x代表了房子的面积, 使用时间, 距离市中心的距离等因素. least square viewpoint 在最小二乘的视角里, 线性回归是用一个x的线性函数拟合y: 使得拟合结果和观测结果的误差尽量小. 不过这次不说最小二乘, 所以接下来不讨论这个思路... assumptions in Bayes viewpoint 在贝叶斯视角里, 我们假设: 假设1. y = 某个x的线性函数 + 观测噪音 即: 其中εi是一个 随机变量 , 所以y也是一个随机变量. 另外再有一个比较强的假设: 假设2. ε服从centered高斯分布, iid. (btw, 对一个随机变量建模, 一般来说, 连续随机变量就用高斯, 离散随机变量用泊松) Bayes formula 贝叶斯公式长这个样子: 只看最左边和最右边的内容, 表达为: posterior = likelihood * prior 后验概率 = 可能性 * 先验概率 (上面其实应该是\"正比于\"而不是等号, 由于P(Y)我们并不关心, 所以可以直接忽略之) 公式里Y代表可以观察到结果, X代表结果背后不能直接观察的量( 不要和数据里的XY混淆... ). 贝叶斯公式的意义在于, 让我们从可观测的Y反推不可观测的X的概率. 既然我们已经得到了观测结果Y, 那么找到使得后验概率最大的X就说明我们在观测基础上得到了最可信的X的估计. 那么在我们这个问题里, X代表模型, 即某一个β的取值; Y代表观测结果, 即我们看到的n个数据点. 所以我们的问题就是: 在已经有了这些观测点的基础之上, 应该选那个β的取值, 使得后验概率最大? lin-reg = max-likelihood 线性回归认为, 对于任意的β的取值, 其先验概率都是一样的, 所以在贝叶斯公式里可以忽略ℙ(X), 只需要考虑最大化likelihood ℙ(Y|​X)即可 — 再一次, 不要把贝叶斯的XY和数据的X和Y混淆... 即选择β: 由于随机变量y只是随机变量ε的一个函数(且给定β, ε和y一一对应): 所以可以最大化ε的likelihood: 由于之前对ε有假设: 那么: (不要以为ε独立于β: ε=y - βt x, 所以上面表达式里其实还是有β的. ) ⇒ 两边取log并加上负号: Voila, 所以极大似然=最小二乘! 对, 饶了一圈还是最小二乘, 但是这样的意义变了, 明白了为什么要最小化平方误差这个值. precise solution to linreg (notation有点混乱了, 之前贝叶斯里面应该用AB而不是用XY的orz...) 每个x是一个列向量, 这里, 把所有数据用矩阵形式表示: 矩阵表示的好处是平方误差可以用矩阵表示: 对矩阵运算求偏导, 偏导为0的时候即可得到最优的β:","tags":"notes","title":"lin-reg = max-likelihood: 贝叶斯视角看线性回归"},{"url":"http://x-wei.github.io/list_array_matrix.html","text":"python科学计算包的基础是numpy, 里面的array类型经常遇到. 一开始可能把这个array和python内建的列表(list)混淆, 这里简单总结一下列表(list), 多维数组(np.ndarray)和矩阵(np.matrix)的区别. list列表 列表属于python的三种基本集合类型之一, 其他两种是元组(tuple)和字典(dict). tuple和list区别主要在于是不是mutable的. list和java里的数组不同之处在于, python的list可以包含任意类型的对象, 一个list里可以包含int, string或者其他任何对象, 另外list是可变长度的(list有 append , extend 和 pop 等方法). 所以, python内建的所谓\"列表\"其实是功能很强大的数组, 类比一下可以说它对应于java里面的 ArrayList<Object> . ndarray多维数组 ndarray是numpy的基石, 其实它更像一个java里面的标准数组: 所有元素有一个相同数据类型(dtype), 不过大小不是固定的. ndarray对于大计算量的性能非常好, 所以list要做运算的时候一定要先转为array( np.array(_a_list_) ). ndarray带有一些非常实用的 函数 , 列举几个常用的: sum, cumsum, argmax, reshape, T, ... ndarray有 fancy indexing , 非常实用, 比如: a[a>3] 返回数组里大于3的元素 ndarray之间的乘法: 如果用乘法运算符 * 的话, 返回的是每个位置元素相乘(类似matlab里面的 .* ), 想要矩阵相乘需要用 dot() . 常见矩阵的生成: ones, zeros, eye, diag, ... matrix矩阵 matrix是ndarray的子类 , 所以前面ndarray那些优点都保留了. 同时, matrix全部都是二维的, 并且加入了一些更符合直觉的函数, 比如对于matrix对象而言, 乘号运算符得到的是矩阵乘法的结果. 另外 mat.I 就是逆矩阵... 不过应用最多的还是ndarray类型. 参考资料: http://docs.scipy.org/doc/numpy/reference/index.html http://math.mad.free.fr/depot/numpy/base.html http://stackoverflow.com/questions/4151128/what-are-the-differences-between-numpy-arrays-and-matrices-which-one-should-i-u","tags":"tech","title":"numpy: list, array, matrix小结"},{"url":"http://x-wei.github.io/algoI_week6.html","text":"Can we do better than BST if we do not need ordered operations ? (No compare methods, use equals method) Idea: save items in an array. Hash function : method for calclulating the array index of a key. Issues: computing hash function equality tests collision resolution Classic space-time tradeoff. 1. Hash Functions Goal: scramble the keys uniformly to produce a table index. effcient to compute all indices are equally likely for any key challenge: need different approach for different key types. Java's hashing All java object has a methode int hashCode() requirement: if x.equals(y) ⇒ x.hashCode()==y.hashCode() (hopefully) if x.equals(y)==false ⇒ x.hashCode!=y.hashCode() Default implementation: memory address for x. Custom implementations for standard types: Integer, Double, String, File, URL, Date... Strings: Horner's method — compute a polynome. recipe for user-defined types: Modeular hasing (hash code VS hash function) Hash code : 32-bit integer between -2&#94;31 and 2&#94;31-1 (can be negative!!) Hash function : integer between 0 and M-1 (used directly as array index, should >=0) BUG code: private int hash(Key k){ return k.hashCode()%M; } → bug: number returned can be negative ! 1-in-billion bug code: return Math.abs(k.hashCode()) % M; → bug: Math.abs() returns negative numbers for -2&#94;31 correct code: just take the first 31 bits: x&0x7fffffff return k.hashCode()&0x7fffffff % M; 另一个hash的思路是用random(用hashcode作为随机种子) uniform hasing assumption each key is equally likely to hash to an integer between 0 and M-1 一些数学结论: 2. Separate Chaining one strategy for collision resolution. idea : Using a table of size M < N, build a list for each of the table positions. implementation public class SeparateChainingHashST<Key, Value>{ private int M = 97; private Node[] st = new Node[97]; private static class Node{ Object key, val; // no generic array creation! Node next; public Node(Key k, Value v, Node nxt){...} } int hash(Key k){ return (k.hashCode()&0x7fffffff) % M; } public Value get(Key k){ int h = hash(k); for(Node x=st[h]; x!=null; x=x.next) if(k.equals(x.val)) return (Value)x.val; //cast return null; } public void put(Key k, Value v){ int h = hash(k); for(Node x=st[h]; x!=null; x=x.next) if(k.equals(x.val)) {x.val = v; return;} st[h] = new Node(k,v,st[h]); } } 注意内部类Node里用的是Object — 因为不能声明generic array. analysis proposition Under the assumption of uniform hashing, the number of keys in a list is within a constant factor to N/M. proof. binominal distribution. M times faster than sequential search → typical choice: M ~ N/5 3. Linear Probing Another approch for collision resolution. idea: open addressing Use an array of size M>N , when a key collides, find next open slot . insert: when occupied, move pointer until got open slot search: when not found, move pointer until find or reach open slot delete : set key to null, then for all items behind this key: set to null, then insert this key...... array M must be larger than N ⇒ array resizing is necessary. implementation public class LinearProbingHashST<Key, Value>{ private int M, N; private Object[] keys;//cannot use generic array private Object[] vals; public void put(Key k, Value v){ int i = hash(k); for(;keys[i]!=null;i=(i+1)%M) if(keys[i].equals(k)) {vals[i]=v; return;} keys[i]=k; vals[i]=v; } public Value get(Key k){ int h=hash(k); for(;keys[h]!=null;h=(h+1)%M) if(k.equals(keys[h])) return (Value)vals[h];//ugly cast return null; } } cluster def. cluster A contiguous block of items. New keys are more likely to hash into a cluster. Knuth parking pb: (当年Knuth就是在证明了这个以后决定写那套书) Typical choice: N/M ~ 1/2 (3/2 for search hit and 5/2 for search miss) 4. Hash Table Context widely used in applicataions. cost for computing hash VS cost for searching: hashCode() for strings in java 1.1: exemain only 8-9 evenly spaced characters to save time. uniform hashing assumption: performance not guaranteed... one-way hash functions Hard to find a key that hash to a desired value, or 2 keys having the same hash value. used for fingurprints or store passwords, but too expensive for ST implementations. Seperate chaining VS linear probing variations ST choice: hash tables VS balanced search trees","tags":"notes","title":"[Algorithms I] Week 6 Hash Tables"},{"url":"http://x-wei.github.io/algoI_week5_2.html","text":"1. 1d Range Search Goal: intersections of geometric objects. Solution: BST 1d range search operations required: insert search delete range search : all keys between k1 and k2 range count : how many keys are between k1 and k2 → find points on an interval implementation by BST range count using the rank() function for the BST (or use the size of a tree) 注意什么时候要加1... public int size(Key hi, Key lo){ if(contains(hi)) return rank(hi)-rank(lo)+1; else return rank(hi) - rank(lo); } range search 类似inorder traversal的方式: find in left subtree (if could fall into range) check current node find in right subtree running time: R+lgN (R=nb of nodes in range) 2. Line Segment Intersection Orthognal line segment intersection search : find all intersections given N horizontal/vertical lines Non-degeneracy assumption: all x-coord and y-coord are distinct. naive algo: check all pairs... Sweep-line algorithm sweep a vertical line from left to right. when hit the left end of horizontal-segment (h-seg) → insert into a BST when hit the right end of a h-seg → delete from BST when hit a vertical-seg: ⇒ 1d range search ! 关于怎么sweep的: 没有仔细讲, 不过我觉得就是把所有的x坐标排好序, 有个 skyline问题 也是涉及如何sweep的. proposition running time is NlgN+R (R=nb of intersections). proof. Sort by x-coord (or use PQ) → NlgN insert/delete y-coord to BST → NlgN range search → NlgN + R 3. Kd-trees An extension of BST: 2d-keys . insert: insert 2d points search range search: find all keys lying in a 2d rectangle (h-v rectangle) range count gird implementation divide space into a M-by-M grid (uniform squares). space: N + M&#94;2 time: 1 + N/M&#94;2 → choose square to balance space and time. problem: points are not uniformly distributed. 2d tree Use a tree to represent the subdivision of the space. 2d tree : recursively divide the space into 2 halfplanes construct the 2d tree by adding points: alternating between horizontal and vertical partitioning for each level of tree . Data structure: BST alternating x and y-coords as key. Range search for 2d tree find all points lying in a rectangle. 依然类似tree traversal算法: check point in node find in left subtree (if could be in range — the rectangle intersects the splitting line) find in right subtree analysis Typical case: R + lgN worst case: R+ sqrt(N) (even if tree is balanced) (proof is hard) Nearest Neighbour seach find closest point to a query point. check dist from query point to node check in left tree ( if could contain a closer point — 和两点连线与splitting line的角度有关系 ) check in right tree analysis typical case: lgN worst case: N Flocking boids 3 simple rules to get a simulation of flocking. Kd tree partition the k-dim space into 2 halfspaces. cycle through k dimensions. (居然时一个本科生发现的!) Nbody simulation: treat clusters as an aggregated node 4. Interval search tree 1d interval search: data are intervals insert interval search interval delete interval intersection query: find all intervals that intersects (lo,hi) Nondegeneracy assumption: all left endpoint of intervals are distinct. API: put(Key lo, Key hi, Value val) get(Key lo, Key hi) delete(Key lo, Key hi) Iterable<Key> intersects(Key lo, Key hi) Interval search tree: BST using left endpoint as key in each node: store the max right endpoint of the subtree insert 类似BST, 加上维护一下maxendpoint即可. search search any one interval that intersects (lo,hi) if node intersects, return if left.maxendpoint < lo: go right else: go left *proof. * 主要证明一点: if no intersection to left ⇒ then no intersection to the right 5. Rectangle intersection Goal: find all intersection among N rectangles. (non degeneracy assumption: all x and y are distinct) bottom line: linearithmic algo. sweep-line algorithm: sweep vertical line from left to right. when hit left part of a rect ⇒ put into an interval search tree when hit right part of a rect ⇒ remove interval every time befor adding ⇒ check intersection reduces the 2d rect intersection pb to 1d interval search pb. complexity: NlgN+RlgN summery:","tags":"notes","title":"[Algorithms I] Week 5-2 Geometric Applications of BSTs"},{"url":"http://x-wei.github.io/algoI_week5_1.html","text":"goal: lgN for insert/search/delete operations (not necessarily binary trees..) 3 algo: 2-3 tree, (left leaning) red-black tree, B-tree 1. 2-3 Search Trees def. 2-3 tree allow 1 or 2 keys per node, & 2 or 3 children per node: 2-node: one key, 2 children (ordinary BST node) 3-node: 2 keys, 3 children (3 children: less, between, more ) perfect balance : every path from root to null link has the same length (2-3 tree的一个超好的性质, 类似于一个满二叉树 !) symmetric order: inorder traversal gives ascending order (和BST类似) search Just follow the correct link... Natural generalization of search in BST... insert case 1: insert into a 2-node at bottom just convert a 2-node into a 3-node case 2: insert into a 3-node at bottom create a temporary 4-node (three keys) move middle key in 4-node into parent, split the rest two keys into two 2-nodes if parent becom a 3-node → continue the process if arrived at the root (root is a 4-node with three keys): split it into three 2-nodes splitting a 4-node : can be done in constant time ( local transformation ). Analysis Invariant: maintains symmetric order and perfect balance. proof. each transformation maintains the order and the balance, all possible transformations: 这个图很好, 3-node的插入一共有三种情况: 自身是root/父亲是2-node/父亲是3-node performance every path from root to null link has the same length. Implementation direct implementation is complicated: bottom line: Could do it, but there's a better way. 2. Red-Black BST LLRB tree: left-leaning red-black tree. BST representation of the 2-3 trees use internal left-leaning links for 3 nodes 红色link即为internal left leaning link (红黑树就是这么来的), 用红色link连接起来的组成一个(虚拟的)3-node 或4-node. 3-node用一个red link表示: 4-node用两个red link表示: ⇒ or or example: properties no node has two red links (不可以一个节点连两个red link) every path from path to null link has the same number of black links (想象所有red link都变为horizontal) all red links lean left representation Each node has only one link from parent ⇒ add a boolean to encode color of links (the color of the link from parent ). private class Node{ private Key key; private Value val; Node left, right; boolean color;//true means red } private boolean isRed(Node nd){ if (nd==null) return false; return nd.color; } insert to parent 操作: 只需把color变为RED即表示该节点 被变成了和父节点一起的一个(虚拟)节点. elementary operations left-rotation (def: convert a right-learning red link to left. ) (symmetric ordering and perfect black balance are maintained) private Node rotateLeft(Node h){ Node s = h.right; h.right = s.left; s.left = h; s.color = h.color; // not = BLACK h.color = RED; return s; } right-rotation (temporarily turn a left-leaning red link to right) private Node rotateRight(Node h){...} right rotation 是为了应对这种情况: rotateRight(c) ⇒ color-flip (split a 4-node, with three kyes — two red links) private void filpColor(Node h){ h.color = RED; h.left.color = BLACK; h.right.color = BLACK; } Implementation Basic strategy Maintain one-to-one correspondence with 2-3 tree by applying elementary operations. search Exactly the same as elementary BST. ( ⇒ The same code for floor and ceiling) insert Each insert will generate a red link (then should rotate to make it legal) 插入的时候有两种可能: insert into a 2-node at the bottom standart BST insert if have red right link: rotateLeft ex: insert into a 3-node 有三种可能: insert into left/middle/right, right最简单, left捎复杂, middle最复杂, 见下图: standard BST insert and color nodes if necessary, rotate to balance 4-node, 比如: flip colors to pass red link to upper level if necessary, rotate to make all links left-leaning ex: ex2: 视频最后一段的demo太帅了! 叹为观止!! Code 原来 只有4种(其实是3种)情况要调整 : left = black, right = red ⇒ rotateLeft(a) left =red, left.right = red [这个不会出现, 因为这对于下一层来说是case 1..] ⇒ rotateLeft(e) ⇒ 变为case 3 left = red, left.left = red ⇒ rotateRight(s) ⇒ 变为case 4 left = red. right = red ⇒ flipColor(r) 几个状态之间的转化: 只要三行代码即可处理LLRB tree !! 老爷子牛逼...... (这个也是在2007年algo第四版的时候才刚刚弄出来的, 以前的代码要复杂) private Node put(Node nd, Key k, Value v){ if(nd==null) return new Node(k,v,RED); int cmp = k.compareTo(nd.key); if(cmp==0) nd.val = v; // 这里不急着返回 -- same trick as for BSTs.. else if(cmp<0) nd.left = put(nd.left, k, v); else nd.right = put(nd.right, k, v); // modifications to maintain LLRB tree property: if( isRed(nd.right) && !isRed(nd.left) ) nd = rotateLeft(nd);//case 1 //if( isRed(nd.left) && isRed(nd.left.right) ) nd.left = rotateLeft(nd.left);// case 2 -- never happen... if( isRed(nd.left) && isRed(nd.left.right) ) nd = rotateRight(nd);// case 3 if( isRed(nd.left) && isRed(nd.right) ) flipColor(nd);//case 4 return nd; } 这三行代码越看越精妙...... Analysis worst case : the left path is alternating red and black . ⇒ longest path <= 2 * shortest path (height<= 2lgN) practical applications: height ~ 1.0 lgN summery: 3. B-trees setting: data access in file system. Probe is much expensive than accessing data within a page. Goal: access data using a minimum number of probes. B-tree def. external nodes: contain just keys, not links internal nodes: contain key-link pairs def. B-tree Generalize 2-3 trees by allowing up to M-1 keys per node: = 2 keys in root = M/2 keys in other nodes external nodes contain client keys internal nodes contain copies of keys to guide search Searching similar to BST/2-3tree ex. (Choose M as large as possible so that M links fit into a page) Insertion similar to 2-3 tree Analysis System implementations system implementations of RBtree. java: java.util.TreeMap , java.util.TreeSet . 八卦1: 八卦2: Sedgewick 的朋友, Philippe Flajolet , 是一个X!","tags":"notes","title":"[Algorithms I] Week 5-1 Balanced Search Trees"},{"url":"http://x-wei.github.io/algoI_week4_2b.html","text":"(BST是锻炼递归代码的好题目) 1. Binary Search Trees def. BST A binary tree where each node has a key: for every node, the key is larger than all nodes in left subtree, smaller than all nodes in right subtree. Fields: key, val, left, right Implementation An inner class of BST nodes: private class Node{ private Key key; private Value val; private Node left, right; public Node(Key k, Value v){...} } skeleton implementation of BST: public class BST<Key implements Comparable<Key>, Value>{ private Node root; private class Node{...} public Value get(Key k){...} public void put(Key k, Value v){} public void delete(Key k){} public Iterable<Key> iterator(){} } search recursive version: (或者把这个函数写到Node类里面也可以. ) private Value get(Node nd, Key k){ if(nd==null) return null; // search miss int cmp = k.compareTo(nd.key); if(cmp==0) return nd.val; // search hit else if (cmp>0) return get(nd.right, k); else return get(nd.left, k); } non-recursive version: public Value get(Key k){ Node nd=root; while(root!=null){ int cmp = k.compareTo(nd.key); if (cmp==0) return nd.val; else if(cmp>0) nd = nd.right; else nd = nd.left; } return null; } insert recursive version: (注意这个recursive函数的返回值不是void! 这里是一个trick: 返回的是在分叉以前的那个节点) private Node put(Node nd, Key k, Value v){ if(nd==null) return new Node(k, v); int cmp = k.compareTo(nd.key); if(cmp==0) nd.val = v; else if(cmp>0) nd.right = put(nd.right, k, v); else nd.left = put(nd.left, k, v); return nd; } non-recursive version: 不如递归版本优美... public void put(Key k, Value v){ Node nd = root; while(true){ int cmp = k.compareTo(nd.key); if(cmp==0) { nd.val = v; break; } else if(cmp>0){ if(nd.right!=null) nd = nd.right; else {nd.right = new Node(k,v); break;} } else if (nd.left!=null) { if(nd.left!=null) nd = nd.left; else {nd.left = new Node(k,v); break;} } } } Analysis complexity: depth of the BST. shape of BST: depends on how the keys come in (order of insertion). if keys come in random order: could be pretty well balanced. BST and quick-sort partitionning The root of BST is just the pivot in quick sort partitioning * if all keys are distinct ⇒ one-to-one correspondence between quick sort and BST. ⇒ proposition if all keys are distinct and come in randome order, the average number of compares for a search/insert is ~2lnN (or 1.39lgN). proof.* 证明见quicksort那里的数学推导... proposition (Reed, 2003) N distinct keys come in random order, average tree height = 4.300lnN Worst-case: The tree becomes just like a linked list: ~N for insertion and search 2. Oredered Operations in BST task: ordered opeartions min()/max() : min/max key deleteMin()/deleteMax() floor(Key k)/ceiling(Key k) : largest key <=k / smallest key >=k rank(Key k) : nb of keys < key select(int i) : key with rank=i Iterator<Key> keys(lo, hi) : iterates through [lo, hi] min/max easy min: left-most max: right-most floor/ceiling a little more complexed... floor (ceiling is similar) if k==nd.key return nd.val if k<nd.key the floor must be in the left subtree if k>nd.key 如果min(nd.right) > k: 返回nd.val 如果min(nd.right) <= k: go to right public Value floor(Node nd, Key k){// largest element with key <= k int cmp = k.compareTo(nd.key); if(cmp==0) return nd.val;//case 1 else if(cmp<0) return floor(nd.left, k);//case 2 if (nd.right==null || min(nd.right).compareTo(k)>0) //case 3 return nd.val; else return floor(nd.right); } 他提供的版本和我写的不一样: 递归函数floor返回的也是一个Node: rank/select In each node, store the number of nodes in the subtree: add an extra field . size private class Node{ private int count; //... } public int size(){ return size(root); } public int size(Node nd){ if(nd==null) return 0;// this is why we do not put size() inside the class Node! return nd.count; } public void put(Node nd, Key k, Value v){ //..... nd.count = size(nd.left)+size(nd.right)+1;//maintain count for each node return nd; } rank (return nb of keys < k) if nd.key==k return size(nd.left) if nd.key>k return rank(nd.left, k) if nd.key<k return size(nd.left)+1+rank(nd,right, k) private int rank(Node nd, Key k){ if(nd==null) return 0;//remember null case int cmp = k.compareTo(nd.key); if(cmp==0) return size(nd.left) else if (cmp<0) return rank(nd.left, k); else return size(nd.left)+1+rank(nd.right,k); } select() similar... iteration Inorder traversal 中序遍历 public Iterable<Key> keys(){ Queue<Key> q = new Queue<Key>(); inorder(root, q); return q; } private void inorder(Node nd, Queue<Key> q){ if(nd==null) return; inorder(nd.left); q.enqueue(nd.key); inorder(nd.right); } property inorder-traversal gives the keys in ascending order . (proof by induction) 3. Deletions in BST one final function to implement: delete(Key k), deleteMin(), deleteMax() → and remember to update the count field... (感觉这篇文章其实就讲的很清楚了: http://www.algolist.net/Data_structures/Binary_search_tree/Removal 这个在递归函数里使用了parent这个参数) lazy approch put(k, null) , and leave the key in the tree (tombstone) → not good if have large number of tombstons... deleteMin/Max go the the left-most node → replace it with its right node. Recusive function with the returning-node trick : private Node deleteMin(Node nd){ if(nd==null) return null; // this might not happen if(nd.left==null) return nd.right; else nd.left = deleteMin(nd.left); nd.count = size(nd.left)+1+size(right);//remember to maintain the count field return nd; } 这个递归的技巧又一次使用了. Hibbard deletion first find node with the key to delete, 3 cases: 0 children: simply set parent link to null 1 child: replace parent link with the child 2 children (most subtle) first replace node key with smallest key in right subtree remove the smallest key in right subtree code of Hibbard deletion Again (for the 3rd time) use the return-nd trick ... private Node delete(Node nd, Key k){ if(nd==null) return null;// search miss int cmp = k.compareTo(nd.key); if(cmp>0) nd.right = delete(nd.right, k); else if(cmp<0) nd.left = delete(nd.left,k); else{ //if nd is the node to delete if(nd.left==null) return nd.right; if(nd.right==null) return nd.left; Key k2 = min(nd.right); nd.key = k2; nd.right = delete(nd.right, k2); } nd.count = size(nd.left)+1+size(nd.right); return nd; } public void delete(Key k){ root = delete(root, k); } 感觉用了recursive return-nd 这个trick的实现很漂亮.... 比那篇博客里放一个参数进递归函数以及用auxroot的办法要好不少... Analysis problem: not symmetric If random insert and delete for a while ⇒ tree become much less balanced ! Tree height tend to be sqrt(N). summery BST is much better in average case, but not guaranteed for worst case.","tags":"notes","title":"[Algorithms I] Week 4-2b Binary Search Trees"},{"url":"http://x-wei.github.io/algoI_week4_2a.html","text":"1. Symbol Table API key-value pair abstraction insert a value with a key given a key, search for its value Association array abstraction Associate a value to a key — generalized array: a[key]=val . public class ST<Key, Value>{ void put(Key k, Value v);//remove key if value=null Value get(Key k);//return null if key is absent void delete(Key k); boolean contains(Key k); boolean isEmpty(); int size(); Iterable<Key> keys();//better to return an ordered sequence of keys } conventions: values are not null get() returns null if key not present put() can overwrite older value → some one-line implementations: contains: return get(k)!=null; delete: put(k, null); Assume keys to be comparable: class ST<Key implements Comparable<Key>, Value> — can thus use compareTo() method. Else → we can only use the equals() method... Be careful when implementing the equals method: 坑不少... 2. Elementary implementations naive implementations using unordered linked list ListNode{key, value, next} search: scan through all keys ~N insert: scan through, if not found, add to front ~N using ordered array using 2 arrays: keys[] (sorted), vals[] ⇒ can improve performance by binary search search operation write a function rank() that returns the number of keys < k searched. 找不到的时候: 比k小的元素个数=lo (lo>hi, 可以想想当hi=lo以后是怎么移动的) private int rank(Key k){ int lo=0, hi=keys.length-1; while(hi>=lo){ int mid = lo + (hi-lo)/2; int cmp = keys[mid].compareTo(keys[k]); if(cmp==0) return mid; else if(cmp>0) hi = mid-1; else lo = mid+1; } return lo; } Using rank() to implement the get() method: public Value get(Key k){ int rk = rank(k); if(rk<N && keys[rk].compareTo(k)==0) return vals[rk]; return null; } insert operation Like insertion sort, time complexity is ~N for each insert. summery: 3. Ordered Opeartions When keys are comparable ⇒ provide more functionalities in the API. for example: min()/max() : min/max key deleteMin()/deleteMax() floor(Key k)/ceiling(Key k) : largest key <=k / smallest key >=k rank(Key k) : nb of keys < key select(int i) : key with rank=i Iterator<Key> keys(lo, hi) : iterates through [lo, hi]","tags":"notes","title":"[Algorithms I] Week 4-2a Elementry Symbol Tables"},{"url":"http://x-wei.github.io/algoI_week4_1.html","text":"1. API and elementary implementations Collection: data struct for inserting and deleting items (ex. stack and queue). Priority queue: a special kind of collection — remove largest/smallest element. API: public class Max<Kye implements Comparable<Key>>{ public MaxPQ(); public void insert(Key k); public Key delMax(); public boolean isEmpty(); public Key max(); int size(); } PQ client example find M largest elements from N items. (N is too huge to store) ⇒ 思路: 用 Min PQ , 当size>M时删掉最小元素 — 最后剩下的就是最大的M个元素了. elementary(naive) implementations unordered array implementation insert ~1, remove ~N ordered array implementation insert ~N — ~ insertion sort, remove ~1 goal : insert O(lgN) & remove max O(lgN) 2. Binary Heaps binary heap: a special kind of complete binary tree . def. \" complete binary tree\" All level except the last level are full, all nodes in the last level are as far left as possible. 完全二叉树大概长这个样子: property: a complete binary tree with N items has height = lgN. def. binary heap Binary heap is a complete binary tree that satisfies: each node >= any of its 2 children (\"heap ordering\"). property of binary heap: max element is root height is lgN each node still makes a heap (use this property to construct heap from bottom up) Array representation of binary heap Use an array a[] to represent a complete binary tree: very easy to get children and parent. index starting from 1 (root = a[1], do not use a[0]) for node at index i , its children are: i*2 and i*2+1 for node at index i (i>1), its parent is: i/2 Implementation 简言之就是: 内部成员变量: public class MaxPQ<Key implements Comparable<Key>>{ private Key[] a; private int N; } siftup (swim) if a node is larger than its parent: exchange with parent until heap ordering is restored. private void swim(int k){ while(k>1 && less(a[k/2], a[k]){ exch(a,k,k/2); k /= 2; } } ⇒ use siftup for inserting elements when inserting an element, first append it to the end of the array, then siftup the element to its right position. public void insert(Key k){ a[N++]=x; swim(N); } siftdown (sink) if a node is smaller than its children: exchange it with the bigger child. 用公司高层变动来类比很形象... private void sink(int k){ while(k*2<=N){ int maxIndex = less(a[k],a[k*2])? k*2 : k; if(k*2+1<=N && less(a[maxIndex],a[k*2+1]) maxIndex = 2*k+1; if(maxIndex==k) break; exch(a,k,maxIndex); k = maxIndex; } } ⇒ use siftdown for deleting max operation First put a[N] to the position of root, then sink. (注意要防止loitering) public Key delMax(){ Key ret = a[1]; a[1] = a[N]; a[N--] = null; // avoid loitering sink(1); return ret; } Analysis insert: ~lgN remove max: ~lgN improvements: use a d-way heap instead of a 2-way heap (for ex. 3-way heap, the children of i are i 3, i 3+1, i*3+2, the parent of i is i/3) Fibonacci heap: insert is ~1 , remove max still ~lgN, (but too complicated to use in practice). considerations: use immutable keys to avoid client from changing the keys ( final keyword in java) underflow/overflow: throw exceptions for delMax() when empty, use resizing array for implementation. expand the API with other operations: remove an arbitrary item, change the prority of an item (like handling an array)... 3. Heapsort Basic idea: create a max heap with all N entries (\"heap-construction\") then repeatedly remove all N items (\"sortdown\") Implementation heap construction Construct the heap using a bottom-up method: build the heaps with small size first. one-node heaps (jus leaf node of heap): need not consider larger heaps: children(subtrees) are already heap-order → just perform sink operation for root ⇒ sink elements at indices from N/2 to 1 for(int k=N/2; k>=1; k++) sink(k); → complexity is linear , see analysis below. heap destruction now that we have a max-heap, to get the sorted array in-place, simply do: while(N>1){ exch(a,1, N--); sink(1); } Analysis proposition Heap construction uses <2N compares and exchanges. proof 一个有h层的heap, 高度为k的subtree有 2&#94;(h-k)个, 每个subtree最多可以交换k次(从root交换到最底层) → 每一层最多有 k*2&#94;(h-k) 次交换 (第一个等式可以用数学归纳法证...) 详细证明见http://algs4.cs.princeton.edu/24pq/ (Q20答案) proposition Heapsort use < 2NlgN compares and exchanges. Heapsort is the first in-place sorting algorithm with NlgN worst-case performance. ⇒ heapsort is optimal for time and space , but: inner loop is longer than quicksort poor use of cache memory (too many references to cache — look far away from the array entry when going down through the tree) not stable: because it does long-distance exchanges Summary: 4. Event-driven simulation Goal: simulating the motion of N particles in elastic collision (using a priority queue). model bouncing balls (without collision) Ball class: challenge: which objects to check? How to do it efficently ? Time driven simulation upate the position of balls every dt seconds if overlap detected: figure how the velocity change and continue the simulation. problem with time-driven simulation: 1/2*N&#94;2 overlap check per time unit if dt small: too much calculation if dt too large: might miss collisions Event driven simulation change state only when something happens particles move in straight line before collision → collision time can be predicted maintain PQ for collisions, key=collision time collision prediction input: each particle has radius s , position (rx, ry) , velocity (vx, vy) . (美国高中物理这么凶残?...) Implementation anyway, 这个是封装好的类: 注意, 使用了count记录一个particle到目前为止的碰撞次数. initialization: quadratic Fill PQ with all potential particle-particle collisions Fill (the same) PQ with all potential particle-wall collisions Main loop get next event from PQ, event time = t ignore if the event is invalidated advance all particles to time t update velocities predict future collisions and insert to PQ 判断invalidate: 一个event构造(predict)时记录粒子构造时刻的碰撞次数(countA, countB), 然后在调用invalidate的时候, 如果粒子的碰撞次数发生了改变, 则返回false. (详见他们的代码: http://algs4.cs.princeton.edu/61event/CollisionSystem.java.html ) Event class:","tags":"notes","title":"[Algorithms I] Week 4-1 Priority Queue"},{"url":"http://x-wei.github.io/algoI_week3_2.html","text":"(maybe best algorithm for sorting.) 1. Quicksort Idea: shuffle the array Partition the array into two subarrays to left and right of pivot (*now pivot is *in its final position ) no larger entry to the left of pivot no smaller entry to the right of pivot sort each subarray recursively Implemetation The partition process: 这个方法也比较巧妙. Use 2 pointers i and j (个人觉得用hi, lo, pivot更好...) : → a[i]>=a[lo], a[j]< = a[lo] (注意是大于 等于 /小于 等于 ) ⇒ exchange i and j → Scan until i and j cross (ie. j<=i) ⇒ finally exchange lo with j 函数的签名定义的好: 把lo到hi部分的数组分成两部分, 并返回分割点的index. private static int partition(Comparable[] a, int lo, int hi){ int i=lo+1, j=hi; while(i<j){ while( i<=hi && less(a[i],a[lo]) ) i++; //a[i]>=a[lo] while( j>=lo && less(a[lo],a[j]) ) j--; //a[j]<=a[lo] if(i<j) exch(a,i++,j--); } exch(a,lo,j); //exchange pivot with j return j; //j in its final position } 这个函数其实并不好写对: test for cross pointers is not trival (ex. edge case: the pivot is the smallest/largest entry in the range) i<=hi is necessary ! for keys equal to a[lo]: better to stop at them invariance: Quicksort: 使用partition函数和辅助sort函数(recursive). 注意在整个流程开始以前先shuffle一下. private static void sort(Comparable[] a, int lo, int hi){ if(hi<=lo) return; int pivot = partition(a, lo, hi); sort(a,lo,pivot-1); sort(a,pivot+1,hi); return; } public static void sort(Comparable[] a){ StdRandom.shuffle(a); // don't forget to shuffle the array sort(a,0,a.length-1); } The randomness is preserved: the subarrays after partitionning is still randomly ordered. Analysis Performance: ~40% faster than mergesort. Best case compares = NlgN (each partition will divide the array in half) Worst case compares = 1/2*N&#94;2 N+(N-1)+...+1 if the array is already in order, each partition will have one subarray of length=0 Average case proposition On average, for array with N distinct keys, the #compares = ~2NlnN, #exchanges = ~1/2 NlnN. Proof.* C(N) := # compares for N entries pivot 在N个数离的排名是uniform的 接下来的数学推到很漂亮(不过可能没啥用..) (上面最后一行写错了... 是2NlnN...orz) random shuffle: probalistic guarantee against worst case. Pitfalls implementations will get quadratic performance if array: is sorted or reverse sorted has many duplicates (even if randomized) Staility Quicksort is NOT stable. partitionning can make long range exchanges Practical improvements cutoff to insertion sort for <10 items → ~20% improvement Or we can leave the small subarrays unsorted and sort them at last using insertion sort estimate median by sampling 3 items → 10% improvement 2. Selection Goal: given un array, find the kth largest item. Upper bound for this problem: NlgN (just sort the array) for small k (ex k=1,2,3), the upper bound is N (one-pass/two-pass) Lower bound is N: at least have to look at everything Quick select Algo proposed also by Hoare: partition the array into two arrays left of pivot and right of pivot. if pivot==k: return continue the partition for just one of the subarrays 类似于二分查找的过程.... 注意这里 是不用递归的 ! 因为partition函数返回的直接就是pivot 在整个数组里的 位置! Implementation privater static int partition(Comparable[] a, int lo, int hi){...} public static Comparable select(Comparable[] a, int k){ StdRandom.shuffle(a); int lo=0,hi=a.length-1; while(true){ int j = partition(a,lo,hi); if(j<k) lo=j+1; if(j==k) return a[j]; else hi=j-1; if(hi<=lo) break; } return a[k];//这里不太理解为什么会在hi<lo的时候直接返回a[k] } Analysis Proposition Quick selection takes linear time on average. *proof * intuitively, each partition will ct the subarray size in half: N+N/2+N/4+... = 2N formal analysis 略... worst case : quadratic (but very rare to happen) Theoretical results 3. Duplicate keys if array contains many duplicate keys. huge array small number of distinct keys for mergesort insensitive... always ~NlgN compares. for quicksort Will get quadratic time if not stop on equal keys. (found in 1990s) mistake: put all items equal to pivot *to just one side * → N&#94;2 compares if all keys are equal from lo to hi. correct: put all items equal to pivot in their final place. 3-way partitionning (by Dijkstra) partition the array into 3 parts: Dijkstra's 3-way partition algo: 使用3个指针: lt指向中间部分的左边界, gt指向右边界; i指针从左向右扫描, 算法很subtle: lt=lo, gt=hi, i=lo if a[i]==v : i++ if a[i]<v: exch(i,lt), i++, lt++ if a[i]>v: exch(i,gt), gt-- repeat until i and gt cross (i>gt) invariance : [lo, lt) all < v [lt, i) all == v (gt, hi] all >v Implementation: 3-way quick sort 不必再写partition函数, 直接在sort递归函数里面. private static void sort(Comparable[] a, int lo, int hi){ if(hi<=lo) return; // 递归函数别忘了先写终止条件... int lt=lo, gt = hi; Comparable v = a[lo]; for(int i=lo;i<=gt;){ //不能写 i++ if( less(a[i],v) ) exch(a, i++, lt++); else if ( less(v,a[i]) ) exch(a,i,gt--); else // v==a[i] i++; } sort(a, lo, lt-1); sort(a, gt+1, hi); } 当N个数有很多重复的时候, lower bound可以变小于NlgN: And Sedgewick proved that the 3-wy partition is propotional to the lower bound.... 4. System Sorts Arrays.sort() in java: import java.util.Arrays; quicksort for primitive arrays, mergesort for objects: java设计者认为如果用obj array表示空间不是问题... Pb in java's system sort: killer input exsit (havn't shuffle)... 总结一下学过的5/6种排序:","tags":"notes","title":"[Algorithms I] Week 3-2 Quicksort"},{"url":"http://x-wei.github.io/algoI_week3_1.html","text":"Two classical sorting algorithms: mergesort, quicksort. 1. Mergesort Divide and conquer: top 10 algorithms of the 20th century, invented by von Neumann. Idea : divide array into 2 halves recursively sort each half merge two sorted halves Implementation Merge : Goal: a[lo] to a[mid] and a[mid+1] to a[hi] are sorted ⇒ get a[lo] to a[hi] sorted. → use an auxiliary array to copy data : using 3 indices i,j,k. private static void merge(Comparable[]a, Comparable[] aux, int hi, int mid, int lo ){ for(int k=lo;k<=hi;k++) aux[k]=a[k]; int i=lo, j=mid+1, k=lo; while(i<=mid && j<=hi){ if(less(aux[j],aux[i])) a[k++] = aux[j++]; else a[k++] = aux[i++]; } while(i<=mid) a[k++] = aux[i++]; while(j<=hi) a[k++] = aux[j++]; } note: we use if(less(aux[j],aux[i])) instead of if(less(aux[j],aux[i])) , because less(a,b) == true iff a<b (strict), and we want to make mergesort stable . See section (5) below. Assertion We can also add assertions : assert isSorted(a, lo, mid); assert isSorted(a,mid+1, hi); Enable/disable assertion at runtime: java -ea MyProgram //enable assertion java -da MyProgram //disable assertion: default Best practice: use assertions to check interval invariants; do NOT use assert for external argument checking ! Mergesort 为了mergesort需要写两个辅助函数: merge和sort(recursive): public class MergeSort extends AbstractSort{ public static void sort(Comparable[] a){ Comparable[] aux = new Comparable[a.length]; sort(a,aux,0,a.length-1); } private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi){ if(hi<=lo) return; int mid = (hi+lo)/2; sort(a,aux,mid+1,hi); sort(a,aux,lo,mid); merge(a,aux,hi,mid,lo); } private static void merge(Comparable[]a, Comparable[] aux, int hi, int mid, int lo ){ // as before... } } 有几点注意的: 注意递归终止条件是 hi<=lo , 不是 == . 实现的代码不难, 但是定义好辅助函数的参数并不简单... 比如在merge里使用了mid作为参数. aux的数组直接写在了函数参数里面, 好处是可以防止反复声明数组带来的开销. 发现他们写的时候mid都是写成: mid = lo+(hi-lo)/2 , 查了一下 发现这样的原因是为了防止hi+lo整数溢出...嗯有道理. Analysis proposition (time) Mergesort takes at most NlgN compares and 6NlgN array access. Proof. def: C(N)=#compares for N elements, A(N)=#array access for N elements, the recurrence eq: C(N) <= 2 C(N/2) + N; C(1) = 0 (最多N次比较: 每个a[]的数都由比较得到) A(N) <= 2 A(N/2) + 6N; A(1) = 0 (~~这里不太理解为什么是6N, 怎么数都是4N或者5N啊?......~~2N复制到aux, 2N复制回去, 2N比较) (上图蓝色的一列是extra cost) 或者用递推公式, 发现D(N)/N的递推关系: 或者用数学归纳法: Proposition(memory) Mergesort takes N extra memory. ⇒ mergesort is NOT in-place sorting method. def . \"in-place\" A sorting algo is in-place if it uses <C*lgN extra memory. ex. insertion sort, shellsort, selection sort... Practical improvements use insertion sort for small arrays: cutoff to insertion sort for <7 items. ⇒ lead to 20% improvement! stop if alread sorted: *a[mid]<a[mid+1]! * switch the role of a[] and aux[] 2. Bottom-up Mergesort bottom-up version of mergesort without recursion . Idea: pass and merge subarrays of size 1 in pairs repeat for subarrays of size 2,4,8,.... implementation 看着上面那个图好写一点... 就是每次用更大的size来两两merge一遍数组 public static void sort(Comparable[] a){ Comparable[] aux = new Comparable[a.length]; for(int sz=1;sz<a.length;sz*=2){ for(int i=0;i<a.length-sz;i+=sz*2) merge( a,aux,i,i+sz-1,Math.min(i+sz*2-1) );//<-注意mid和hi的计算方法 } } 3. Sorting Complexity some items: computational model: allowed operations, (ex. decision tree for compare based sorting ). cost model: operation counts. upper bound: cost guarantee (ex. NlgN for mergesort). . lower bound: limit on cost guarantee for all algorithms (no algorithm can do better). Optimal algorithm: algo with best cost guarantee. (upper bound=lower bound) 证明lower bound 的方法很有意思: lower bound for sorting (binary) decision tree for the case of 3 distinct items: each possible ordering is a leaf of the decision tree → there are N! possible orderings → tree height should be lg(N!) = NlgN (cf. https://en.wikipedia.org/wiki/Stirling%27s_approximation ) proposition. Any compare-based sorting algorithm must have at least lg(N!)~NlgN compares in the worst case. (for N distinct keys) proof. binary tree of height h has at most 2&#94;h leaves N! possible orderings → at least N! leaves So, lower bound for sorting = ~NlgN ⇒ mergesort is an asymptotical optimal algorithm . (In terms of time complexity, the shortcoming of mergesort is always the extra space usage) 但是并不是说NlgN是 所有 排序问题的下界: 4. Comparators Java mechenism for comparing same data on different ways. Comparable interface →Sorting using natural order for a data type. public class Data implements Comparable { public int compareTo(Date that){ //...} } Comparator interface →Sorting using an alternative order . (total order property is required...) public interface Comparator<Key>{ int compare(Key v, Key w); } Comparator can be passed as arguments in java system sort: Using Comparators in our sorting algos Use another signature: change Comparable to Object add Comparator in arguments public static void sort(Object[] a, Comparator comparator); public static boolean less(Comparator c, Object v, Object w); public static void exch(Object[] a, int i, int j); Implementing a comparator Add static comparator to a class: In the declaration of a class, define an inner class that implements the Comparator interface, Then declare an instance of this inner class as a static final variable... note: the inner class should be static also. 以上的方法目的是为某个类提供预先定义好的comparator(比如 String.CASE_INSENSITIVE_ORDER ), 另外也可以直接再声明一个类作为comparator, 然后实例化这个类传进去, 就像之前做过的那样. Application: Graham scan algo for convex hull We can get the result of compare by calling ccw(a,b,c) : *ccw(p,q1,q2)=true ⇔ q2>q1 in terms of polar angle wrt p. (别忘了p是y坐标最小的点, 否则还要讨论角坐标为负数的情况, 见下图). * 5. Stability Typical application: first sort by student name then by section. def. :stable\" A stable sort preserves the relative order for items with equal keys. Insertion sort and mergesort are stable, whereas selection sort and shell sort are not. (And always need to carefully check code: \"less than\" vs. \"less than or equal to\"). insertion sort: stable proof: we never move equal items pass each other: if(less(a[j], a[j-1])) exch(...) selection sort: not stable counter example: when exchanging A1 and B1, we move B1 behind B2 shell sort: counter example: long-dist exchanges merge sort: stable proof: suffices to verify that merge operation is stable, if keys are equal, always take element from left subarray.","tags":"notes","title":"[Algorithms I] Week 3-1 Mergesort"},{"url":"http://x-wei.github.io/algoI_week2_2.html","text":"1. Introduction rearanging array of size N into ascending order test client code: Insertion.sort(a); sort any datatype callback callback = reference to executable code i.e. passing functions as argument to sort() method sort() function calls object's compareTo() method → implement the Comparable interface: public class XX implements Comparable<XX>{ ... } the interface: public interface Comparable<Item>{ public int compareTo(Item that); } compareTo(): return -1 (if this<that)/+1/0; needs a total order. → in the sort() implementation: has not dependencies on type of data. public static void sort(Comparable[] a){ if(a[i].compareTo(a[j])>0)... } helper functions less private static boolean less(Comparable v, Comparable u){ returnv.compareTo(u)<0; } exch private void exch(Comparable[] a, int i, int j) { Comparable swap = a[i]; a[i] = a[j]; a[j] = swap; } isSorted test if sorted if algo passes the test using only less ant swap, then it's correct. 2. Selection Sort Idea: each time : find the minimum from the remaining items. a[min] is the smallest element to right of a[i] ⇒ swap a[i] and a[min] (elements to left of i are sorted) invariants entries to the left of i are in sorted order, and are fixed ( in final position ) ever since no entry to the right of i is smaller than any entry to the left of i implementation public class SelectionSort extends AbstractSorting{ //... public static void sort(Comparable[] a){ for(int i = 0; i<a.length; i++){ int min = i; for(int j = i+1; j<a.length; j++) if(less(a[j],a[min])) min = j; exch(a,min,i); } } } analysis proposition: selection sort uses N-1 + N-2 + ... + 1 = ~N&#94;2/2 compares, and N exchanges. → quadratic time insensitive to input: quadratic time even if input is already sorted. data movement is minimum : linear time of exchanges (every exchange puts an item to its final position) 3. Insertion sort quite different performance characteritics than selection sort. Idea: In iteration i: move all entries larger than a[i] to its left. invariants entries to the left of i are in ascending order (but not in final position) entries to the right of i are not yet been seen implementation publc class InsertionSorting extends AbstractSorting{ public static void sort(Comparable[] a){ for(int i=1; i<a.length; i++) for(int j=i; j>0; j--){ if(less(a[j],a[j-1])) exch(a,j,j-1); else break; } } //... } analysis proposition (average case): (the performance on average — for randomly sorted array ) proof: expect each entry to move halfway back best case and worst case best case if array already sorted min ascending order: N-1 compares, 0 exchanges. worst case if array sorted in descending order: every element goes all the way back → 1/2 N&#94;2 compares, 1/2 N&#94;2 exchanges partially sorted arrays def. \" inversion\" an inversion is a pair of entries that are out of order. def. \"partially sorted\" An array is called partially sorted if the number of inversions is <= c N. * proposition. Insertion sort runs in linear time for partially sorted array. proof. number of exchanges = number of inversions. number of compares = number of exchanges + N-1 4. Shell Sort First non-trival sorting methode: an improvement of insertion sort. def. \"h-sorted array\" an array is h-sorted if every h-interleaved subarray is sorted. (h=1: just a sorted array) Idea: move entries >1 position at a time by h-sorting the array, then decrease h. use decreasing sequences of value h: implementation How to h -sort simply insertion sort with stride length=h . why insertion sort: for big h: small subarray for small h: nearly in order proposition A g-sorted array remains g-sorted after h-sorting it. (subtle to prove...) which sequence of h to use 3x+1 sequence proposed by Knuth. 1,4,13,40.... public class ShellSort extends AbstractSort{ public static void sort(Comparable[] a){ int h = 1, N=a.length; while(h<N/3) h = h*3+1;//find the beginning h (N>h>N/3) while(h>=1){//performs h-sort for(int i= h;i<N;i+=h) for(int j = i;j-h>=0;j-=h) if( less(a[j],a[j-h]) ) exch(a,j,j-h) else break; h = h/3; } } //... private static boolean isHsorted(Comparable[] a, int h) { for (int i = h; i < a.length; i++) if (less(a[i], a[i-h])) return false; return true; } } 每次hsort, 外围的循环是 for(int i= h;i<N;i+=h) , 需要理解一下: i移动一次以后, 进行的是另一个subarray 的插入排序, 当移动到N-1的时候所有subarray的插入排序才结束. (也就是说不是先完成一个subarray的插入排序再完成另一个, 这些是插入排序是同步进行的) analysis proposition (for worst case ) → better than quadratic time ! property (found in practice) of compares < Cte * N * (# of h used ) → #compares < NlgN * Cte *accurate model has not been discovered * (所以shellsort在实际使用中几乎和快速排序一样快! — 尽管没有数学证明来保证) why we are interested in shell sort useful in practice: fast for medium sized arrays (beat even the classical sophistiated algorithms) tiny code volumn (used in embeded systems) lead to interesting questions for 50 years: asymptotic growth rate ? best sequence of h ? average case performance ? 5. shuffling shuffle array using sort one way to shuffle an array: for each array entry, generate a random real number sort the array of real numbers ⇒ the original array is shuffled ! proposition this shuffle sort produces a uniformly random permutation of input array drawback: cost for sorting... Goal: get uniformly random permutation in linear time . Knuth shuffle algo: for i = [0,N): - r = rand( [0~i ] ) or rand( [ i, N-1] ) - swap a[r] and a[i] implementation: public static void shuffle(Object[] a){ for(int i=0;i<a.length;i++){ int r = StdRandom.uniform(i+1); exch(a,r,i); } } proposition Knuth algo produces an uniformly random permutation of input array. proof. Sufficient to prove that, for card i and position j, the proba(card i comes to position j) = 1/N. if i<=j, P = 1/j * j/(j+1) * (j+1)/(j+2) * ... * (N-1)/N if j<i, P = 1/i * i/(i+1) * (i+1)/(i+2) * ... * (N-1)/N CQFD. example: online poker https://www.cigital.com/papers/download/developer_gambling.php ←那个扑克网站已经被黑出翔了... bugs: r never get 52 (52th card never moved) r = rand(N) instead of rand(0~i), → shuffle not uniform random() uses 32bit seed: only 2&#94;32 possible shuffles, 2&#94;32<52! seed = millisec from midnight, ~86*10&#94;6 suffles 6. Convex Hull application of sorting for the field of computational geometry. convex hull smallest polygoneenclosing all N points. input: N points output: sequence of vertices in counterclockwise ( ccw ) order. application: robot motion planning; farest pair. geometric properties : can traverse convex hull by making only ccw turns let p be the point with lowest y-coord, wrt p, vertices appear in increasing order of polar angle. Algo Graham scan algorithm: * choose p with smallest y coord * sort points by polar angle with p * consider points in order (stack is used), discard unless creates a ccw turn . CCW given three points a b c, returns if a→b→c is a CCW turn. (assumption: no 3 points on a line) ⇒ calculate cross product of ab and bc ⇒ determinants! area>0 ⇔ CCW implementation public class Point2D{ private double x,y; public static boolean ccw(Point2D a,Point2D b,Point2D c){ double area2 = (b.x-a.x)*(c.y-a.y) - (b.y-a.y)*(c.x-a.x); return area2>0; } } convex hull: public static Stack<Point2D> GrahamScan(Point2D[] p){ //* assumes that points are sorted by polar angle in p[] Stack<Point2D> hull = new Stack<Point2D>(); hull.push(p[0]); hull.push(p[1]); for(int i=2;i<p.length;i++){ Point2D b = hull.pop(), a = hull.peek(), c = p[i]; while(!Point2D.ccw(a,b,c)){ b = hull.pop(); a = hull.peek(); } //now a,b,c makes a ccw turn: hull.push(b); hull.push(c); } } running time: NlgN for sorting and linear for the rest.","tags":"notes","title":"[Algorithms I] Week 2-2 Elementary Sorts"},{"url":"http://x-wei.github.io/algoI_week2_1.html","text":"fundamental data types: stacks and queues operations: insert, remove, test empy, iterate, ... module programming: seperate interface and implementation 1. Stacks ex. a stack of strings API: public interface StackoOfStrings{ void push(String item); String pop(); boolean isEmpty(); //int size(); } implementation 1: using a linkedlist insert/remove from the top of the linkedlist inner class class ListNode { String item ; ListNode next ; } implementation public class LinkedStackOfStrings implements StackoOfStrings{ class Node{ String item; Node next; Node(String item, ListNode nxt){...} } private Node first; public LinkedStackOfStrings(){ first = null; } public void push(String item){ Node nd = new Node(item,first); first = nd; } public String pop(){ String firstItem = first.item; first = first.next; return firstItem; } public boolean isEmpty(){ return first==null; } } complexity: const time for every operation array implementation use array (of length N) to store items → defect: stack has limited capacity keep a pointer *top: *pointing to the next empty space to push (top 的定义很重要) problems of the array implementation: underflow: pop from an empty stack overflow: size larger than capacity ⇒ resizing loitering : holding a ref to an obj which is no longer needed: ex. return s[top--] java system will not know that s[top] is no longer needed ⇒ have to clear it explicitely ⇒ String item = s[top--]; s[top]=null; return item implementation (containg resizing array operations) public class ArrayStackOfStrings implements StackOfStrings{ private String[] s; private int top=0; public ArrayStackOfStrings(){// to be tuned s = new String[1];//initial capacity=1 } public boolean isEmpty(){ return top==0; } private vois resize(int capacity){//helper functoin String[] s2 = new String[capacity]; for(int i=0;i<top;i++) s2 = s[i]; s = s2; } public void push(String item){ if(top==s.length)//doubling size resize(s.lenth*2); s[top++]=item; } public String pop(){ String item = s[--top];//NOT top--! s[top]=null; if(top>0 && top==s.length/4) //top>0 is necessary resize(s.length/2); return item; } } 2. Resizing Arrays resolving the overflow pb: grow and shrink the array → need to copy all items when changing array size ⇒ pb: ensure that sizing happens infrequently resizing strategy repeated doubling: (initial capacity=1) when array is full, double the size amortized complexity for inserting N: N+(2+4+8+...+N) ~3N shrinking array ⇒ shrink the array by half when array is 1/4 full not half full → thrashing will happen if push-pop-push-pop when array is full [invariant] : array always 20%~100% full complexity: in an amortized sense , will be constant proposition : from empty stack, M operations of push/pop taked time propotional to M comparison: resizable array vs linkedlist linkedlist implementation: operations takes const time even in worst time extra time and space for dealing with linkes resizing array implementation: operation taked const amortized time but in worst case takes linear time (ex. to be evited for critical systems) less wasted space 3. Queues FIFO data structure API public interface QueueOfStrings{ void enqueue(String item); String dequeue(); boolean isEmpty(); //int size(); } linked list implementation maintain first and last node pointers: pointing to 2 points of queue ( first for dequeue, last for enqueue ) → take care of corner cases : - empty queue: first is null (and last is also null) - just one item in queue: first and last point to the same node (总之first和last的定义很重要) public class LinkedQueueOfStrings implements QueueOfStrings{ class Node{... } private Node first,last; public LinkedQueueOfStrings(){ first = null; } public void enqueue(String item){//same as push Node nd = new Node(item,null); if(isEmpty()){ last = nd; first = last; } else{ last.next = nd; last = nd; } } public String dequeue(){//same as pop in stack String firstItem = first.item; first = first.next; if(isEmpty()) last=null; return firstItem; } public boolean isEmpty(){ return first==null; } } resizing array implementation maintain head and tail : head is the queue head, tail is the next empty position for the next element to enqueue → trick: head and tail should take mod capacity + resizing array 不知道写的对不对: public class ArrayQueueOfStrings implements QueueOfStrings{ private String[] q; private head=0,tail=0; public LinkedQueueOfStrings(){ q = new String[1];//init capacity } public boolean isEmpty(){ return head==tail; } private void resize(newsz){ q2 = new String[newsz]; int i = head,j=0; while(i!=tail){ q2[j++] = q[i]; i=(i+1)%q.length; } q = q2; head=0; tail=j; } public void enqueue(String item){ if( (tail+1)%q.length==head ) resize(q.length*2); q[tail] = item; tail = (tail+1)%q.length; } public String dequeue(){ String firstItem = q[head]; head = (head+1)%q.length; int sz = (tail-head)%q.length; if(sz>0 && sz==q.length/4) resize(s.length/2); return firstItem; } } 4. Generics queues/stacks for other types of data ⇒ generics 泛型 (java 1.5 才引进泛型机制...) use type paramater → avoid casting, and discover type mismatch errors at compile time public interface Stack<Item>{ public void push(Item item); public Item pop(); public boolean isEmpty(); } a pb with array implementation java不支持创立泛型数组 generic array creation is not allowed. 不可以new 一个泛型数组! s = new Item[capacity]; 会报错 ⇒ use an ugly cast : s = (Item[]) new Object[capacity]; (will get warning: \"unchecked cast\" → java被黑了... ) autoboxing for primitive types each primitive type has a wrapper class ex. int ↔ Integer autoboxing: automatic cast between a primitive type and its wrapper class. (syntactic sugar 语法糖 i.e. 对语言功能没有影响只是方便使用) btw: https://zh.wikipedia.org/wiki/%E8%AF%AD%E6%B3%95%E7%B3%96 (居然还有语法盐和语法糖精......) 5. Iterators Interface support iteration over stacks and queues, without revealing the internal representation of stack/queue ⇒ implement the Iterable interface Iterable interface: can return an *Iterator * public interface Iterable{ Iterator<Item> iterator(); } Iterator interface: hasNext() and next() interface public interface Iterator<Item>{ boolean hasNext(); Item next(); void remove();//optional, bad practice to use it } to make a data structure Interable → elegant client code how-to: implement Iterable interface write a private inner class XXIterator that implment the Iterator interface. ex. Bag data structure Supports adding and iterating through without caring about the order. API: public class<Item> Bag implements Iterable<Item>{ public void add(Item); int size(); } can be implemented by stack or queue(without pop/dequeue) 6. Applications Java collections library List interface: java.util.List implementations: ArrayList , LinkedList . pb with the java's implementation of stacks and queues: Stack class also implements List interface ( get() , remove() , contains() are implemented); Queue is an interface rather than a class... ⇒ poorly designed API Stacks applications function calls: recursion: can always use an explicit stack to remove recursion arithemic evaluation (Dijkstra) 四种类型: 左括号, 右括号, 数字, 算子 最后一行应该是value stack. ⇒ 后缀表达式, 逆波兰式......","tags":"notes","title":"[Algorithms I] Week 2-1 Stacks and Queues"},{"url":"http://x-wei.github.io/algoI_week1_lab.html","text":"model & problem (原文描述太啰嗦了) A system using an N-by-N grid of sites. → Each site is either open or blocked. → A full site is an open site that can be connected to an open site in the top row via a chain of neighboring open sites. (这个full的定义有玄机 而且导致后面写程序时有个问题, 看论坛想了半天才想出来, 见后文.) → We say the system percolates if there is a path of connected open sites form the top row to the bottom row. ⇒ pb: if sites are independently set to be open with probability p , what is the probability that the system percolates? → When N is sufficiently large, there is a threshold value **p such that when p < p a random N-by-N grid almost never percolates, and when p > p , a random N-by-N grid almost always percolates. → No mathematical solution for determining the percolation threshold p has yet been derived. ⇒ Your task is to *write a computer program to estimate p . Method API: public class Percolation { public Percolation(int N) // create N-by-N grid, with all sites blocked public void open(int i, int j) // open site (row i, column j) if it is not open already public boolean isOpen(int i, int j) // is site (row i, column j) open? public boolean isFull(int i, int j) // is site (row i, column j) full? public boolean percolates() // does the system percolate? public static void main(String[] args // test client (optional) } Corner cases: the row and column indices i and j are integers between 1 and N. 1≤i,j≤N if i/j out of range: java.lang.IndexOutOfBoundsException if N<=0 in constructor: java.lang.IllegalArgumentException Performance requirements: N2 for constructor, const for other operations Monte Carlo simulation all sites init to be closed → randomly choose a blocked site (i,j) and open it → repeat until percolates ⇒ the fraction of opened sites is an estimation of p* ex. 20*20 grid, when percolated: ⇒ estimated p* = 204/400=0.51 repeat the estimation for T times, get T estimations → get mean and std: → 95% 置信区间: create API for this simulation: public class PercolationStats { public PercolationStats(int N, int T) // perform T independent experiments on an N-by-N grid public double mean() // sample mean of percolation threshold public double stddev() // sample standard deviation of percolation threshold public double confidenceLo() // low endpoint of 95% confidence interval public double confidenceHi() // high endpoint of 95% confidence interval public static void main(String[] args) // test client (described below) } -if N ≤ 0 or T ≤ 0: java.lang.IllegalArgumentException - main() : takes two command-line arguments N and T ⇒ performs T independent computational experiments on an N-by-N grid, and prints out the mean, standard deviation, and the 95% confidence interval for p*. (Use standard random from our standard libraries to generate random numbers; use standard statistics to compute the sample mean and standard deviation. Here is the algo API: http://algs4.cs.princeton.edu/code/index.php ) Code 注意一定要用它们提供的那些库, 否则自己写的话代码就长了.... shuffle, mean, stddev什么的直接用他们的函数库就可以做到. http://algs4.cs.princeton.edu/code/index.php 另外UF也是用他们写好的, WeightedQuickUnionUF. 按照提示, 除了格子的N&#94;2个节点以外再增加两个节点: 顶部和底部的虚拟节点. 这里写的时候注意一开始也是不恩能够把它们与第一行/最后一行相连的 — 要在一个格子open以后再相连. backwash问题 这次题目有一点比较困难就是, 需要实现isFull()函数, 这个函数判断一个格子(i,j)是否和顶部相连. 这里如果直接用UF的connected()判断是否和顶部虚拟节点相连的话是有问题的, 如下图: 白色格子表示格子是open的, 蓝色格子表示格子是open并且是 full 的(i.e. 和顶部相连的), 左边图片里的状态是对的, 右边图片里底下部分的格子状态则不对: 如左下角的格子, 其实是没有和顶部联通的, 如果我们用两个虚拟节点的话, 由于底部虚拟节点和顶部虚拟节点相连, 所以和底部虚拟节点相连的左下角部分就被判断成了full的. 这个问题一开始我以为可以很简单解决, 后来发现没那么容易... (注意题目还要求isFull()也要在常数时间给出结果). 一个不优雅的办法是, 建立两个UF, 一个用来判断percolation, 另一个UF里没有底部虚拟节点所以可以专门用来判断isFull(). 这样解决的话使可以通过测试, 不过非常不好看, 另外一个UF的内存占用是8N&#94;2(内部有size[]和id[]两个int数组), 比较大. 在论坛上找了半天, 看了一些人的分享终于想到了这个非常妙的办法: UF只建立顶部虚拟节点, 不建立底部虚拟节点. 判断isFull只需要用UF的connected()一下就好了 问题是怎么判断percolation: a. 建立一个数组 boolean connectedToBottom[] , 指示某一点是否和底部相连 b. trick在这里: 不必修改一个联通分支的所有点的 connectedToBottom 的值, 只需要修改联通分支的root(UF的find)即可 . 在进行union的时候先查看两个component的root是不是连到底部, 然后有一个连到底部的话, 在union以后把合并后的联通分支的 connectedToBottom 状态改为true即可 c. 然后判断percolate: 先找到顶部虚拟节点锁在component的root, 然后看这个root是否连到底部即可! 这样用一个boolean数组(N&#94;2内存)代替了一个新的UF(8N&#94;2内存), 而且实现也更加优雅. 非常有意思的练习...","tags":"notes","title":"[Algorithms I] Week1-Lab: Percolation"},{"url":"http://x-wei.github.io/algoI_week1_2.html","text":"1. Introduction 2. Observations ex. 3-SUM pb given N distinct numbers, how many triples sum up to 0? (pb related to computatioal geogtry) brute force method: for(int i=0;i<N;i++) for(int j=i+1;j<N;j++) for(int k=j+1;k<N;k++) {if(a[i]+a[j]+a[k]==0) count++; } mesuring running time: stdlib.jar里面提供了一个 Stopwatch 类用于记录运行时间. log-log plot T(N) = running time for input of size N log(N)-log(T(N)) plot: often get a straight line — power law doubling ratio : (for checking the power law relationship, checking the power order) each time double the size of input, then take log of the time ratio of 2 runs: log( T(2N)/T(N) ) 3.Mathematical Models total running time: sum of cost*frequency of operations cost of some basic operations: array allocation: c*N (because all array entries have to be set to 0/false/null) string concatenation: c*N (proportional to the length of string !) simplification crude analysis ignore lower terms tilde notation estimating discrete sum by relaxation Replace the sum with an integral, and use calculus — 很机智... 4. Order of Growth Classification (discard the leading coefficient when considering the growth order) only a small set of growth functions: 1, logN, N, NlogN, N&#94;2, N&#94;3, 2&#94;N exemples: binary search ⇒ logN divide and conquer ⇒ NlogN exhaustive search ⇒ 2&#94;N practical performance: ex. binary search public int binearch(int arr[], int key){//arr[] already sorted int lo=0,hi=arr.length; while(i<j){ int m = (lo+hi)/2; if(arr[m]==key) return m; else if(arr[m]<key) lo=m+1; else hi=m-1; } return -1; } (→ Bug in Java's Arrays.binarySearch() discovered in 2006......) → invariant: if key in arr, arr[lo]<=key<=arr[hi] proposition. binary search uses at most logN+1 compares to search a sorted array of size N. pf. denote T(N) := nb of compares for array with size <=N → T(1)=1 → recurrence relation: T(N)<=T(N/2)+1 ⇒ T(N)=logN a faster 3-SUM → first sort the array (~NlogN) → for any pair a[i] and a[j], do binary search for -(a[i]+a[j]) ~(N2LogN) ⇒ reduce from N3 to N2logN ! (for 8k numbers, running time goes from 51s to 0.96s) 5. Theory of Algorithms types of analysis -best case -worst case -average case(random input, \"expected cost\") notations big Theta/big O/big Omega - big O: upper bound → * once a specific algo is found, find an upper bound - big Omega: lower bound → proove that no algo can do better - big Theta: symptotic growth (same order, optimal algo) → lower and upper bound match* ⇒ in this course: use tilde notation: contain leading constants for highest order term 6. Memory KB: 2&#94;10 bytes MB: 2&#94;20 bytes (1 million) GB: 2&#94;30 bytes (1 billion) 64-bit machines: 8 byte pointers typical memory usage: for primary types: for arrays (with array overhead=24bytes ) : Obj overhead: 16 bytes (obj的大小=16+obj内部filed的大小) references : 8 bytes (ex. inner class has a ref to encolsing class) padding : each obj uses a multiply of 8 bytes (obj大小=8 bytes的整数倍)","tags":"notes","title":"[Algorithms I] Week 1-2 Analysis of Algorithms"},{"url":"http://x-wei.github.io/sparkmooc_notelab4.html","text":"Part 0: Preliminaries Each line in the ratings dataset ( ratings.dat.gz ) is formatted as: UserID::MovieID::Rating::Timestamp ⇒ tuples of (UserID, MovieID, Rating) in ratingsRDD Each line in the movies ( movies.dat ) dataset is formatted as: MovieID::Title::Genres ⇒ tuples of (MovieID, Title) in ratingsRDD 487650 ratings and 3883 movies ⇒ Since the key is an integer and the value is a unicode string, we can use a function to combine them into a single unicode string (e.g., unicode('%.3f' % key) + ' ' + value ) before sorting the RDD using sortBy() . Part 1: Basic Recommendations naive method: always recommend the movies with the highest average rating... ⇒ 20 movies with the highest average rating and more than 500 reviews movieNameWithAvgRatingsRDD : (avgRating, Title, nbRatings) Part 2: Collaborative Filtering MLlib: https://spark.apache.org/mllib/ Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. 一图胜千言: Matrix Factorization CF问题实际上是矩阵分解的问题: We have a matrix whose entries are movie ratings by users (shown in red in the diagram below). Each column represents a user (shown in green) and each row represents a particular movie (shown in blue). 其中 rating矩阵 (用户/电影矩阵)只有一些项的值存在(即用户打分的那些项), 所以要用分解后的两个矩阵之乘积来估计rating矩阵中的缺失项. With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue). 若N个用户, M个电影 ⇒ 把rating矩阵(N M)分解为 一个N d矩阵( user矩阵 )与一个d M( movie矩阵*)矩阵之积. 其中d个维度可以有(隐含的)意义: 比如f[j]第一个维度代表了电影j中动作片的成分, f[i]的第一个维度表示用户i对动作片的喜爱程度, 以此类推... 所以f[i]与f[j]的内积就可以是用户i对电影j的评分的一个不错的预测. 假设 f[j]已知 , 那么f[i]要满足: 对那些用户i已经打过分的电影(即r_ij存在)上的估计偏差最小: (后面加上的那一项是正则项: 不希望f[i]的模过大) 不过前面的假设, \"f[j]已知\"这个条件其实并不成立 ⇒ Alternating Least Squares algorithm : 交替优化f[i]和f[j]的取值, 每次固定一个, 而优化另一个, 交替进行, 直到收敛(好像Kmeans也是利用的这种方法). first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized. Then, it holds the movies matrix constrant and optimizes the value of the user's matrix. train-test-validation split ⇒ break up the ratingsRDD dataset into three pieces: A training set (RDD), which we will use to train models A validation set (RDD), which we will use to choose the best model A test set (RDD), which we will use for our experiments trainingRDD, validationRDD, testRDD = ratingsRDD.randomSplit([6, 2, 2], seed=0L) Root Mean Square Error (RMSE) compute the sum of squared error given predictedRDD and actualRDD RDDs. Both RDDs consist of tuples of the form (UserID, MovieID, Rating) alternating least square of MLllib https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS ALS takes a training dataset (RDD) and several parameters that control the model creation process. The most important parameter to ALS.train() is the rank , which is the number of rows in the Users matrix (green in the diagram above) or the number of columns in the Movies matrix (blue in the diagram above). (In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to overfitting.) 貌似ALS接受一个(userid, itemid, rating)的RDD作为输入, 预测时接受一个(userid, itemid)的RDD作为输入, 返回一个(userid, itemid, rating)的RDD. (也就是说, 前面的notation在这里继续被使用了). model = ALS.train(trainingRDD, rank, seed=seed, iterations=iterations, lambda_=regularizationParameter) predictedRatingsRDD = model.predictAll(validationForPredictRDD) 可以在这里查看job详情: http://localhost:4040/jobs/ compare model Looking at the RMSE for the results predicted by the model versus the values in the test set is one way to evalute the quality of our model. Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set. ⇒ 这里没有太理解, 难道是说test set 的平均rating预测结果和training set的平均rating应该比较接近么?? ⇒ 终于明白了: \" Your model more accurately predicts the ratings than using just the average rating, as the model's RMSE is significantly lower than the RMSE when using the average rating. \" 求一个tuple rdd最后一列的和的时候, 需要先map成最后一列再reduce: trainingRDD.map(lambda x:x[-1]).reduce(lambda x,y:x+y) 直接写 reduce(lambda x,y:x[-1]+y[-1]) 貌似是不行的","tags":"notes","title":"[Spark MOOC note] lab4. Predicting Movie Ratings"},{"url":"http://x-wei.github.io/algoI_week1_1.html","text":"1. Dynamic Connectivity pb pb statement a set of N obj, indexed by 0,1,...,N-1 ⇒ UNION: connect objects void union(int p, int q) FIND: is there a path connecting 2 obj? boolean connected(int p, int q) ex: connect components (联通分支): max set of obj that are mutually connected. UF API union(p,q): connect 2 obj connected(p,q): test if p and q are connected find(p) : find the component id of p count(): nb of components 注意: 命名不是很好, 这里的 find() 函数不对应FIND query, connected() 函数才是真正的FIND query, find()函数是为了connected()函数而做的一个辅助函数(find(p): return the root of the node p ) 在connected里就可以调用find: return find(p)==find(q) 应该是interface更好一些... public interface UF{ void union(int p, int q); boolean connected(int p, int q); //int find(int p); //int count(); } 测试client: 2. Quick Find \"eager approach\" data structure ⇒ an int array id[] initialized to id[p]=p for all p interpretation: id[p] = component id of obj p ⇒ p and q are connected iff* id[p]==id[q] (ie. find very fast) 没有用find()函数 UF operations FIND: connected(p,q): very fast, just check id[p] and id[q] UNION: when merging 2 components : union(p,q) : id[p]=id[q] (总是让第一个参数p的id变为第二个参数q的id), ⇒ then have to modify all entries with id equal to id[p] ! ⇒ too many entries to change implementation (class QuickFindUF implements UF) complexity: FIND: cte UNION: lin... if N obj + N unions ⇒ quad time ! btw, 程序运行速度: ~10&#94;9/s 3. Quick Union \"lazy approach\" data structure ⇒ also an int array id[] considering a set of trees, 此时每个联通分支都是一个tree interpretation: id[p] = parent index of obj p (觉得这个数组叫做father更好....) ⇒ p is a root node iff id[p]==p UF operations FIND: connected(p,q): check if root of p == root of q UNION: union(p,q): just set p's root to be child of q's root (把第一个参数p的那棵树放入第二个参数q的树的根节点作为子树) root(): 前两个的操作都需要一个函数查找一个节点的root, 需要写一个函数实现, 也很简单, 一路找parent即可: private int root(int p){ while (p!=id[p]) p=id[p]; return p; } implementation complexity in the worst case (all elements is in a list form), root() is ~N, so: FIND: lin UNION: lin quick find和quick union的问题: 4. Quick Union Improvements improvement1: weighting keep track of tree size ⇒ balance by taking the small tree be a child of the large tree ⇒ add an extra array: sz[] sz[i] is the size of the tree with root i implementation (数组 int sz[] 初始全部为1) 依然需要 root() 函数. private int root(int p){ while(p!=id[p]) p=id[p]; return p; } FIND public boolean connected(int p, int q){ return root(p)==root(q); } UNION public void union(int p, int q){ int rp = root(p), rq=root(q); if(rp==rq) return; // if (sz[rp]<sz[rq]){ id[rp]=rq; sz[rq]+=sz[rp]; } else{...} } complexity FIND: proportional to depth of p and q in their tree UNION: const if p and q are root proposition the max depth of weightedQuickUnion is lgN [pf] considering a node x , in tree T1 , dep(x) is x's depth in its tree. → dep(x) will increase by 1, iff T1 is merged into another tree T2 (and by the algo, shoud have |T1|<=|T2| ) → x's tree's size become |T1|+|T2| >= 2 |T1| ⇒ everytime dep(x) increased by 1, x's tree's size will at least double* at first dep(x)=1, if dep(x) increases lgN times, the size of the tree will be >= N CQFD. so the root() function takes only lgN time. conclusion : both UNION and FIND will be in lgN time. improvement 2: path compression imporve the root() function: when looking for root of a node ⇒ link all nodes in the path up to the root. ⇒ just a constant extra time compared to old implementation. 2 pass implementation: private int root(int p){ int r = p; while(r!=id[r]) r=id[r]; while(p!=r){ int t = p; p=id[p]; id[t]=r; } return r; } flatens the tree greatly. single pass implementation: just make all other node point to its grandparent (halving the path length) ⇒ not as flatening as before, but in practice will almost be the same. just one extra line of code: private int root(int p){ while(p!=id[p]){ id[p] = id[ id[p] ]; p=id[p]; } return p; } complexity (for weighet quick union with path compression — WQUPC ) very very small: lg*() function: \" iterated log function \", lg (N) = the number of time to take log to get to 1 lg ()几乎可以看成常数了: ex. lg (65536) = 4* (x&#94;16=65536) because: lg(65536)=16 ; lg(16) = 4; lg(4)=2; lg(2)=1. ⇒ N obj, M unions will take (almost) linear time (有人证明了不存在 理论上 linear的算法. ) conclusion : both UNION and FIND will be in constant time . summery 上面这个表格好像quick union的部分有问题? 最坏情况下应该是N+MN吧?? 书上是这么写的: WQUCF reduce 30 years to 6 seconds. 5. Union Find Application percolation dynamic connectivity Kruskal MST algo Games (GO) ....... percolation model: N N grid of sites ⇒ each site is open with proba= p* ⇒ sys percolate iff bottom and top are connected by open sites. question: the percolation probability as a function of p ( phase transition ) nobody knows how to get the threshold mathematically ⇒ run simulations to find out the phase transition threshold . Monte Carlo simulation → all sites initilized to be closed → randomly open sites one by one → when the sys percolates, the vacancy percentage is an estimate of p *(run above simulation for millions of times) implementation N&#94;2 sites, named 0 to N&#94;2-1 add 2 more vertual sites: one on top, one on bottom openning a site: union to adjcent open sites (at most 4 unions)","tags":"notes","title":"[Algorithms I] Week 1-1 Union-Find"},{"url":"http://x-wei.github.io/sparkmooc_note_lec8.html","text":"STATISTICS, BUSINESS QUESTIONS, AND LEARNING TECHNIQUES 2 different kinds of statistics: descriptive statistics ex. median — describes data, but cannot generalize beyong that inferential statistics ex. t-testing — inferences beyond the data techniques leveraged for machine learning and prediction supervised learning (clf, reg), unsupervised learning (clustering, dim-reduction) → UL often used in a larger SL pb (ex. auto-encoder ) EXPLORATORY DATA ANALYSIS 5-number summary: The five-number summary is a descriptive statistic that provides information about a set of observations. It consists of the five most important sample percentiles: The sample minimum (smallest observation) The lower quartile or first quartile The median (middle value) The upper quartile or third quartile The sample maximum (largest observation) → box plot: THE R LANGUAGE AND NORMAL DISTRIBUTIONS R: intractive exploration and visulization of data + statistical models and distributions + CRAN Central Limit Th: sum/mean of n iid random variables many statistical test assume data to be normally distributed DISTRIBUTIONS poissons distribution: accurrence freq exponential distribution: interval between 2 (poissons) events Zipf/Pareto/Yule distributions : frequencies of different terms in a document, or web site visits binomial/multinomial distribution: nb of count of events RHINE PARADOX SPARK'S MACHINE LEARNING TOOLKIT mllib: scalable, distributed ML library, sklearn-like ML toolkit https://spark.apache.org/docs/latest/mllib-guide.html lab: collaborative filtering — matrix factorisation ⇒ alternating least square(ALS): trouble with summary stats : Anscombe's Quartet → have same statistics property → quite different in fact: Takeaways : • Important to look at data graphically before analyzing it • Basic statistics properties often fail to capture real-world complexities Lab3. Text Analysis and Entity Resolution Entity Resolution (ER) refers to the task of finding records in a data set that refer to the same entity across different data sources (e.g., data files, books, websites, databases). ER is necessary when joining data sets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A data set that has undergone ER may be referred to as being cross-linked. The file format of an Amazon line is: \"id\",\"title\",\"description\",\"manufacturer\",\"price\" The file format of a Google line is: \"id\",\"name\",\"description\",\"manufacturer\",\"price\" re.split re.split()有个很讨厌的地方: 字符串以句号等结尾时, 最后总是会出现一个空字符串: >>> re.split('\\W+', 'Words, words, words.') ['Words', 'words', 'words', ''] 解决办法就是用个filter: filter(None,re.split(split_regex, string.lower()) ) tfidf TF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document. IDF rewards tokens that are rare overall in a dataset. cosine similarity The metric of string distance we will use is called cosine similarity. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors. flatMap : 一行变多行, 别忘了... broadcast variable we define the broadcast variable in the driver and then we can refer to it in each worker. Spark saves the broadcast variable at each worker, so it is only sent once. 声明广播变量的办法也很简单, 只要: idfsSmallBroadcast = sc.broadcast(idfsSmallWeights) 然后用的时候要改成 xx.value EXCEPT语句 找了一下没发现spark有SQL的EXCEPT语句(就是和join相反), 于是只好这么写了: nonDupsRDD = (sims .leftOuterJoin(goldStandard) .filter(lambda x: x[1][1]==None) .map(lambda x:(x[0],x[1][0]))) 用leftouterjoin 然后再只保留为None的那些... 应该不是最佳写法吧... complexity 用的ER办法(cosine similarity)的复杂度是O2 太高了... ⇒ An inverted index is a data structure that will allow us to avoid making quadratically many token comparisons. It maps each token in the dataset to the list of documents that contain the token . So, instead of comparing, record by record, each token to every other token to see if they match, we will use inverted indices to look up records(documents) that match on a particular token . 这种操作的基础是: 有很多向量的support是完全不重合的 . collectAsMap() : 把pair rdd变为map groupByKey(): 这个也用上了 lab4前后做了四个小时 不过很有意思... 第五部分出现out of memory error就没办法了...","tags":"notes","title":"[Spark MOOC note] Lec8. Exploratory Data Analysis and Machine Learning"},{"url":"http://x-wei.github.io/sparkmooc_note_lec7.html","text":"DATA CLEANING ex. deal with missing data, entity resolution, unit mismatch, ... deal with non-ideal samples ⇒ tradeoff between simplicity and accuracy. DATA QUALITY PROBLEMS data quality problems: Conversions in complex pipelines can mess up data Combining multiple datasets can result in errrors Data degrades in accuracy or loses value over time 还提供了一些工具帮助cleaning data: http://vis.stanford.edu/wrangler/ EXAMPLE: AGES OF STUDENTS IN THIS COURSE (students' ages are self-reported...) DATA CLEANING MAKES EVERYTHING OKAY? ex. the appearance of a hole in the ozone layer. DIRTY DATA PROBLEMS Data Quality Continuum: DATA GATHERING solutions in the data gathering stage: re-emptive (先发制人) integrity checks retrospective duplicate removal DATA DELIVERY solutions: DATA STORAGE physical pb: storage is cheap → use data redundancy logical pb: poor metadata, etc ⇒ solutions: publish data specifications data mining tools DATA RETRIEVAL ...总之就是各种方面都会引起data quality pb... DATA QUALITY CONSTRAINTS static constraints: ex. nulls not allowed, field domains data constraints follow a 80-20 rule: Data quality metrics : ... ex. in lab2, examine log lines that are not correctly parsed. TECHNICAL APPROACHES TO DATA QUALITY ex. entity resolution in lab3 EXAMPLE: DEDUP/CLEANING bing shopping被黑了 convert to canonical form (ex. mailing address)","tags":"notes","title":"[Spark MOOC note] Lec7. Data Quality"},{"url":"http://x-wei.github.io/sparkmooc_note_lec6.html","text":"RELATIONAL DATABASE review: key data management concepts: data model schema relational data model structured data: have a specific schema to start with relationl database: a set of relations. 2 parts to a Relation: schema: name of relation, name and type of columns instance: any data at given time ( cardinality :=nb of rows, degree :=nb of fields) LARGE DATABASES RELATIONAL DATABASE EXAMPLE AND DISCUSSION cardinality=3 degree=5 advantages of Relational Databases: well-def structure maintain indices for high performance consistancy maintained by transactions disadvantages: limited, rigid structure most disk space taken by large indices transactions are slow poor support for sparse data (which is common) STRUCTURED QUERY LANGUAGE (SQL) supported by DataFrame of pyspark JOINS IN SQL cross join: carteian product EXPLICIT SQL JOINS explicit version is preferred TYPES OF SQL JOINS ⇒ controls how unmatched keys are handled LEFT OUTER JOIN: keys appearring in left table but not in right table will be included with NULL as value JOINS IN SPARK for spark DataFrame: support inner/left outer/semi-join for pair RDDs : support inner join(), leftOuterJoin(), fullOuterJoin() join ex: outerjoin ex: fullouterjoin ex: Lab 2 - Web Server Log Analysis with Apache Spark Apache Common Log Format (CLF): 127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] \"GET /images/launch-logo.gif HTTP/1.0\" 200 1839 Row( host = match.group(1), client_identd = match.group(2), user_id = match.group(3), date_time = parse_apache_time(match.group(4)), method = match.group(5), endpoint = match.group(6), protocol = match.group(7), response_code = int(match.group(8)), content_size = size ) distinctByKey 一个pair RDD按照key来distinct不知道有没有distinctByKey之类的东西, 只好写成这样, 不知是不是对的: dayHostCount = dayGroupedHosts.map(lambda group : (group[0], len(set(group[1])) ) ) ...总体来说很有意思的一个lab...","tags":"notes","title":"[Spark MOOC note] Lec6. Structured Data"},{"url":"http://x-wei.github.io/sparkmooc_note_lec5.html","text":"KEY DATA MANAGEMENT CONCEPTS data model: collection of concepts for describing data schema: a description of a particular collection of data using a given data model structure spectrum: semi-structured data: apply schema after creating data. FILES files: named collection of bytes, in hierarchical namespace (but: In a Content-Addressable Storage system files are stored, arranged, and accessed based on their content or metadata, not in hierarchy) SEMI-STRUCTURED TABULAR DATA table: a collection of rows and columns, each row has an index , each column has a name . cell: by a pair (row, col), values can be missing, types are inffered from content CSV: PDB:(filed name can be repeated on multuple lines) CHALLENGES WITH TABULAR DATA challenges: challenges for tabular data from multiple source : challenges for tabular data from sensors : PANDAS AND SEMI-STRUCTURED DATA IN PYSPARK pandas DataFrame : represented as python dict (colname → series) pandas Series : 1D labeled array capable of holding any data type spark DataFrame : Distributed collection of data organized into named columns. types of columns are inferred from values. Using dataframes can be 5 times faster than using RDDs: SEMI-STRUCTURED LOG FILES ex. Apache web server log format EXPLORING A WEB SERVER ACCESS LOG NASA http server access log http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html DATA MINING LOG FILES Data mining log files is a data exploration process that often involves searching through the data for unusual events, a task that can be done using dashboards for visualizing anomalies. The data being analyzed usually includes machine resource usage data and application queue information. FILE PERFORMANCE binary/text performance benchmark: ⇒ read and write times are comparable binary files are mach faster than palin text files compression performance benchmark: ⇒ write times are much larger than read times small range of compressed file size binary still much faster than text LZ4 compression ~= raw IO speed","tags":"notes","title":"[Spark MOOC note] Lec5. Semi-structured Data"},{"url":"http://x-wei.github.io/sparkmooc_note_lec4.html","text":"PYTHON SPARK (PYSPARK) a spark prog has 2 programs: dirver program: runs on driver machine worker program: runs on local threads or cluster nodes a spark prog first creates a SparkContext object: tells how and where to access a cluster shell will automatically create the sc varible in iPython: use constructor to create a SparkContext obj ⇒ use this SparkContext obj to create RDDs Master: The master parameter (for a SparkContext) determines which type and size of cluster to use RDDs Resilient Distributed Dataset : immutable once created spark tracks linege information to compute lost data efficiently operations on collections of elements in parallel to create RDDs : paralizing existing python collections transforming existing RDDs from files can specify the number of partitions for an RDD 2 types of operations on RDD: tranformation: lazy, executed only one action runs on it action Working with RDD: create an RDD apply transformations to that RDD (ex. map, filter) apply actions on RDD (collect, count) ex code: data = [1,2,3,4] rDD = sc.paralize(data, 4) distFile = sc.textFile(\"readme.txt\", 4) // elements are lines in the file SPARK TRANSFORMATIONS to create new dataset from existing one (lazy) examples of transformations: PYTHON LAMBDA FUNCTIONS single expression TRANSFORMATIONS ⇒ spark truns the function litral into a cloture, balck code runs in driver, green code in workers SPARK ACTIONS cause spark to execute recipe to transform source. SPARK PROGRAMMING MODEL CACHING RDDS to avoid having to reload data: rdd.cache() ⇒ read from memory instead of disk SPARK PROGRAM LIFECYCLE create/paralise ⇒ transform ⇒ [cache] ⇒ action SPARK KEY-VALUE RDDS each element of a pair RDD is a pair tuple key-value transformations: ex: careful using groupByKey : create lots of data traffic and iterables at works PYSPARK CLOSURES one closure per worker is sent with every task no communication between workers changes to global vars will not effect driver / other workers ⇒ pbs: inefficient to send large data to each job one-way: driver → worker pyspark shared vaiables : 2 types: Broadcase variables : send large, read-only variables to all workers Accumulators aggregate values from worker to drivers only driver can access its value for workers the accumulators are write-only SPARK BROADCAST VARIABLES ex. give every worker a large dataset SPARK ACCUMULATORS can only be \"add\" to by associative operation careful to use accumulators in transformations: Lab1 VB更新以后虚拟机打不开了, 解决办法在: http://bbs.deepin.org/forum.php?mod=viewthread&tid=26001","tags":"notes","title":"[Spark MOOC note] Lec4. Spark Essentials"},{"url":"http://x-wei.github.io/Scrapy 上手笔记.html","text":"Scrapy是用来爬取数据的很流行的包, 这里小记一下. 以前几天做的 一个爬虫 为例子, 这个爬虫把韩寒一个app的前九百多期的文章抓了下来. I. installation scrapy的安装参考: http://scrapy-chs.readthedocs.org/zh_CN/latest/topics/ubuntu.html (直接pip安装的好像缺少什么包) II. prerequisite XPath 需要学习scrapy首先需要会XPath, 这是一种方便与在html/xml文档里查找所需元素的语句. 这个还是很好学的, 其实只需要花一刻钟时间看看w3school的 教程 , 就可以掌握够用的知识进行下一步了. 这里总结一下我觉得会用到的语句(不全, 不过经常用到): //book 选取所有名字叫做book的元素 bookstore/book 选取bookstore的子元素中所有叫book的元素 //title[@lang='eng'] 选取lang属性为\"eng\"的所有title元素 //titile/text() 选取title元素的文字内容 descendant-or-self::text() : 选取自己或者所有后代节点的文字内容 另外还有个在线测试XPath语句的网站, 可以用这个测试XPath语句: http://xpath.online-toolz.com/tools/xpath-editor.php 审查元素 再一个就是要用chrome的\"审查元素\"功能, 用这个功能可以看到想查找的网页内容对应在html文件的位置, 甚至可以直接右键复制想要的元素的XPath......(不过有时候并不是最合理的, 所以刚才XPath也不是白学...) III. scrapy shell 网上的教程一般是从一个 tutorial 开始的, 介绍了一个小项目, 但是我觉得从scrapy shell开始应该更合理, 有时候甚至没必要建立一个工程, 在这个shell里就可以抓到想要的数据. 启动的办法很简单: $ scrapy shell 'url' 其中 url 就写想要爬取的一个网址. 这个shell简单说来, 就是一个测试爬虫的交互环境, 除了 多了一些特殊变量和函数 , 就是一个普通的(i)python shell. 先说两个scrapy shell多出来的变量: response : 把启动的 url 抓取后得到的 Response 对象, 比如 response.body 就包含了抓取来的html内容 sel : 用刚刚抓取的内容建立的一个 Selector 对象, 简单理解, Selector对象可以让我们执行XPath语句提取想要的内容 经常的用法就是用 response 对象查看爬取的情况( response.status ), 用 sel 对象测试XPath的正确: sel.xpath(\"xpath_statement\").extract() 会在获取的response.body里用xpath查找并提取内容. 再说两个scrapy shell添加的函数: fetch(request_or_url) : 修改请求或者网址, 这样scrapy shell会从新用这个request/url抓取数据, 相应的sel和response等对象也会自动更新. view(response) : 在浏览器里查看刚刚抓取的内容. 这里举个例子, 抓取一个的文章标题: $ scrapy shell 'http://wufazhuce.com/one/vol.921#articulo' ...... In [1]: response.status Out[1]: 200 In [2]: sel.xpath('//*[@id=\"tab-articulo\"]/div/h2/text()').extract() <string>:1: ScrapyDeprecationWarning: \"sel\" shortcut is deprecated. Use \"response.xpath()\", \"response.css()\" or \"response.selector\" instead Out[2]: [u'\\n\\t\\t\\t\\t\\t\\t\\u78b0\\u4e0d\\u5f97\\u7684\\u4eba\\t\\t\\t \\t\\t'] In [3]: print sel.xpath('//*[@id=\"tab-articulo\"]/div/h2/text()').extract()[0] 碰不得的人 scrapy shell的完整文档在: http://doc.scrapy.org/en/latest/topics/shell.html IV. scrapy project 接下来说建立scrapy工程, 这个按照tutorial走就好了. 建立工程: scrapy startproject my_proj 会新建一个my_proj文件夹, 里面的结构是: $ tree . └── my_proj ├── scrapy.cfg └── my_proj ├── __init__.py ├── items.py ├── pipelines.py ├── settings.py └── spiders └── __init__.py 要修改的文件主要有两个: items.py 定义要抓取的数据 spiders/xxx.py 定义自己的爬虫 1. 自定义爬虫 先定义爬虫, 在spiders文件夹里面, 新建一个python文件, 这里定义一个 scrapy.spider.Spider 的子类: class OneSpider ( scrapy . spider . Spider ): name = \"one_spider\" start_urls = [ \"http://wufazhuce.com/one/vol. %d #articulo\" % i for i in range ( 1 , 924 ) ] def parse ( self , response ): title_path = '//*[@id=\"tab-articulo\"]/div/h2/text()' title = response . xpath ( title_path ) . extract ()[ 0 ] . strip () print title 这里, Spider子类一定需要定义三个东西: name ： 是爬虫的名字, 一会爬取的时候需要 start_urls : 启动时进行爬取的url列表 parse() 方法 爬虫启动的时候会把每一个start_urls里的网址下载, 生成的 Response 对象会传入这个 parse() 方法, 这个方法负责解析返回的 Response 对象, 提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象等... 2. 保存抓取的信息到item 刚才只是做到了抓取需要的信息, 还没有能够保存到文件里, 下面要将抓取的信息做成一个 Item 保存. 首先定义要保存的信息: 修改items.py文件, 里面定义一个 scrapy.Item 的子类: class OnearticleItem ( scrapy . Item ): # define the fields for your item here like: vol = scrapy . Field () title = scrapy . Field () author = scrapy . Field () content = scrapy . Field () 这个文件很简单, 只是说明一下要抓取的信息, 他们都是 scrapy.Field() , 这个东西类似一个字典. 然后在爬虫里保存item: 为了保存抓取的内容, 在parse()方法里, 得到需要的数据以后, 新建一个 OnearticleItem , 把抓到的内容放进这个item里, 然后返回这个item即可. def parse ( self , response ): nb = re . findall ( '\\d+' , response . url )[ 0 ] title_path = '//*[@id=\"tab-articulo\"]/div/h2/text()' author_path = '//*[@id=\"tab-articulo\"]/div/p/text()' content_path = '//div[@class=\"articulo-contenido\"]/descendant-or-self::text()' title = response . xpath ( title_path ) . extract ()[ 0 ] . strip () author = response . xpath ( author_path ) . extract ()[ 0 ] . strip () content = ' \\n ' . join ( response . xpath ( content_path ) . extract () ) . strip () print nb , title , author item = OnearticleItem () item [ 'vol' ] = nb item [ 'title' ] = title item [ 'author' ] = author item [ 'content' ] = content return item 3. 运行爬虫 以上的文件修改好了以后, 只需 在命令行里 启动爬虫即可, 这时候就用到了刚才定义的spider的 name 属性: $scrapy crawl one_spider -o one.csv 大约几分钟功夫, 九百多篇文章就放到了one.csv文件里~","tags":"tech","title":"Scrapy 上手笔记"},{"url":"http://x-wei.github.io/hashcode2014-solved-by-LP.html","text":"INF580(programmation par contraintes) 大概是在X学到的最有用的一门课, 它让我能够用把运筹学(MAP557)里学到的东西和计算机结合起来: 用电脑的力量解决(大规模)运筹问题. 这门课的projet是去年巴黎谷歌举行的一个比赛的题目: 最优化谷歌街景拍照小车的路线. 做这个projet的三周里, 我和Manu从一开始信心满满, 到中间一筹莫展, 再到后来柳暗花明, 以及最后乘胜追击终于在今晚得到了近乎完美的解答, 非常精彩, 这里特意一记. 问题描述 谷歌那次比赛的题目在这里(我们做的是Main Round的题目): https://sites.google.com/site/hashcode2014/tasks 简单来说, 就是已知巴黎的道路信息, 设法用八辆车(每辆车的行驶时间有限)从巴黎谷歌出发, 尽可能多的走遍巴黎的所有街道, 参赛者给出这些车的路线, 他们的分数就是这八辆车走过的街道的长度之和(重复走的街道不算分). 去年四月份我们也参加了这个比赛, 不过当时纠结于如何设计每辆车的路线, 最后只是用了贪心算法, 再加上一点点的随机, 得到的结果并不好... 当时ENS的人包揽了前三名, 而且比赛后进一步把分数刷到了满分: 他们的路线可以把所有街道都跑遍. 这学期学了INF580以后, 手里有了 JaCoP 以及 AMPL 等\"重型武器\", 我们经过一番努力也得到了满分, 而且用了几乎最少的时间. 代码放在了: https://github.com/X-Wei/INF580_HashCode2014 (里面还有个pdf的报告, 比这篇博客里说的精炼). I. 初步建模 线性规划(LP)模型 站在一个更高的角度上考虑这个问题, 从一个线性规划的角度看, 给定一个图 G(V,E) , 目标是最大化走过的街道长度, 如果用对每条街道 e , 定义一个 x_e ,: 当一条街被经过的时候使它为1, 否则为0, 那么目标函数就是一个x的线性组合(系数就是对应街道的长度). 但是如果一条街被经过了大于一次, x_e 也只能是1, 所以再引入一个变量 y_e : 用来表示一条街被经过的 次数 . 所以 x_e 可以看成是 y_e 的indicatrice函数: , 这一点可以通过增加线性约束: x_e<=y_e 即可实现, 因为 x_e 的取值范围是{0,1}. 不过一条街可能从两个方向被经过( G 里面的边是有向的), 如果 e '代表反方向的 e , 那么还应该增加约束: x_e+x_e'<=1 就好了. 接下来 y 要满足的就是所有的 y 要组成一个 路径(path) , 对于既不是起点又不是终点的一个节点 v , 还是很好写的, 类似flot: sum(进入v的y_e) = sum(从v流出的y_e) . 对于起点 v_start 来说, 其实也不难: sum(进入v_start的y_e) = sum(从v-start流出的y_e)+1 . 但是对于终点的话, 有个问题是我们不知道车会停在哪里! Manu太聪明了, 对每一个节点 v 引入了另一个变量 f_v ,取值范围也是{0,1}, 如果在节点 v 停下来, 那么 f_v 等于1, 否则就是0. 所以上面的约束可以写成: sum(进入v的y) = sum(从v流出的y_e)+f_v . 另外别忘了只能停在一个节点, 所以再加约束: sum(所有的f_v)=1 . 以上的目标函数以及约束, 虽然数目庞大, 但是都是线性的, 所以是一个整数线性规划问题, 写成数学形式就是: 从solution得到路径 一旦上面那个线性规划问题解决了, 我们得到的将会是那些变量 x , y ,以及 f , 但是需要从中提取一条路径最后才能作为solution!! 这个问题实际上可以归结为: 给定一个一笔画的曲线(其实就是那些 y : 可以想象把每条边都复制 y 次, 得到的就是这么一个曲线了), 找出一个可以将其一笔画出来的路径(这条路径也叫\"欧拉路径\", 没\"汉密尔顿路径\"有名). 聪明的Manu很快想出来一个算法(我们叫它\"Orsini算法\"): 大意是当走到底没有走完的时候, 把那些错过的路径夹在原路径的中间就好了. 算法描述如下: i) 从起点开始出发走, 把走过的边涂上颜色(以后不能再走), 只要还可以继续走(还有没有着色的边可走)就一直走下去, 一直到无路可走. (应该会停在 f=1 的地方), 得到路径 p0 . ii) 如果所有边已经被走过了: over . 如果没有: 找一个没有走过的边与 p0 的交叉点: v (如果找不到的话说明有问题: 后面就遇到这个问题了), 然后执行(iii) iii) 从 v 出发, 在进行类似(i)的操作得到一条路径(应该是一个环), 然后把 p0 从 v 那里劈开, 把这个环塞到中间组成新的 p0 , 然后再执行(ii). 这样就可以得到对应的路径了. 一开始的想法 以上的建模是针对一辆车的, 我们一开始的想法是: 对一辆车进行这个操作, 得到结果以后更新一下 G (把那些已经走过的街道的长度设为0, 然后再走下一辆车... 这样跑8次就能得到最优解了. 一切看起来 似乎 都非常完美...... II.遇到的问题 计算能力问题 写好了AMPL的程序以后, 我们遇到的第一个问题是: 这么大的问题, 一般电脑算不出来(曾经用glpk让电脑跑了一夜, 还是没有得到结果). 后来问老师, 老师说glpk并不是非常高效的求解器, 然后推荐给了我们一个非常给力的网站: NEOS ! 这个网站可以让人上传AMPL程序, 然后用它们的服务器跑, 而且使用的求解器也是商用的, 比开源求解器快的不是一个数量级(我们发现最快的求解器是 Gurobi , 十分给力). 这样一来计算的问题就解决了, 但是我们很快发现一个更严重的问题, 是我们的建模里的一个严重缺陷... subtour问题 我们兴冲冲的拿NEOS的结果跑Orsini算法的时候, 发现总是报错: 也就是在第二步, 程序无法找到一个和 p0 有交叉点的边: 也就意味着我们的模型得到的路径并不是只有一个connected component!!! 也就是说, 我们得到的解其实是一条从起点到某个终点的路径, 外加很多和这个路径没有交点的圈圈(subtour)! 是啊, 这个问题很类似旅行商问题(TSP), 而TSP的困难之一 就是要解决subtour的话需要加入2&#94;n个新的约束.... 2&#94;10000个约束? impossible... III. subtour问题的\"解决\" 各种纠结 在一次PC上有一道题目介绍了TSP subtour问题的一种建模方式, \"potentiel\"建模, 可以防止加入2&#94;n个约束: 为每个节点 v 引入新的变量 u_v , u_v 代表了节点 v 被访问的顺序, 约束做的非常巧妙, 是这样的: 这样, 当 x_ij 是1的时候, 就保证 u_j 比 u_i 大1, 而当 x_ij 是0的时候, 这个约束则非常松弛, 几乎相当于没有. 不过这个方法套到我们这个projet的话也有问题: 那就是每个节点只能有一个 u_v , 所以每个节点只能最多访问一次.... 不过我们很快想到了办法: 把所有节点复制一个(复制\"一层\"), 然后每条边的话也进行复制, 同时加上那些连接各层的边. 举个例子, 原来的一条边是: (u, v) , 我们复制了一层, 这些新节点叫它们 u',v ',... 那么在这个两层的图里, 我们要把原来的这条边变成4条: (u, v) , (u, v') , (u', v) 和 (u', v') . 这样就可以保证路径可以经过一个点两次了, 如果想要保证可以经过一个点K次, 只要做K层就好了(每条边变成K&#94;2条). 然后, x 也不需要了, 因为每条边的 y 最大就是1. 但是还有一个问题是目标函数, 由于一条边变成了四条, 如果走四遍的话就多算了四次这条边的长度... 有两种解决办法: 一是类似前面的方法, 给每条边指定定一个indicatrice, 这样就不会重复了, 还有一种更简单的办法是, 在四条边里, 只取一条边的长度保持不变, 其他长度都设成0, 算法自然会优先走那条长度不是0的边(这个没有仔细证明, 不过貌似是这样的). 我们采用了第二个办法.... 就这样废了好大功夫, 写好了两层节点的程序, 送去NEOS一跑.... 超时了(超过8h后NEOS会自动停止求解过程)... 后来我们发现即使是不多加新一层的程序, 也会超时. 那节PC里也说, 这种建模方法实际上计算效率比加入2&#94;N个约束的建模还要差... 所以总结一下就是: 问题还是没有解决... 绕过subtour问题 就在一筹莫展之际, Manu提议把搜索的范围缩小: 不用整个巴黎的数据, 只取起点附近的区域, 这样的话, 说不定我们原先的模型得到的结果会少一点connected components... 试了一下, 果然! 虽然没有强制要求路径的连通性, 得到的结果确实(几乎)只有一个联通分支! 后来就想到了一个很聪明的办法: 用八倍的T作为时间限制, 让一辆车去跑, 由于时间很充分, 我们得到的解应该不太会出现多个联通分支, 然后一旦得到了这个路径(后来我们叫它\"big path\"), 只需要把它分成8段, 每辆车先从起点(巴黎谷歌)跑到每一段的开头, 然后沿着这条路跑就好了啊!! 虽然从巴黎谷歌跑到每一段的起点会稍微浪费一些时间, 但是这点损失其实微不足道!! 而且也不用跑8次LP这么麻烦了 -- 要是早点想到这个就好了!! 而且很神奇的是, 把时间变成了8T以后, 我们的Lp模型计算的更快了: 原先要计算30分钟左右, 现在只要差不多3分钟就出结果了!! 这一点没太想明白, 可能是搜索的空间变小了?? 说写就写, 最短路径用Dijkstra算法就能得到了, 不一会就写出了这个把一条big path变成8条small path的程序. 然后我们一举得到了1957596分(离满分还差了两千米左右)!! IV. 进一步优化 优化的话有两个方向: 第一个方向是修改那个程序代码, 使得每次走到big path某段起点的时候可以少走些路, 不过这个方向应该没什么前途: 程序写起来麻烦不说, 可以改进的空间也很有限, 因为big path一共也没有剩多少时间, 所以八辆车是不可能把big path走完的. 第二个方向则是设法优化big path的时间使用, 使得它在得到最长路径的同时使用尽量少的时间. 引入时间正则项 其实8T作为时间来说非常充裕, 但是我们的解里面, big path还是把时间用的差不多了: 因为时间根本没有出现在目标函数里. 所以, 可以把时间也作为目标函数加上去, 不过要注意最优先要优化的还是路径长度, 所以时间项前面要乘以一个非常小的数(比如0.000001), 类似做regularization. 所以新的目标函数是: 这样一运行, 我们的big path居然剩下了六千多秒的时间!!! 这些时间足够八辆车跑到各段的起点了!! 我们这样, 八辆车走完了big path所有的路程得到的结果离满分只差了7米, 而最后一辆车还有两千多秒没有走! 检查了一下, 发现有一个7米长的路是唯一剩下没走过的路: 于是只要再让最后一辆车去走一下就好了(因为时间够用), 最终我们的方案走完了整个巴黎: 等一下, 这里有点奇怪: 为什么优化得到的结果并不是最优的(没有走完所有的路程)? 我们通过后来手动走那条剩下的路都没有超时, 所以说其实肯定可以在8T的时间走遍所有街道的! 后来查了一下才发现, 原来Gurobi并不是返回最优解, 而是当当前可行解与最优解足够接近的时候就直接停止, 这个参数的名字叫mipgap, 更多参数可以参考这里: http://www.gurobi.com/documentation/6.0/ampl-gurobi-guide/parameters . 这也就解释了为什么NEOS得到的解不是最优的, 不过还好, 通过最后手动添加那条没走的边, 还是走完了所有的街道. ............ 只是作为强迫症的话觉得还是有点......不够完美. 直接优化时间 今晚, 机智的Manu想到了解决办法: 既然知道所有的街道都会被走遍, 直接不把它看成目标, 而是直接作为约束好了, 然后约束直接改成优化时间! 要就经过所有街道的约束很简单, 那就是对任何街道 e , 都有: y_e+y_e'>=1 . 另外肯定所有街道都会经过, x 就没有必要存在了. LP模型表达为: 用这样的模型, 我们的big path 省下了一万两千多秒, 所以我们的最终结果里, 最后的一辆车跑完以后还有9596秒没有用!! 到这一步, 真的可以算是完美解决这个问题了, 而且强迫症也得救了. Yeah! : )","tags":"tech","title":"运筹的力量: 用线性规划解决Google 2014 HashCode问题"},{"url":"http://x-wei.github.io/Linux下pdf文件的压缩与合并.html","text":"压缩pdf 用 convert 只简单指定resize好像不太好使: convert -resize 50% input.pdf out.pdf 用 gs ( http://blog.sciencenet.cn/blog-467089-773990.html ): gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/screen -dNOPAUSE -dQUIET -dBATCH -sOutputFile=out.pdf input.pdf 后来发现老是compress以后的结果不好, 看到 这篇帖子 发现 convert有好多选项 . 最后实验下来这样convert的效果很好, 既能压缩文件, 又保证了压缩后还足够清楚: convert -density 200 -compress jpeg input.pdf out.pdf 合并pdf 之前 博客 写过, 用pdfjoin: pdfjoin $(ls *.pdf|sort -n) --outfile out.pdf","tags":"soft","title":"Linux下pdf文件的压缩与合并"},{"url":"http://x-wei.github.io/一个简单的python进度条.html","text":"在处理大量数的时候, 如果输出类似 \"process i out of n files...\" 这样的内容来指示进度的话, 虽然可以显示目前的进度(用来安慰等待的心情...)但有个问题是, 如果输出了太多行(比如一万行...), 就看不到前面的内容了... 所以想找一个命令行下面的进度条, 其实python已经有了(不止一个)进度条的包了, 比如 progressbar , 但是不知为什么这个包在windows下面没有能做到刷新显示 -- 就是刷新进度的时候, 没有把原先那一行去掉, 而是在下面再输出了一行... (不过后来在linux下面使用这个包是没问题的, 好奇怪...) 所以想办法自己写了一个, 发现要实现一个简单的进度条还是很简单的, 关键就是使用 \\r , 这样会把光标移动到当前行的开头: 这样下次输出的时候就会把原先的内容冲掉了. 代码只有不到二十行: import sys class SimpleProgressBar (): def __init__ ( self , width = 50 ): self . last_x = - 1 self . width = width def update ( self , x ): assert 0 <= x <= 100 # `x`: progress in percent ( between 0 and 100) if self . last_x == int ( x ): return self . last_x = int ( x ) pointer = int ( self . width * ( x / 100.0 )) sys . stdout . write ( ' \\r %d%% [ %s ]' % ( int ( x ), '#' * pointer + '.' * ( self . width - pointer ))) sys . stdout . flush () if x == 100 : print '' 用法也很简单, 先新建一个SimpleProgressBar对象, 在要更新进度条的时候, 调用update方法即可... # An example of usage... pb = SimpleProgressBar () for i in range ( 301 ): pb . update ( i * 100.0 / 300 ) time . sleep ( 0.1 ) 再吐槽一下windows, 不仅那个progressbar的包不好使, multiprocessing的包也不好使, 郁闷... [08-15补充] 后来想到, 既然用 \\r 就可以实现刷新当前行, 还要用毛的进度条啊.... 直接这样写就好了: for i in range ( 301 ): print 'processing %d out od %d items...' % ( i + 1 , 301 ), ' \\r ' , time . sleep ( 0.1 ) 注意print最后要加逗号 否则就换行了...","tags":"tech","title":"一个简单的python进度条"},{"url":"http://x-wei.github.io/正则表达式入门简介.html","text":"以前虽然也用过正则表达式(比如那个饮水思源的 PPP版图片下载器 ...)但是那时候基本上是网上到处搜 然后把代码拿过来改, 没有系统的学过这个东西. 前一段实习一开始的时候要处理很大量的文本, 从文本里提取出需要的信息, 所以用到了不少的正则表达式, 也好好的学了一下, 现在回来进行一下总结. 什么是正则表达式 很多时候,我们需要在文本里寻找满足一种 模式 ( pattern )的一段子字符串(substring), 注意是一种模式而不是某一个具体的字符串. 举个例子, 在一段文本里寻找这里面出现的所有的网址, 那么对应的模式就是: \"以 www. 开头, 中间有一些东西(可以是字母也可以是数字等), 最后以 .com/.org/.edu... 结尾的所有的字符串\" 再比如, 要查找文本中出现的电话号码, 电话号码的格式是区号加横线再加号码, 那么模式就应该是: \"以3个或4个数字开头, 三个或四个数字之后跟一个横线, 横线后再跟7个或8个数字\" 再再比如, 要查找出现的电子邮件地址, 那么模式大概是: \"以字母或数字或下划线开头, 之后跟着一个@符号, @符号以后一些用点分隔的字母或数字, 最后应该以.com/.org/.edu等结束\" 可以想象实际应用中会有描述起来更加罗嗦的模式, 而 正则表达式就是用来描述这种\"模式\"的语法 , 使用正则表达式, 可以用很简短的代码表达很复杂的模式. 有人给正则表达式起了个别名叫\" 字符串模板 \", 其实更贴切... 还有, 后面总是会提到\"某个正则表达式 exp 匹配(match)了某个字符串 str \", 这里说 \"匹配\" 的意思其实是 \"字符串str满足了正则表达式exp描述的那种模式\" . 关于正则表达式, 这篇博客 可以说是经典之作, 搜索\"正则表达式\"这篇文章几乎都是第一个被找到的. 每次我有问题都要再翻出来看一下这篇, 讲的非常好. 另外强烈推荐一个非常赞的在线测试正则表达式的网站: http://regex101.com/ 做得超级棒(具体怎么棒见后文). 匹配单个字符 匹配某个 特定的 字符: 只需要写出来就好了, 类似于记事本的查找功能. ex. 查找\"abc\"这个字符串, 正则表达式就是: abc 匹配 某一类 字符: 比如匹配数字, 只需要把候选的字符放进 中括号 , 而且可以用横线表示一系列的字符: ex. 匹配一个数字: [0-9] ex. 匹配大写的一个字母: [A-Z] ex. 匹配一个数字或字母(大小写都可以): [A-Za-z0-9] ex. 匹配一个元音字母(aeiou中的一个): [aeiou] 匹配某一类以外的字符: 在这一类字符前面加上 &#94; , 起到的作用就是取这些字符的补集( \"反义\" ). ex. 匹配一个不是数字的字符: [&#94;0-9] ex. 匹配任意一个不是元音字母的字符: [&#94;aeiou] ex. 匹配以b开头, 以d结尾的三个字母长的单词: b[a-z]d , \"bed\", \"bad\"都满足这个条件 匹配正则表达式中用到的的特殊字符 比如要匹配'&#94;'这个字符, 由于这个字符是正则表达式里使用的有特殊含义的字符, 需要在前面加 反斜杠转义 : \\&#94; 类似的特殊字符还有不少: & , . , * , ? , ( , ) , [ , ] ,...... 匹配任意字符 用的最多的是这一个: 匹配除了换行符以外的任意字符: . (在开启\"单行模式\"的时候, 可以连换行符一起匹配, 见后文.) 然后, 在反斜杠后面加某些字母的时候有特殊含义( \"元字符\" ): 匹配字母或数字或下划线或汉字: \\w (用前面的 [A-Za-z0-9] 之类的方法也可以做到) 匹配任意空白字符(空格或TAB): \\s 匹配任意数字: \\d (用前面的 [0-9] 也可以做到) 把上面这些反斜杠加小写字母变成反斜杠加 大写字母 的话, 成了取补集(类似前面那个 &#94; 的作用) 匹配任意不是字母，数字，下划线，汉字的字符: \\W 匹配任意不是空白符的字符: \\S 匹配任意非数字的字符: \\D 所以如果要匹配, 包括换行符的话, 只需要把 \\s \\S 一起用: 匹配所有字符, 包括换行符: [\\s\\S] http://bbs.csdn.net/topics/300041987 匹配位置 有时候还要指定某些位置, 比如说: 要匹配网址, 网址要以\"www.\"开头, 所以如果有个网址是: \"www.abcwww.com\" 的话, 有可能匹配成了中间那个\"www.\", 那么就有问题了. 所以要指定\"www.\"要在一个字符的开头出现, 而不能是在中间(前面不能有字符). 这种情况下我们就不是要匹配某个具体的字符串, 而是 匹配一个位置 . 匹配单词的开始或结束( word boundary ): \\b 这就是我们前面说的情况, 只需要加上\\b, 就不会匹配在某个单词中间出现的\"www.\"了. ex. 匹配网址开头的\"www.\": \\bwww\\. 例子链接 ⇐ 可以把最开始那个 \\b 去掉, 看看结果有什么区别. 注意这个 \\b 可以是单词的开始, 也可以是单词的结束. 还是网址的例子, 如果一个网址是:\"www.abcwww.qwert.coming.com\"的话, 我们说\"匹配以.com\"结尾的字符串, 就有可能是匹配到了 qwert.com ing, 也会有问题, 所以我们应该在\"com\"后面加上一个 \\b . ex. 匹配网址结束处的\".com\": \\.com\\b 例子链接 ⇐ 同理可以看看去掉 \\b 以后有什么不同. 然后这个元字符 \\b 变成大写以后也是反义, \\B 匹配所有不是单词开头或结束的位置. 匹配整个字符串的开头: &#94; 注意是整个字符串的开头, 而不是指每一行的开头! (但是在\"多行模式\"时可以代表每一行的开头, 见后面冠以正则表达式选项的讨论) 匹配字符串的结束: $ 同样是指整个字符串的结束. 重复次数 这个和前面介绍的匹配字符配合起来(指定要匹配的字符, 再指定字符重复的次数), 就可以完成大部分的工作了. 字符重复 (exactly)n次 只需要把数字n放进 大括号 里就可以了. ex. 找一个六位的数字, 那么就是0-9的数字重复6次: [0-9]{6} 重复 m到n次 只需要在大括号里把m和n用逗号进行分隔. ex. 找一个6到8位的数字: [0-9]{6,8} 但是要重复m 或 n次的话, 我没有找到好的写法, 可以用后面介绍的分支条件实现, 不过比较繁琐... 重复 大于等于n次 (重复小于等于n次等价于0到m次) 大括号里面n后面加一个逗号: ex. 找6位以上的数字: [0-9]{6,} 平时用到的比较多的是要求 \"重复大于等于1次\" 或者 \"重复大于等于0次\" , 由于上面这样的写法有的时候比较罗嗦, 所以有了下面的简写: 重复 大于等于1次 : + 等价于 {1,} ex. goo+gle , 可以匹配\"google\", \"gooogle\", \"goooogle\"等, 因为第二个o可以出现大于等于1次. (当然上面这个例子也可以在前后加上单词边界 \\b , 以防止匹配到某个长单词的中间) 重复 大于等于0次 : * 等价于 {0,} ex. 匹配以a开头以t结尾的单词: a[a-z]*t , \"at\", \"attachment\", \"act\"等都符合条件 重复一次或0次: ? 等价于 {0,1} ex. 匹配\"color\"或者\"colour\": colou?r 贪婪(greedy)模式/懒惰(lazy)模式 前面介绍的那些指定重复次数的部分, 由于有时是指定一个重复次数的范围, 而经常匹配到的重复的次数也是一个范围, 所以有\"贪婪\"和\"懒惰\"的区别. 好像上一句话没有说清楚... 举个例子, 要匹配大于六位的数字, 我们的正则表达式是 [0-9]{6,} , 意思是要有六个或更多的重复次数. 那么对于一个字符串: \"123456789\", 一共有9位, 它既可以看作重复6次(取前6位), 也可以看作重复7次, 8次, 9次... 那么岂不是含义上出现歧义(ambiguous)了?? 而实际上, 我们写的表达式 [0-9]{6,} 在这个例子里最终会匹配所有的9位, 这是因为我们写的这个表达式默认上是 贪婪模式 的, 意思是匹配重复>=6次的的重复, 但是会 尽可能多重复 . 与之相反, 懒惰模式 的意思是 尽可能少重复 . 正则表达式默认是贪婪模式 , 要开启懒惰模式只需要在重复次数的后面加上一个问号\" ? \". +? : 重复>=1次, 但是尽量少重复 *? : 重复>=0次, 但是尽量少重复 ?? : 重复0或1次, 但是尽量少重复 {n,}? : 重复>=n次, 但是尽量少重复 {m,n}? : 重复m到n次, 但是尽量少重复 再插一句, 这个\"贪婪模式的开启标志\" ? 和\"重复次数的符号\" ? 虽然都是问号, 但是由于放的位置不一样, 所以不会造成歧义, 因为\"重复次数的符号\"的问号是出现在\"匹配字符\"(比如 \\w , [0-9] , . )的后面的, 而\"贪婪模式的开启标志\"的问号是出现在\"重复次数\"(如 {m,n} , + , * )的后面的... 分支条件(alternative) 意思就是可以匹配两种模式的任意一种, 看例子: ex1. 中国的电话号码有的城市是三位区号加横线加八位号码(比如\"021-54749110\" ⇐ 这是上海一所学校的保卫处号码), 有的是四位区号加横线加七位或八位号码的(比如\"0635-8238080\" ⇐ 这是山东一所大学的保卫处, 以及\"0531-88881234\" ⇐ 这是山东台小么哥的号码==...). 要匹配这样的号码, 如果这样写: \\d{3,4}-\\d{7,8} (别忘了 \\d 等价于 [0-9] ) 那么其实是不对的, 因为我们不允许出现3位区号后面跟7位号码的情况出现, 所以这里其实是两种模式, 所以我们是要 匹配这两种模式其中的任意一种 . 这种情况下, 只需要把两种情况分别写出来, 中间用 | 分割即可: \\d{3}-\\d{8}|\\d{4}-\\d{7,8} 不过看起来有点乱了... 可以加上小括号这样显得更清楚一些: (\\d{3}-\\d{8})|(\\d{4}-\\d{7,8}) 不过小括号在正则表达式里面除了让正则表达式(稍微)更容易读以外, 还有一个作用是后面提到的\"分组\"或者\"捕获\", 这个后面再说... ex2. 前面说到过怎么匹配重复m 或 n次的数字, 我没有找到更方便的方法, 只好用分支条件实现了: \\d{m}|\\d{n} 但是要注意, 这个时候会有和前面贪婪/懒惰类似的问题: 如果一串数字有9位长, 我要匹配6位或9位的数字, 那么应该匹配到哪里呢? 其实这个时候取决于我们是写 \\d{9}|\\d{6} 还是 \\d{6}|\\d{9} , 正则表达式有类似于编程语言里的\" 条件短路 \"(shortcut)性质, 一旦第一个模式满足了, 就不会去寻找竖线后面的第二个模式了... 小括号: 对结果进行分组(group), \"捕获\" 先说一句别的, 前面介绍重复次数的部分, 介绍了怎么指定某个类型的 单个字符 重复的次数, 要是想寻找某个特定字符组合的重复次数该怎么办呢? 只需要把要重复的部分用小括号括起来, 然后后面指定重复的次数就可以了... ex. 寻找好几个\"bla\"连着的字符串: (bla)+ , 这样\"blablabla\"之类的字符串就可以被匹配了. 所以小括号的这种作用类似于小括号在一般编程语言的作用, 就是把一部分内容放在一起. 但是与此同时, 每一个小括号其实还\" 捕获 \"了一个\" 分组 \"(group). 意思就是在匹配到的字符串里(叫做一个\"match\"), 我们还可以得到它的一个子字符串. 还是举电话号码的例子, 前面的方法我们可以得到一个类似\"021-54749110\"这样的匹配结果, 但是如果我们想 把区号和区号后的号码分别保存 的话, 还需要再 在程序里 对这个字符串做个处理: 比如把字符串的前三个截取出来保存为区号, 把第五个字符到最后的子字符串截取出来保存为号码... 这样会有问题, 因为我们也可能得到\"0531-8881234\"这样的结果... 于是我们只能先在得到的字符串里寻找\"-\", 然后再从\"-出现的位置那里把字符串截成两段...... 总之这样的话在程序里还要进行很麻烦的后期处理, 非常不爽(c'est trop lourd!)...... 幸好我们可以使用正则表达式的分组功能, 在得到结果的字符串的同时, 还在不同的分组里放了对应的的子字符串, 这样在程序里只要得到每一个分组就可以了 . 而给字符串分组的方法也很简单, 就是简单的 给要分组的地方加上小括号括起来就好了 . (当然, 可能你只是为了看得清楚才给表达式加小括号, 但是在正则表达式匹配字符串的时候也会帮你把它捕获到一个分组里.) 所以没加一对小括号就会在结果中增加一个分组, 分组的顺序是按照小括号出现的顺序排列的. 所以在电话号码这个例子里, 我们可以写: (\\d{3})-(\\d{8})|(\\d{4})-(\\d{7,8}) 例子链接 我喜欢reg101这个网站的原因就是它可以显示非常丰富的信息. 在网站的右上角,有关于表达式的解释: 在右下角会显示捕获到的分组(groups): (这里的分组编号可能有点问题, 后两个分组的编号应该是1和2的... 这是由于我们使用了分支条件造成的, 不知是bug还是feature...) 然后关于怎么在程序里获得正则表达式的分组, 可以去看每一种编程语言的相关文档...... 正则表达式选项(\"模式\") 使用正则表达式的时候, 还可以指定一些\"模式\"选项, 比如在reg101网站上, 表达式右边有一个选项窗口: 当开启某些\"模式\"的时候, 正则表达式的匹配行为稍有不同. 其实上面的截图已经讲的很清楚了, 下面捡比较常用的说一下: 全局模式global, \"g\": 就是不止匹配第一个, 而是把所有的匹配都显示出来. 扩展模式(或者\"注释模式\")extended, \"x\": 使用后正则表达式也可以换行, 可以随便加空格(将被忽略), 也可以使用注释(使用 # 开始注释) 这个是我经常用的选项, 有时候表达式写的很长很乱, 如果不能换行或者加注释的话, 下次看得时候就不知道是怎么回事了... 比如刚才那个电话的例子, 我们在开启了扩展模式以后, 可以写成这样: (\\d{3}) - (\\d{8}) #region code, -, phone number | #alternative (\\d{4}) - (\\d{7,8})#region code, -, phone number 例子链接 多行模式multiline, \"m\": 前面\"匹配位置\"那一节提到了, 就是改变 &#94; 和 $ 的意思, 不让它们匹配整个字符串的开始和结束, 而是每一行的开始和结束. 单行模式singleline, \"s\": 前面也提到了, 作用是让 . 也匹配换行符... (所以可见, 单行模式和多行模式根本不是相互排斥的关系 ....... 这名字起的不好...) 关于怎么开启这些选项, 还是要看不同语言的文档... 比如在python里, 只要在 re.compile() 函数使用加入 re.X 等参数, 见 文档 ... 其他内容 这里我只写了我用到的一些东西, 关于正则表达式还有很多没有提到的内容(所谓的\"高级议题\"?), 比如\"零宽断言\"以及\"平衡组\"等内容, 这些都在deerchao的那篇 经典博客 里提到了... 实际例子 例子1 要分析很长的文本, 文本的一部分包含了一趟航班的信息, 比如出发和到达的时间, 出发和到达的机场, 以及航班号等... 这些信息中间都包含在一长串字符中间. 我们用正则表达式处理, 并且把不同的信息放在不同的分组里, 这样就方便了程序的处理. 这里 是我写的正则表达式. 例子2 再举个例子, 我想要现在 这个页面 的所有的东西, 可是用鼠标一个一个点击实在是太麻烦了, 所以可以写一个程序把这些链接都提取出来, 然后自动进行下载. 所以需要分析这个页面的html文件, 右键 → 显示网页源代码就可以看到了, 虽然我不懂html的语法, 但是可以发现, 下载的链接都是这样的片段: en 就是说在 class=\"download_link\" 之后, href= 后面的用引号引起来的内容就是我们想要的下载链接了, 于是可以很快用正则表达式来提取它, 我写的表达式见 这里 .","tags":"tech","title":"正则表达式入门简介"},{"url":"http://x-wei.github.io/putty使用备忘.html","text":"最近要用SSH连接服务器, Windows下面当然就是用putty了, 遇到的问题总结一下. 保存session 打开putty.exe以后, 输入服务器ip, 之后先别点击登录, 先保存一下session下一次就不用再输入了: 之后点击登录就好了. 本地和服务器之间传输文件 传输的时候貌似不能用linux里的scp命令, 而需要使用另一个putty的工具: psftp 下载的时候那个putty.zip压缩包里有一个 psftp.exe , 点击它就打开了. psftp也是一个命令行的工具, 和ssh类似, 用 pwd/ls/cd 等在 服务器的 文件系统里进行移动. 而在 本地的 文件系统里移动的话, 用 lpwd/lcd/lls. 移动到了想要传输文件的目录以后(本地和服务器都移动好了以后), 使用 put filename 上传本地文件到服务器, 使用 get filename 下载服务器文件到本地. http://www.lellansin.com/putty%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6.html 然后这个put和get的命令在文件传输比较慢的时候也没有什么进度提示, 不过可以再开一个putty登录进服务器, 然后用 ls -lh 看看已经传输了多少了...","tags":"soft","title":"putty使用备忘"},{"url":"http://x-wei.github.io/IPython上手学习笔记.html","text":"Learning IPython for Interactive Computing and Data Visualization 这本书的前两章的笔记, 这本书还被放在了IPython官网上, 虽然只有一百页多一点点, 但是讲的内容却很丰富, 介绍了IPython, numpy, pandas以及并行计算等方面. (在开始系统学IPython之前简单使用过IPython, 那时候我还是更喜欢bpython的代码提示功能...) ch1: 10 IPython essentials 在任何变量后面加问号 ? 或者双问号 ?? , 将会输出详细的信息(按 q 退出), ?? 的信息更加详细些 Tab Completion: 没啥好说的 没有bpython做的好 也凑合吧... _, __, ___ 保存最近三次的输出; _i, __i, ___i 保存最近三次的输入(作为字符串保存) magic commands 在IPython里面可以使用一些标准unix命令, 比如 cd , pwd,ls 等... 这个太好了 否则还要 import os , 然后再什么 os.chdir('...') 其实这些unix命令是IPython的 magic commands , 这些magic commands一般用 % 作为前缀. 但是由于默认IPython开启了 automagic system , 上面那些命令可以不用加前缀了(或者使用Tab自动给加上前缀) %run 命令, 运行一个.py脚本, 但是好处是, 与运行完了以后这个.py文件里的变量都可以在Ipython里继续访问 %timeit 命令, 可以用来做基准测试( benchmarking ), 测试一个命令(或者一个函数)的运行时间 ex. %timeit [ x * x for x in range ( 1000 )] 10000 loops , best of 3 : 56.5 µ s per loop %timeit [ x * x for x in xrange ( 1000 )] 10000 loops , best of 3 : 51.7 µ s per loop %debug 命令: 当有exception的时候, 在console里输入 %debug 即可打开debugger. 在debugger里, 输入 u,d (up, down)查看stack, 输入 q 退出debugger %debug > < ipython - input - 34 - 17 c374156862 > ( 2 ) < module > () 1 if 1 < 2 : ----> 2 raise Exception 3 ipdb > u *** Oldest frame ipdb > d *** Newest frame ipdb > q 使用%pdb开启自动pdb模式 %pdb Automatic pdb calling has been turned ON %pylab 命令, 大杀器, 看一下都import了什么: %pylab makes the following imports:: import numpy import matplotlib from matplotlib import pylab, mlab, pyplot np = numpy plt = pyplot from IPython.display import display from IPython.core.pylabtools import figsize, getfigs from pylab import * from numpy import * 画图的时候可以不用非要加 plt. 前缀了, 直接 plot() 即可. 图像化出来的时候, 画图窗口并没有block, 可以动态( interactively )画图. 另外, qtconsole和notebook一样, 指定了 inline 选项以后可以直接在窗口里画图: 后来发现inline的图片貌似不能放大看, 所以有时候还是单独一个窗口比较好, 换到非inline模式只需要再输入以下%pylab, 加上选项qt: %pylab qt IPython Notebook (重头戏) 这个可以在浏览器里(!!)使用IPython, 并且可以使用多行编辑后再一并执行. The Notebook brings the functionality of IPython into the browser for multiline textediting features, interactive session reproducibility, and so on. 在shell/cmd窗口里启动ipython的时候加上notebook: $ipython notebook 看到浏览器打开了, 很神奇: 新建一个notebook, 用用看: 使用的过程中渐渐理解了为什么书里说可以用来做\" multiline textediting features, interactive session reproducibility \"... 因为这不只是个编程的东西, 还可以作为一个笔记本 — 而且是一个交互式的笔记本! ( 注: 更多关于notebook的介绍在下面第二章的内容里. ) 代码,或者段落, 按照cell(格子)进行组织, 一个cell里面的内容可以是code, 但是也同样可以是markdown的段落, 或者是一个标题(heading). 在一个代码的cell里, 写入多行代码, 就像在编辑器里写python程序一样, 按回车只会换行, 不会运行程序. 写了一段程序代码以后, 按 ctrl+Enter 运行程序, 运行结果也是一个作为cell. ( 注 : 在qtconsole里面相反, 如果要输入多行程序的话, 按 Ctrl+Enter 换行(按一一次ctrl+enter即可进入多行编辑模式), 写了几行代码以后要运行的话, 就按两次回车, 或者按 Shift+Enter ) ...还有好多快捷键, 按Esc以后再按h就可以看到... 这个还分编辑模式和命令模式呢... 真不能小看了IPython了! customizing IPython 保存自己的IPython配置文件, 只需要在shell/cmd里输入ipython profile create , 配置文件存储在 ~.ipython 或者 ~/.config/ipython 目录里. ch2: Interavtive Work with IPython IPython可以实现 shell(OS)和python的交互 . 这样做一些unix shell的操作的时候可以不必退出console了. navigating the file system 例子: 完成下载压缩包, 解压缩, 以及打开解压后的文件这些操作... 在py变量前面加入$, 可以把这个变量共享给OS或者magic command: folder = ' data ' %mkdir $folder 这样就在当前目录下建立一个'data'文件夹 — 这可比py的命令好记多了啊... %mkdir 的原理其实是给了shell命令一个别名( alias ). 然后, %bookmark 可以把当前的目录加入收藏夹 下次cd的时候方便直接跳到这里来: ex. %bookmark bm 那么以后可以直接用 cd bm 跳到这个目录下. %bookmark -l 可以列出收藏夹的目录内容. 然后发现原来IPython连文件名都是可以提示的啊!... with open('0<TAB> 0.circles 0.edges Accessing system shell with IPython 在IPython里调用系统的命令, 不用再使用 sys.exec('...') 之类冗长的方式了, 只需要在系统的命令前面加上一个感叹号 ! 即可... shell返回的结果可以作为一个string的列表保存在一个python variable里. ex. In [2]: files = !ls -1 -S | grep edges In [3]: files Out[3]: ['1912.edges', '107.edges', [...] '3980.edges'] (当然 上面这一行只能在unix系统下运行, 因为Windows的cmd没有ls 和 grep命令) 还可以把一条比较长的命令作为alias保存起来, 用 %alias 命令... (这个应该一般用不到) %alias largest ls -1sSh | grep %s The Extended Python Console %history 或者 %hist , 显示之前的记录, 有一些参数可用... %store 把python变量的内容保存下来, 以后的session可以用 %paste 导入并执行剪贴板里面的内容 %run 之前讲过了, 运行py文件, 运行后py文件里的变量可以在console里访问 %edit 打开系统的文件编辑器, 并且在关闭这个编辑器时自动运行程序 介绍了一个包 networkx, 可以用来分析复杂网络(graph)的.... debug debug加入断点: %run -d -b29 script.py 运行script.py 并且在29行的时候暂停, 当输入 c 的时候再继续运行. 一些pdb(debugging环境)里常用的命令: u/d for going up/down into the call stack s to step into the next statement n to continue execution until the next line in the current function r to continue execution until the current function returns c to continue execution until the next breakpoint or exception p to evaluate and print any expression a to obtain the arguments of the current functions The ! prefix to execute any Python command within the debugger benchmarking(\"基准测试\") %timeit fun() 测试一个 函数 的执行速度 %run -t 和 %timeit 效果类似, 作用是测试一个py脚本 文件 的执行速度 更精细的运行时间测试, 可以用 profile模块 The profiler outputs details about calls of every Python function used directly or indirectly in this script. @@...好高级!!! 这样的话就更容易发现程序运行的瓶颈在哪里了! 方法是使用 %run -p 或者 %prun Using the IPython notebook 这个notebook的功能实在是很NB... 不仅可以加入代码/markdown段落, 还可以加入图片和视频... notebook的格式为.ipybn文件, 用JSON存储数据. 输入 ipython notebook (或者在ipython里输入 !ipython notebook )以后, 会在8888端口建立一个web server, 访问 http://localhost:8888/ 就可以看到上面的那个截图, 或者称之为 notebook dashboard. cell magics 的作用域是整个cell(多行), 而magic command的作用域是一行, cell magics的前缀是两个百分号 %% . 从一个py文件直接建立一个notebook, 只需要把文件拖入dashboard即可, 然后notebook也可以保存为文件. 编辑了Markdown以后, 还是 Ctrl+Enter/Shift+Enter , 即可成为格式化的文本, 再双击就可以编辑!! 让plot的图片直接嵌入在notebook里面: 使用 ipython notebook --pylab inline , 或者在notebook里面输入 %pylab inline notebook的一些快捷键 Esc从编辑模式(edit mode)退出到命令模式(command mode) Enter从命令模式到编辑模式 (编辑模式下) ctrl+Enter: 运行程序/markdown代码 shift+Enter: 运行程序, 并自动跳到下一个cell alt+Enter: 运行程序, 并自动在后面新建一个cell在 (命令模式下) c: 复制一个cell x: 剪切一个cell v: 粘贴cell a: 在当前cell上面( a bove)新建一个cell b: 在当前cell下面( b elow)新建一个cell m: 让当前cell变成一个markdown的cell y: 让当前cell变成code的cell 1,2,3...: n级标题 j,k: 上下移动选中的cell, vim风格.. dd(d按两下): 删除一个cell(vim 风格...) ......爽到爆!!","tags":"tech","title":"IPython上手学习笔记"},{"url":"http://x-wei.github.io/pandas学习笔记.html","text":"首先, 导入pandas import pandas as pd 以及开启pylab: IPython里输入 %pylab http://www.bearrelroll.com/2013/05/python-pandas-tutorial/ 基本操作 http://cloga.info/python/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/2013/09/17/pandas_intro/ pandas和numpy的关系 : pandas是建立在numpy上面的, pandas可以处理不同类型的数据集合(heterogeneous data set: DataFrame ), numpy处理的是相同类型的数据集合(homogeneous data set: ndarray ) 读写csv文件 read_csv() df=pd.read_csv('data.csv') 说一下数据类型的问题: 返回类型数据帧( DataFrame ): type(df) = pandas.core.frame.DataFrame df.columns 包含了所有列的标签( 字段名 ) df.index 包含了所有行的标签(可能没有的话, 就是一系列递增的数字了) 但是其中的每一列是 Series 类型: type(df.dep)=pandas.core.series.Series 然后可以将Series转换为numpy的ndarray: array(df.dep) to_csv() 没啥好说的.. df.to_csv('csvfilename') 要是不希望把index也作为一列写进csv文件的话, 就选择参数 index=False http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html?highlight=to_csv#pandas.DataFrame.to_csv indexing & slicing 选择一列: df['dep'] 或者 df.dep 选择前3行(前三条记录): df[:2] 使用标签选取数据 : df.loc[行标签, 列标签] 选择前两列: df.loc[:,('one','two')] 或者用 df.loc[:,df.columns[:2]] 使用位置选取数据 : df.iloc[行位置, 列位置] df.iloc[:,:2] 自动判断的切片 : df.ix[行位置或行标签, 列位置或列标签] 所以前面俩基本用不着了... df.ix[:,('one','two')] df.ix[:,:2] boolean indexing ex. 选择dep是'PAR'的记录 hk[hk.dep == 'PAR'].head() ex. 多个条件, 比如dep是'PAR', dst是'BHM': hk[(hk.dep == 'PAR')&(hk.dst=='BHM')].head() 注意 : 中括号里面的表达式, 每一个条件需要括号括起来, 中间的 & 不能用 and , 等于号 == 不能用 is . 文档里的一个表格: 设置小数精度 http://pandas.pydata.org/pandas-docs/stable/options.html?highlight=precision 设置小数点后六位的精度: pd.set_option('precision',7) 注意六位精度的话要设置precision为7=6+1. 调整某一列的次序 df.reindex(columns=pd.Index(['x', 'y']).append(df.columns - ['x', 'y'])) http://stackoverflow.com/questions/12329853/how-to-rearrange-pandas-column-sequence 随机抽取几行 rand_idx = random.choice(df.index,9, replace=False) #要设置replace = False以防止重复! df.ix[rand_idx] 两个df相merge 两个df的column都一样, index不重复(增加行): pd.concat([df1,df2]) 两个df的index一样, column不同(增加列) pd.concat([df1,df2], axis = 1) adding/deleting columns http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion 新建一列, 加到最后面: df['new_col']=xxx 想要把一列插进中间某一处, 使用df.insert: df.insert(1, 'bar', df['one']) 删除一列, 只需用 del 关键字: del df['one_col'] 两个Series组成一个dataframe: pd.concat([s1, s2], axis=1) 重命名一列: df=df.rename(columns = {'old_name':'new_name'}) 或者: df.rename(columns = {'old_name':'new_name'}, inplace=True) http://stackoverflow.com/questions/20868394/changing-a-specific-column-name-in-pandas-dataframe http://www.bearrelroll.com/2013/05/python-pandas-tutorial/ apply() & map() & agg() apply() 对dataframe的内容进行批量处理, 这样要比循环来得快. df.apply(func, axis=0,...) func : 定义的函数 axis : =0的时候对列操作, =1的时候对行操作 ex. df.apply(self, func, axis=0, map() 和python内建的没啥区别 df['one'].map(sqrt) groupby() 按照某一列( 字段 )分组, 得到一个 DataFrameGroupBy 对象. 之后再对这个对象进行分组操作, 如: df.groupby(['A','B']).sum()##按照A、B两列的值分组求和 groups = df.groupby('A')#按照A列的值分组求和 groups['B'].sum()##按照A列的值分组求B组和 groups['B'].count()##按照A列的值分组B组计数 agg() 对分组的结果再分别进行不同的操作... 参数是一个dict, 把每个字段映射到一个函数上来...... 说的不清楚, 直接看例子: In [82]: df Out[82]: one two three index a 1 1 2 b 2 2 4 c 3 3 6 d NaN 4 NaN In [83]: g=df.groupby('one') In [84]: g.agg({'two': sum,'three': sqrt}) Out[84]: two three one 1 1 1.414214 2 2 2.000000 3 3 2.449490 甚至还可以对每一列进行多个处理操作: In [100]: g.agg({'two': [sum],'three': [sqrt,exp]}) Out[100]: two three sum sqrt exp one 1 1 1.414214 7.389056 2 2 2.000000 54.598150 3 3 2.449490 403.428793 具体见: http://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns 统计出现频率 方法1: _hkhist=hk.groupby(groups).count().ix[:,0]#count of groupes 方法2: hk.groupby('dep').size() 方法3: (只适用于一列的情况) hk.dep.value_counts() 把一列index转为column(不再作为index使用) http://stackoverflow.com/questions/20461165/how-to-convert-pandas-index-in-a-dataframe-to-a-column 比如, 原来的dataframe是三层index的, column只有一列(名字叫做'0'): df.reset_index(level=2,inplace=True) 这样就可以把第三层的内容作为使用, 而不是作为index, 现在column有两列了, 再给两列命名一下: hist_hub.columns = ['hub','occurrence'] 就得到了: 关于level这个参数: level : int, str, tuple, or list, default None Only remove the given levels from the index. Removes all levels by default Plotting http://cloga.info/python/2014/02/23/Plotting_with_Pandas/ 统计出现次数, 画柱状图: g=hk.groupby('dep') dd=g['dst'].count() dd.plot(kind='bar') 或者用pandas提供的: http://pandas.pydata.org/pandas-docs/stable/basics.html#value-counts-histogramming-mode nb=hk['#vol_hacker'] hist=nb.value_counts()*100.0/len(hk) hist=hist.sort_index() hist.plot(kind='bar') 积累分布曲线 http://stackoverflow.com/questions/6326360/python-matplotlib-probability-plot-for-several-data-set counts, start, dx, _ = scipy.stats.cumfreq(data, numbins=20) x = np.arange(counts.size) * dx + start plt.plot(x, counts, 'ro') 或者用pandas提供的东西也能做吧: http://pandas.pydata.org/pandas-docs/stable/basics.html#discretization-and-quantiling hist2d 用pcolormesh http://www.physicsforums.com/showthread.php?t=653864 貌似要转置!! http://stackoverflow.com/questions/24791614/numpy-pcolormesh-typeerror-dimensions-of-c-are-incompatible-with-x-and-or-y","tags":"tech","title":"pandas学习笔记"},{"url":"http://x-wei.github.io/Eclipse快捷键总结.html","text":"Eclipse让我很喜欢的最大原因大概就是它的好用的快捷键了, 这里进行一下总结, 掌握这些快捷键可以让编辑代码变得更加高效. 基本快捷键 格式化代码: ctrl+alt+F 非常实用的快捷键, 暂时还不知道别的编辑器还没有发现有同样功能的. 按一下代码就自动缩进得非常整齐了! 注释/反注释: ctrl+alt+C pydev下面的注释/反注释: ctrl+/ 另外一些有用的快捷键 显示提示: alt+/ 这个可以说是eclipse快捷键里面最有用的一个. 可以显示代码提示的窗口, 这也是我喜欢eclipse的原因之一. 虽然编写代码的时候eclipse的提示框也会在适当的时候出来, 比如按下了 . 的时候, 但是当它没有出现的时候我们总可以按下 alt+/ 让提示框弹出来. 尤其在使用Java的时候, eclipse的提示相当智能, 也就是说会根据上下文以及函数的定义等东西来提示那些可能出现的项目. 另外, 有时候可能定义了一个超长名字的变量(这样代码可能比较清楚一些), 然后用这个快捷键, 就只需要打一下这个很长名字的前几个字母就可以自动补全了(如果补全的不是想要的, 只需要多按几次这个快捷键就会显示其他可能了)! 打开函数/变量的声明: F3 比方说在程序的某一段看到了调用一个函数, 然后想看看这个函数的内容, 这是不需要拿着鼠标上下滚动着找(有时候定义的函数可能在另一个文件里, 那就更难找了), 只需要按下 F3 , 就会自动跳转到函数的声明的代码片段里. 运行程序: ctrl+F11 显示eclipse提供的解决方案: ctrl+1 比如说, 有时候可能一个变量的名字拼错了, 或者是忘记了导入相应的一个package, 所以在变量下面有红线. 而这时其实eclipse可以提供一些解决方案, 比如修改拼写或者import package, 要查看eclipse提供的解决方案, 只需要把光标移动到画红线的部分, 然后按一下 ctrl+1 就好了. 其他一些快捷键 复制上一行代码: ctrl+alt+down 在写那些好几行比较相似的代码时有点用处. 不过linux的用户有些可能不能用 — 和那个跳到下一个工作区的快捷键重复了... 跳转到下一个/上一个函数/类: ctrl+shift+down/up 现在光标在某一个函数里, 想跳到下一个函数的时候, 按半天的向下按钮不如这个来得快~ 自动导入package: ctrl+shift+o 比如写Java代码的时候, 开始没有import LinkedList 这个包, 写到一半突然要用到了, 那么先写上, 然后不用再回到开头加上导入LinkedList的语句, 而只要按一下这个组合键就可以了. 显示outline: ctrl+o 按下以后, 显示outline视图, 这个视图上看类的成员和函数很一目了然 重命名: shift+alt+r eclipse的重命名和简单的查找替换还是不一样的, 它可以在替换正确的那些命名而不是全部查找替换. 比如重命名一个函数内部的变量的话, 外部即使有重名的也不会影响到. 要是这个快捷键记不住, 就用menu(键盘右边的ctrl的左边)==> refactoring... 查找下一处出现的地方: ctrl+k 有时候也用得着.","tags":"soft","title":"Eclipse快捷键总结"},{"url":"http://x-wei.github.io/Eclipse插件推荐.html","text":"Eclipse被称为编程的瑞士军刀, 意思就是只用一个eclipse就可以做很多事情. 最近越来越喜欢eclipse了, 因为作为编辑器而言, eclipse的编辑器让我用起来最舒服的一个(配合一些 快捷键 以后更是非常爽). 这半年多来发现了一些非常好用的eclipse的插件, 有了这些插件, eclipse可以做更多的事情... 接下来简单介绍一下: 插件安装的方法 先简单说一下eclipse的插件安装方法, 在eclispe界面上, 点击: Help->Install New Software: 之后添加对应的URL就可以了... ADT 链接 eclipse里添加的URL: https://dl-ssl.google.com/android/eclipse/ 这个不用说了吧, Eclipse已经基本上是android开发的御用IDE了. PyDev 链接 URL: http://pydev.org/updates python的开发更需要有一个提示的东东(好像叫pyLint, 就是可以发现代码里的一些显然的错误), 这个插件装好以后我就很少用geany写python程序了, 尤其是当程序超过100行以后... 但是让人不爽的是, 他需要新建一个project才能运行. 这也是Eclipse的一个让人不爽的方面. 如果是一个小的python程序的话我还是会用geany. 另外, 强烈推荐 bpython 作为python的terminal, 非常爽快!! color theme 链接 URL: http://eclipse-color-theme.github.com/update 修改配色的一个插件, 非常喜欢, 因为默认的配色看的时间长了眼睛就不舒服... 有多种配色方案可选. jigloo 这是当年学java GUI编程的时候用到的, 可以拖放着来写Swing/AWT的GUI代码... 有点像现在ADT设计界面的东西... 不过我不喜欢写java的GUI... 乱糟糟的... 做这个的机构已经很久没有更新了... 不过2010年的版本现在用还是没有问题的. greenUML 这个之前的 帖子 已经介绍了, 写报告的时候可以用一用... xmind 链接 这个\"插件\"已经作为一个独立的软件了... 用来画思维导图的工具, 蛮好用的. 作为一个独立的软件下载的话, 去 这里 . TeXlipse 链接 URL: http://texlipse.sourceforge.net/ 没错! Eclipse也可以写tex的... 不过这个我装了但是没有用(因为发现它的时候报告已经写好了...). 貌似也可以实时预览的, 再加上eclipse的编辑器的提示功能, 应该不错! 设置代理 X是个神奇的学校, 所以上网要设置代理..... 关于设置代理的问题在之前一篇帖子里提到了, 粘过来: window → preference → General → Network Connections 插件卸载的方法 说不定哪一天也会需要卸载插件, 卸载的方法是 : Help--> About Eclipse Platform--> Installation Details 最后再补充一句, Eclipse的最大优势是编辑器非常讨我喜欢, 但是缺点是在老机器上运行比较卡, 而且不管写什么都要先指定workspace以及建立一个project. 如果想要一把轻量级的\"瑞士军刀\"的话, geany是个不错的选择.","tags":"soft","title":"Eclipse插件推荐"},{"url":"http://x-wei.github.io/python pickle 的一个小问题.html","text":"python的pickle/unpickle机制可以非常方便的保存一些计算的中间结果, 这一点java虽然也可以做到, 但是java里面的包的名字实在是长的让人记不住... 不过今天在使用pickle的时候遇到了一个很奇怪的问题. 是这样的, 原本写了一个程序 main.py , 这个程序里进行了一些计算并且pickle下了这些内容, 后来我觉得一个程序main.py写这么多实在太长了, 于是就把那些辅助函数以及class的定义通通放进了一个 util.py 文件里. 并且在main.py的第一行写上: from util import * 按理说这应该没有问题, 和一个main文件时运行的效果相同的, 但是当我运行的时候却显示util.py里面这行unpickle的语句有错误: airport_info = pk . load ( file ( 'airport_info.dict' , 'rb' )) >> AttributeError : 'module' object has no attribute 'Airport' 其中 Airport 是我定义的一个类, 本来在main.py里面, 后来被我移动到了util.py里面... 感觉很奇怪, 于是去 水源 求助, 果然fcfarseer学长就很快给了 回复 : 在pickle一個對象的時候，pickle會記住這個對象的class是定義在哪個python 源文件裏，然後再unpickle的時候，pickle會自動import那個源文件以獲得class的定義。 所以如果定義class的文件在這期間改過的話，就會拋出類似的錯誤。 所以问题出在这里(我的理解): 原先我把数据pickle进文件的时候, Airport这个class是定义在了main.py里面, 所以当我在util.py里面load数据的时候, pickle发现原来的main.py里面已经没有了 Airport这个class, 于是就出现了Error... 解决办法也不难, 只需要在 util.py 里面再生成一下那些要load的数据文件, 之后再次unpickle的时候就会去 util.py 而不是 main.py 里找class的定义, 也就没有问题了! 今天碰到的这个问题不是那么evident, 所以特地记一下.","tags":"tech","title":"python pickle 的一个小问题"},{"url":"http://x-wei.github.io/使用Eclipse的UML插件生成类图.html","text":"Created samedi 31 mai 2014UML就是可以把程序的结构用图的形式表达出来的东西(好像叫类图), 虽然写程序的时候不大会用到这种东西来搞, 但是写报告的时候如果能够加上一张图的话, 就可以少费些口舌来解释代码了, 而且还有一种高大上的赶脚... 所以写完程序写报告的时候可以用一下. 废话不多说, 看看我最后生成的UML图: 这张图表示一个抽象类 Operration 有三个子类, 然后他们之间的关系... 如果用文字的话要解释半天吧... 这张图是用 Green UML 做出来的, 这是一个eclipse插件, 安装方法为: 在eclipse里, Help->Install New Software 然后Add这个URL: http://www.cse.buffalo.edu/faculty/alphonce/green 然后一路Next安装就可以了... 如果老师显示pending, 可能是代理的问题(在X非常不爽的一点...哎...), 不过没事, 代理的设置在: window → preference → General → Network Connections , 填上就应该好了... 用Green UML生成类图的时候, 在java文件上点击右键, 选项里有Green UML的选项: 然后就OK了... 另外还在网上找到了 model goon , 不过它生成的类图貌似没有把所有信息都标上, 而且不太好看...","tags":"soft","title":"使用Eclipse的UML插件生成类图"},{"url":"http://x-wei.github.io/fr-input.html","text":"Well, to be brief... I made a french input method, if you got annoyed by the freaking french keyboard when typing accented words (i.e. é, û, ü, î,... ), or you are using a QWERTY keyboard, this will be quite useful ! In fact, I used XiaoxiaoInput and Fcitx , which are 2 excellent Chinese input methods, I just made a french word table, and added it to the program... screenshot: linux version: windows version: how to use download the zip file (see the links below) unzip the file if you are using 32-bit system, cilck the /yong.exe, if you in 64-bit system, click /w64/yong.exe you can see a round french flag icon in the right-bottom corner click the icon, or press ctrl+space , you can see the icon become brighter, which means the input method is active in the meantime, you can also see a small panel appear in the right-bottom corner with a french flag at left: you can type french with the input method now, enjoy ! choose the words by pressing the corresponding number button , or press space key to choose the first candidate sometimes there are >9 candidates, to see more candidates, use -/= to flip pages forward and backward if you want to input what you type instead of the candidates in the list, just press Enter button if you want to change to normal keyboard, you can press LeftShift , and you can see that the icon in the panel become a icon of a keyboard: or you can just press ctrl+space , to disable the input method (then the round flag icon become dark again, indicating it's not active) or you can right-click the round flag icon to totaly exit the program: dowload links windows (adapted from Xiaoxiao Input) link1 link2 linux . (adapted from Fcitx) see here mac I haven't made the mac version yet...","tags":"soft","title":"A French Input Method"},{"url":"http://x-wei.github.io/byte_of_python笔记.html","text":"据说这本书是最好的入门读物, 况且只有100来页 (减掉前面后面那些扯淡的 不到100页...) 那就用这本书过一下py的基本知识点吧! 看完以后收获不少, 把py涉及的很大一部分都讲到了. 这本书已经是够压缩的了, 不过我还是边看边自己再压缩了一遍(写在zim笔记里). 我看的是1.20版本, 2004年的, 因为这个版本针对的是py2.x, 作者主页上现在的版本针对的是py3. 另外感觉没必要看中文翻译版, 因为这里用的英语比较简单, 而且有的时候中文翻译反而不如原文表达的恰当. preface+ch1+ch2 扯淡... ch3. First Steps There are two ways of using Python to run your program - using the interactive interpreter prompt or using a source file. Anything to the right of the # symbol is a comment. the shebang line - whenever the first two characters of the source file are #! followed by the location of a program, this tells your Linux/Unix system that this program should be run with this interpreter when you execute the program. (Note that you can always run the program on any platform by specifying the interpreter directly on the command line such as the command python helloworld.py .) use the built-in help functionality. or example, run help(str) - this displays the help for the str class which is used to store all text (strings) that you use in your program. ch4. The Basics Literal Constants It is called a literal because it is literal - you use its value literally. ex. number 2, or string \"hello\". number Numbers in Python are of four types - integers, long integers, floating point and complex numbers. -Examples of floating point numbers (or floats for short) are 3.23 and 52.3E-4. The E notation indicates powers of 10. In this case, 52.3E-4 means 52.3 * 10-4. -Examples of complex numbers are (-5+4j) and (2.3 - 4.6j) string string可以用Single/Double/Triple Quotes括起来 escape sequence : \\', \\n, \\t, 以及在行末作为续行符号 raw string : to specify some strings where no special processing such as escape sequences are handled, then what you need is to specify a raw string by prefixing r or R to the string. ex. r\"Newlines are indicated by \\n\" unicode text: prefix u or U. For example, u\"This is a Unicode string.\" Remember to use Unicode strings when you are dealing with text files, especially when you know that the file will contain text written in languages other than English. Strings are immutable: once you have created a string, you cannot change it. String literal concatenation: If you place two string literals side by side, they are automatically concatenated by Python. For example, ' What\\'s' 'your name? ' is automatically converted in to \"What's your name?\". Note for Regular Expression Users: Always use raw strings when dealing with regular expressions. Otherwise, a lot of backwhacking may be required. Variables 顾名思义就是可以可以变的量... Unlike literal constants, you need some method of accessing these variables and hence you give them names . Identifier(标示符) Identifiers are names given to identify something. The first character of the identifier must be a letter of the alphabet (upper or lowercase) or an underscore ('_') . Objects Python refers to anything used in a program as an object. Python is strongly object-oriented in the sense that everything is an object including numbers, strings and even functions . Variables are used by just assigning them a value. No declaration or data type definition is needed/used. Logical and Physical Lines: Implicitly, Python encourages the use of a single statement per line which makes code more readable. If you want to specify more than one logical line on a single physical line, then you have to explicitly specify this using a semicolon (;) explicit line joining: ex. 续行符\\; implicit line joining: ex. 括号... Indentation Leading whitespace (spaces and tabs) at the beginning of the logical line is used to determine the indentation level of the logical line, which in turn is used to determine the grouping of statements. This means that statements which go together must have the same indentation. Each such set of state- ments is called a block . Do not use a mixture of tabs and spaces for the indentation as it does not work across different platforms properly. ch5. Operators and Expressions expressions An expression can be broken down into operators and operands . 一些oprators: **, //, <<, >>, &, |, &#94;, ~, not, and, or Operator Precedence: 优先级的一个表... Associativity: Operators are usually associated from left to right i.e. operators with same precedence are evaluated in a left to right manner. For example, 2 + 3 + 4 is evaluated as (2 + 3) + 4 . Some operators like assignment operators have right to left associativity i.e. a = b = c is treated as a = (b = c) . ch6. Control Flow if if-elif-else statement: This makes the program easier and reduces the amount of indentation required. There is no switch statement in Python: You can use an if..elif..else statement to do the same thing (and in some cases, use a dictionary to do it quickly) while Remember that you can have an else clause for the while loop . for -The for..in statement is another looping statement which iterates over a sequence of objects i.e. go through each item in a sequence, a sequence is just an ordered collection of items. -optional else part also. break to break out of a loop statement i.e. stop the execution of a looping statement, even if the loop condition has not become False or the sequence of items has been completely iterated over. -An important note is that if you break out of a for or while loop, any corresponding loop else block is not executed. continue used to tell Python to skip the rest of the statements in the current loop block and to continue to the next iteration of the loop. ch7. Functions Functions are reusable pieces of programs. def func_name() parameters: Note the terminology used - the names given in the function definition are called parameters(行参) whereas the values you supply in the function call are called arguments(实参) . scope local variables: All variables have the scope of the block they are declared in starting from the point of definition of the name. global variables : If you want to assign a value to a name defined outside the function, then you have to tell Python that the name is not local, but it is global. We do this using the global statement. Default Argument Values Default Argument Values默认参数 You can specify default argument values for parameters by following the parameter name in the function definition with the assignment operator (=) followed by the default value. Note that the default argument value should be immutable. you cannot have a parameter with a default argument value before a parameter without a default argument value in the order of parameters declared in the function parameter list. This is because the values are assigned to the parameters by position . For example, def func(a, b=5) is valid, but def func(a=5, b) is not valid. Keyword Arguments If you have some functions with many parameters and you want to specify only some of them, then you can give values for such parameters by naming them - this is called keyword arguments - we use the name (keyword) instead of the position to specify the arguments to the function. return used to return from a function i.e. break out of the function. We can optionally return a value from the function as well. return None -a return statement without a value is equivalent to return None . None is a special type in Python that represents nothingness. For example, it is used to indicate that a variable has no value if it has a value of None. -Every function implicitly contains a return None statement at the end unless you have written your own return statement. pass the pass statement is used in Python to indicate an empty block of statements. DocStrings A string on the first logical line of a function is the docstring for that function (also apply to modules and classes). func.__doc__ The convention: a multi-line string where the first line starts with a capital letter and ends with a dot. Then the second line is blank followed by any detailed explanation starting from the third line. ch8. Modules A module is basically a file containing all your functions and variables that you have defined . To reuse the module in other programs, the filename of the module must have a .py extension. ex. sys module When Python executes the import sys statement, it looks for the sys.py module in one of the directores listed in its sys.path variable. If the file is found, then the statements in the main block of that module is run and then the module is made available for you to use. The sys.argv variable is a list of strings, contains the list of command line arguments i.e. the arguments passed to your program using the command line. 即程序执行时传给的参数列表. The sys.path contains the list of directory names where modules are imported from. Observe that the first string in sys.path is empty - this empty string indicates that the current directory is also part of the sys.path which is same as the PYTHONPATH environment variable. This means that you can directly import modules located in the current directory. Otherwise, you will have to place your module in one of the directories listed in sys.path . Byte-compiled .pyc files Importing a module is a relatively costly affair. This .pyc file is useful when you import the module the next time from a different program - it will be much faster since part of the processing required in importing a module is already done. Also, these byte-compiled files are platform-independent. from..import If you want to directly import the argv variable into your program (to avoid typing the sys. everytime for it), then you can use the from sys import argv statement. not recommended... __name__ Every Python module has it's __name__ defined and if this is ' __main__ ', it implies that the module is being run standalone by the user and we can do corresponding appropriate actions. Every Python program is also a module. You just have to make sure it has a .py extension. dir() function You can use the built-in dir function to list the identifiers that a module defines. The identifiers are the functions, classes, variables and imported modules defined in that module. When you supply a module name to the dir() function, it returns the list of the names defined in that module. When no argument is applied to it, it returns the list of names defined in the current module. ch9. Data Structures Data structures are structures which can hold some data together. In other words, they are used to store a collection of related data. 3 built-in data structures in Python - list, tuple and dictionary . List [a,b,c] a data structure that holds an ordered collection of items. a mutable data type you can add any kind of object to a list including numbers and even other lists. methods: indexing operator: a_list[1] len(a_list) a_list.append() for..in loop to iterate through the items of the list a_list.sort() : this method affects the list itself and does not return a modified list del a_list[0] Tuple (a,b,c) Tuples are just like lists except that they are immutable Tuples are usually used in cases where a statement or a user-defined function can safely assume that the collection of values (i.e. the tuple of values) used will not change. can contain another tuple, another list...... singleton: t=(2,) (comma is necessary!) empth: t=() methods: indexing: a_touple[0] len(a_tuple) used for output format: print '%s is %d years old' % (name, age) Dictionary key-value mapping you can use only immutable objects (like strings) for the keys of a dictionary but you can use either immutable or mutable objects for the values of the dictionary. (This basically translates to say that you should use only simple objects for keys.) 一个dict中的keys不必同样type, values也是! key/value pairs in a dictionary are not ordered in any manner. instances/objects of the dict class. methods: adding key-value pair by indexing: dic[key]=val (overwrite if key already exists!) deleting: del dic[key] (KeyError if key doesn't exist!) dic.items() 返回一个list of tuples : dic.items() [(k1,v1), (k2,v2)] for k,v in dic.items: print k, v dic.keys() 返回keys的list test: the in operator: if akey in dic or even the has_key method of the dict class: if dic.has_key(k) Sequences Lists, tuples and strings are examples of sequences Two of the main features of a sequence is the indexing operation which allows us to fetch a particular item in the sequence directly and the slicing operation which allows us to retrieve a slice of the sequence i.e. a part of the sequence. The great thing about sequences is that you can access tuples, lists and strings all in the same way! indexing(seq can be List or Tuple or String): seq [2], seq[-1] slicing seq [1:3] (from 1 to 2!) seq[:] (a whole copy of the list) References What you need to remember is that if you want to make a copy of a list or such kinds of sequences or complex objects (not simple objects such as integers), then you have to use the slicing operation( list[:] ) to make a copy. If you just assign the variable name to another name, both of them will refer to the same object and this could lead to all sorts of trouble if you are not careful. String methods: str.startswith('a') return boolean str.find(substr) return index of subster or -1 if not found substr in str return boolean str.join(strseq) use str as delimiter to joint the items in strseq ch10. Problem Solving - Writing a Python Script \"a program which creates a backup of all my important files\" 1st version Run the command using the os.system function which runs the command as if it was run from the system i.e. in the shell - it returns 0 if the command was successfully, else it returns an error number. source = ['/home/swaroop/byte', '/home/swaroop/bin'] target_dir = '/mnt/e/backup/' target = target_dir + time.strftime('%Y%m%d%H%M%S') + '.zip' zip_command = \"zip -qr '%s' %s\" % (target, ' '.join(source)) if os.system(zip_command) == 0: print 'Successful backup to', target else: print 'Backup FAILED' 2nd version using the time as the name of the file within a directory with the current date as a directory within the main backup directory. if not os.path.exists(today): os.mkdir(today) # make directory ... target = today + os.sep + now + '.zip' os.sep variable - this gives the directory separator according to your operating system i.e. it will be '/' in Linux, Unix, it will be '\\' in Windows and ':' in Mac OS. 3rd version attaching a user-supplied comment to the name of the zip archive. comment = raw_input('Enter a comment --> ') if len(comment) == 0: # check if a comment was entered target = today + os.sep + now + '.zip' else: target = today + os.sep + now + '_' + \\ comment.replace(' ', '_') + '.zip' More Refinements allow extra files and directories to be passed to the script at the command line. We will get these from the sys.argv list and we can add them to our source list using the extend method provided by the list class. use of the tar command instead of the zip command. One advantage is that when you use the tar command along with gzip, the backup is much faster and the backup created is also much smaller. If I need to use this archive in Windows, then WinZip handles such .tar.gz files easily as well. tar = 'tar -cvzf %s %s -X /home/swaroop/excludes.txt' % (target, ' '.join(srcdir)) The most preferred way of creating such kind of archives would be using the zipfile or tarfile module respectively. \"Software is grown, not built\" ch11. Object-Oriented Programming fields, methods class: fields , methods Fields are of two types - they can belong to each instance/object of the class or they can belong to the class itself. They are called instance variables and class variables respectively. ou must refer to the variables and methods of the same object using the self variable only. This is called an attribute reference . we refer to the class variable as ClassName.var and not as self.var . self Class methods have only one specific difference from ordinary functions - they must have an extra first name that has to be added to the beginning of the parameter list , but you do do not give a value for this parameter when you call the method, Python will provide it. create an object/instance of this class using the name of the class followed by a pair of parentheses. The init method The __init__() method is run as soon as an object of a class is instantiated. The method is useful to do any initialization you want to do with your object. analogous to a constructor in C++, C# or Java. the same, __ del__() method: run when the object is no longer in use and there is no guarantee when that method will be run. If you want to explicitly do this, you just have to use the del statement. All class members (including the data members) are public and all the methods are virtual in Python. One exception: If you use data members with names using the double underscore prefix such as __privatevar , Python uses name-mangling to effectively make it a private variable. Inheritance ex: class Teacher ( SchoolMember ):// '''Represents a teacher.''' def __init__ ( self , name , age , salary ): SchoolMember . __init__ ( self , name , age ) self . salary = salary print '(Initialized Teacher: %s)' % self . name To use inheritance, we specify the base class names in a tuple following the class name in the class definition. -- multiple inheritance. the __init__ method of the base class is explicitly called using the self variable so that we can initialize the base class part of the object. This is very important to remember - Python does not automatically call the constructor of the base class, you have to explicitly call it yourself. ch12. Input/Output Files open and use files for reading or writing by creating an object of the file class and using its read , readline or write methods appropriately to read from or write to the file. Then finally, when you are finished with the file, you call the close method to tell Python that we are done using the file. f = file('poem.txt', 'w') # open for 'w'riting f.write(poem) # write text to file f.close() # close the file f = file('poem.txt') # if no mode is specified, 'r'ead mode is assumed by default while True: line = f.readline()# This method returns a complete line including the newline character at the end of the line. if len(line) == 0: # Zero length indicates EOF break print line, # Notice comma to avoid automatic newline added by Python f.close() # close the file Pickle Python provides a standard module called pickle using which you can store any Python object in a file and then get it back later intact. This is called storing the object persistently. There is another module called cPickle which functions exactly same as the pickle module except that it is written in the C language and is (upto 1000 times) faster. pickling & unpickling: import cPickle as p f = file ( shoplistfile , 'w' ) p . dump ( shoplist , f ) f . close () f = file ( shoplistfile ) storedlist = p . load ( f ) print storedlist To store an object in a file, first we open a file object in write mode and store the object into the open file by calling the dump function of the pickle module. This process is called pickling . Next, we retrieve the object using the load function of the pickle module which returns the object. This process is called unpickling . ch13. Exceptions Try..Except We can handle exceptions using the try..except statement. We basically put our usual statements within the try-block and put all our error handlers in the except-block. ex import sys try : s = raw_input ( 'Enter something --> ' ) except EOFError : print ' \\n Why did you do an EOF on me?' sys . exit () # exit the program except : print ' \\n Some error/exception occurred.' # here, we are not exiting the program print 'Done' The except clause can handle a single specified error or exception, or a parenthesized list of errors/exceptions. If no names of errors or exceptions are supplied, it will handle all errors and exceptions. If any error or exception is not handled, then the default Python handler is called which just stops the execution of the program and prints a message. You can also have an else clause associated with a try..catch block. The else clause is executed if no exception occurs. Raising Exceptions using the raise statement. You also have to specify the name of the error/exception and the exception object that is to be thrown along with the exception. The error or exception that you can arise should be class which directly or indirectly is a derived class of the Error or Exception class respectively. ex. class ShortInputException ( Exception ): '''A user-defined exception class.''' def __init__ ( self , length , atleast ): Exception . __init__ ( self ) self . length = length self . atleast = atleast try: s = raw_input ( 'Enter something --> ' ) if len ( s ) < 3 : raise ShortInputException ( len ( s ), 3 ) # specify the name of the error/exception and the exception object that is to be thrown except EOFError: print '\\nWhy did you do an EOF on me?' except ShortInputException , x : print 'ShortInputException: The input was of length %d, \\ was expecting at least %d' % ( x . length , x . atleast ) else: print 'No exception was raised.' Try..Finally What if you were reading a file and you wanted to close the file whether or not an exception was raised ? before the program exits, the finally clause is executed and the file is closed. ch14. The Python Standard Library sys module sys.argv there is always at least one item in the sys.argv list which is the name of the current program being run and is available as sys.argv[0] . Other command line arguments follow this item. sys.exit : to exit the running program. os module os.getcwd() gets the current working directory i.e. the path of the directory from which the curent Python script is working. os.listdir() os.remove() os.system() : run a shell command. os.linesep : string gives the line terminator used in the current platform. os.path.split() : returns the directory name and file name of the path. os.path.isfile() and os.path.isdir() ch15. More Python Special Methods Generally, special methods are used to mimic certain behavior. For example, if you want to use the x[key] indexing operation for your class (just like you use for lists and tuples) then just implement the __getitem__() method and your job is done. __init__(self, ...) __del__(self) __str__(self) Called when we use the print statement with the object or when str() is used. __lt__(self, other) Called when the less than operator ( < ) is used. Similarly, there are special methods for all the operators (+, >, etc.) __getitem__(self, key) Called when x[key] indexing operation is used. __len__(self) Called when the built-in len() function is used for the sequence object. List Comprehension used to derive a new list from an existing list. ex listone = [2, 3, 4] listtwo = [2*i for i in listone if i > 2] Here, we derive a new list by specifying the manipulation to be done (2*i) when some condition is satisfied (if i > 2). Receiving Tuples and Lists in Functions receiving parameters to a function as a tuple or a dictionary using the * or ** prefix respectively. This is useful when taking variable number of arguments in the function. def powersum(power, *args):... Due to the * prefix on the args variable, all extra arguments passed to the function are stored in args as a tuple. If a ** prefix had been used instead, the extra parameters would be considered to be key/value pairs of a dictionary. Lambda Forms A lambda statement is used to create new function objects and then return them at runtime . ex. def make_repeater(n): return lambda s: s * n twice = make_repeater(2) print twice('word') print twice(5) output: $ python lambda.py wordword 10 A lambda statement is used to create the function object . Essentially, the lambda takes a parameter followed by a single expression only which becomes the body of the function and the value of this expression is returned by the new function. Note that even a print statement cannot be used inside a lambda form, only expressions . The exec and eval statements The exec statement is used to execute Python statements which are stored in a string or file. The eval statement is used to evaluate valid Python expressions which are stored in a string. The assert statement to assert that something is true. For example, if you are very sure that you will have at least one element in a list you are using and want to check this, and raise an error if it is not true, then assert statement is ideal in this situation. When the assert statement fails, an AssertionError is raised. The repr function or Backticks(`) to obtain a canonical string representation of the object. you will have eval(repr(object)) == object most of the time. Basically, the repr function or the backticks are used to obtain a printable representation of the object. can control what your objects return for the repr function by defining the __ repr__ method in your class.","tags":"tech","title":"A byte of Python 笔记"},{"url":"http://x-wei.github.io/linux下安装并使用java开发opencv的配置.html","text":"今天花了四个小时, 终于在linux下把eclipse下java开发opencv给搞定了... 至于为什么花这么久的时间... 且听我慢慢讲... linux编译安装opencv 首先, linux下安装opencv其实不麻烦的, 参考文档即可完成: http://docs.opencv.org/trunk/doc/tutorials/introduction/linux_install/linux_install.html 大致有以下几个步骤: 1) 安装gcc以及cmake等等乱七八糟的软件(不过ubuntu下默认差不多都有了吧...) sudo apt-get install build-essential python-dev cmake 2) 下载opencv-2.4.8.zip并解压缩 3) 新建一个build文件夹 cd ~/opencv mkdir build cd build 4) 在终端里输入: cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. make sudo make install make命令会花费几十分钟时间... 然后 理论上 就结束了... 如果不用java开发的话 就OK了 比如 可以运行sample文件夹下的python代码... 但是 , 如果要使用java开发的话 需要接着这样做..... 安装opencv-java 这一步非常DT, 因为官网上还有一个文档, 专门讲怎么用java开发的... http://docs.opencv.org/doc/tutorials/introduction/desktop_java/java_dev_intro.html 1) 首先, 需要安装ant: sudo apt-get install ant 2) 其次 需要运行cmake, 关键是cmake的参数, 官网给的是这样的: export JAVA_HOME=/usr/lib/jvm/java-6-oracle#这句不加应该也可以的 cmake -DBUILD_SHARED_LIBS=OFF .. 注意看输出的, 如果\"To be built\"里面有java这一项的话, 就 应该 是OK的.... 接下来只要 make -j8 就可以了... 最坑爹的地方来了, 以这样的参数运行cmake以后, 运行 make -j8 总是不成功, 说有错误..... 查了半天, 发现这是一个BUG: http://code.opencv.org/issues/2859 那么试一试加上参数: -DBUILD_TESTS=OFF 吧... 所以cmake的参数要这样写才可以 : cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -DBUILD_TESTS=OFF .. 3) 用上面那样的参数运行cmake以后, 再: make -j8 sudo make install 这样就可以成功make了... 如何判断是否安装成功? 以上操作结束后, 看看build文件夹, bin目录下是不是有一个 opencv-248.jar 文件: 如果有的话 就说明成功安装了opencv的java组件... !!注意!! 不能用windows下opencv安装目录下的那个opencv-248.jar !! 刚才生成那个jar和windows下的jar不一样的!! (我查了MD5码... 其实看文件大小也能看出来!!) Eclipse下的配置 这里有一篇非常好的帖子, 写的非常到位, 我就不重复了, 链接: http://exintopro.net/blog/2013/10/02/setting-up-eclipse-for-using-opencv-java-in-ubuntu/ 这个帖子虽然很好, 但是没有介绍编译的那一部分, 由于安装java组件时的编译过程和官网给的不一样, 所以才导致我废了半天的时间啊!......","tags":"tech","title":"linux下安装并使用java开发opencv的配置"},{"url":"http://x-wei.github.io/java线程小结.html","text":"INF422的TD2, 这节TD是要做一个多线程筛选质数的程序, 关于java的多线程, 以前用过但是不知道啥意思, 在这里总结下. 创建进程 为了实现多线程, 需要定义一个新的class, 有两种方法: 或者继承自 Thread 类, 或者实现 Runnable 接口 (关键是重载run()方法). 继承自Thread类 写法1: 定义一个继承自Thread的内部类-- class [ 类名 ] extends Thread { 方法1 ; 方法2 ； … public void run (){ // … } 属性1 ； 属性2 ； … } 或者用下面种 内联(inline) 的写法, 不用给这个类起名字了(不过还要给这个实例取名字): private Thread [实例名] = new Thread () { public void run() { // ... } } ; 然后让进程开始, 就是: t.start(); 如果用 t.run() 的话, 则依然是并行执行的, 可能达不到多线程效果... 实现Runnable接口 方法1, 代码: class [ 类名 ] implements Runnable { 方法1 ; 方法2 ； … public void run (){ // other code … } 属性1 ； 属性2 ； … } 方法2, 内联写法: private Runnable [实例名] = new Runnable() { public void run() { //... } }; android上需要注意的一点 \"Android modifies the user interface and handles input events from one single user interface thread. This thread is also called the main thread .\" Android collects all events in a queue and processed an instance of the Looper class. 所以注意要修改用户交互的组件(View, Toast, ect.)的时候, 需要在main Thread里面操作, 否则运行时会出错!! 那么, 当其他线程进行计算完成以后, 要刷新屏幕的显示时, 需要告知main Thread 进行更新显示: \"Ajouter un nouvel objet comportant une méthode run() mettant à jour le nombre d'entier premiers (nouvel objet implémentant l'interface Runnable). À la fin du crible, le thread de calcul devra alors envoyer un message (ce nouvel objet) au thread principal pour mettre à jour l'affichage graphique. Cette mise à jour doit être faite via un appel à la méthode post() d'une instance de la classe Handler (l'objet Handler doit être instancié dans le thread principal).\" Bref, 要做到多线程计算, 计算结果显示在屏幕上, 需要: 添加一个Runnable实例(\"一个实现了Runnable接口的类的实例\"), 重载run()方法实现calcul 一个 Handler 实例, 然后调用这个Handler的post()方法. 看看文档里是咋说的: public final boolean post (Runnable r) Added in API level 1 Causes the Runnable r to be added to the message queue. The runnable will be run on the thread to which this handler is attached. Parameters r The Runnable that will be executed. Returns Returns true if the Runnable was successfully placed in to the message queue. Returns false on failure, usually because the looper processing the message queue is exiting. 意思是 handler.post(r) 会把r(一个Runnable实例)加入message queue中去, 这个Runnable会在这个handler关联的Thread中执行 . 所以只要handler关联的是main Thread, 就可以在这个Runnalbe里面写graphic的代码也不会出错了. 如果在Activity类的声明里声明handler为一个属性: private Handler handler = new Handler(); 这个构造函数没有参数, 根据文档, \"Default constructor associates this handler with the Looper for the current thread.\" 所以这个handler关联到了current thread, 也就是main Thread..... 例子 所以, 多线程计算, 计算结束后修改屏幕显示的话, 需要以下__三个步骤__(比上面的俩步骤多了一个, 不知道是否还可以简化): 在主线程里声明handler, (声明成Activity的一个属性): private Handler handler = new Handler(); 写一个用于修改屏幕显示的Runnable r(也声明成一个属性了): private Runnable r = new Runnable() { public void run() { //code to update graphic display... } }; 再写一个进行计算的Thread t, 在计算结束后, 用handler.post(r)实现刷新显示的效果: private final Thread t = new Thread() { public void run() { //...code for calculating... //结束计算以后, 刷新屏幕: handler.post(r); } }; 然后在onCreate()函数里, 让Thread t 启动起来: t.start(); 需要让t一直循环(监视某个flag), 当flag变为true的时候, 进行计算. 为了达到这个效果, Thread t里面的run()方法需要这样写: public void run() { while(true) { if(flag){ //...do the calculation... handler.post(r); flag=false; } } } Process, Thread和Runnable的区别? 线程(Thread)是指进程(Process)中的一个执行流程，一个进程中可以运行多个线程。比如java.exe进程中可以运行很多线程。线程总是属于某个进程，进程中的多个线程共享进程的内存。 参考链接: http://www.vogella.com/articles/AndroidBackgroundProcessing/article.html http://www.cnblogs.com/rollenholt/archive/2011/08/28/2156357.html http://developer.android.com/reference/android/os/Handler.html http://lavasoft.blog.51cto.com/62575/99150","tags":"tech","title":"java线程小结"},{"url":"http://x-wei.github.io/ADT环境搭建.html","text":"这学期INF422, 第一节课就是android安装调试环境的搭建(居然要一节课?), 这里总结一下. 第一步: 下载bundle 下载 ADT-Bundle 解压缩以后啥都有了, 包含一个eclipse... 第二步: 修改环境变量 linux下的环境变量放在 .bashrc文件 , 加入下面一行: PATH=$PATH:<dir>/sdk/tools/:<dir>/sdk/platform-tools/: 其中 <dir> 是放置SDK的目录地址. 测试一下好不好使, 在终端中输入 android , 看是否会弹出SDK manager 第三步: 新建一个virtual machine 可以用eclipse的AVDmanager做, 也可以用命令行做. 输入: android list targets 找到想要的target(我们要4.1.2)的那个id(一个数字), 然后, 运行命令: android create avd -t <target_id> -n inf422 --abi armeabi-v7a 这样就生成了一个名叫\"inf422\"的虚拟机 第四步: 关联到自定义的镜像 inf422这门课提供了一个修改过的android镜像, 在 这里 和 这里 下载, 下载到本地的目录上了以后, 运行: emulator @inf422 -ramdisk <IMAGES>/ramdisk.img -kernel <IMAGES>/kernel-qemu 其中 是刚刚存放那俩镜像文件的目录. 第五步: telnet连接虚拟机 课程里用的emulator使用telnet服务器, 且是在虚拟机的23端口接收信息. 先重定向一下端口, 定向到localhost的4444端口: adb forward tcp:4444 tcp:23 这样, 以后要登录模拟器emulator的时候, 只需要输入: telnet localhost 4444 (用户名是root, 不要密码) 第六步: 用两种方法进入虚拟机 刚才的4444端口是进入emulator用的, 而打开虚拟机的时候, 窗口标题是一个数字再加虚拟机的名字(我的显示的是\"5554:inf422\"), 这个5554是另外一个端口, 用于用shell方式登陆, 登陆后可以使用shell命令查看文件或者进行一些操作. 而与之对应, 从4444端口登陆, 则是进入emulator的控制console 两种方法和AVD交流: console: linux命令, 命令行 emulator: 发送命令产生一些事件(电话, 短信, GPS等) 第二种方式可以模拟一些手机事件, 很有用... 使用help命令查看emulator怎么使用","tags":"tech","title":"ADT环境搭建"},{"url":"http://x-wei.github.io/android开发教程第一季笔记.html","text":"S01E02: 开发环境搭建 直接去 developer.android.com/sdk/ 下载bundle解压即可, 里面包含eclipse 新建一个模拟器: 内部存储一般64MB足矣: S01E03: android项目目录结构 第一个android程序 new-->android application 目录结构 com.example.helloworld这个包名一般为网址倒着写, android是按照包名来区分不同APP的 src/目录下存放java程序 gen/目录下存放系统自动生成的文件, R.java里给每一个控件或变量赋予一个id, 千万__不要手动修改R,.java内容__!! assets/目录下和res/目录下都可以放各种外部文件(ex. 图片), 但是assets下的文件不会在R.java中生成id res/layout/ 存放布局文件(xml格式) AndroidManifest.xml用于统筹 S01E04: 技术结构 四层结构图 开发位于最上层application 基于组件的应用程序开发 搭积木 常见组件 activity(负责用户交互); service(后台处理数据); Content Provider(对外提供数据); BroadcastReceiver(接收broadcast) S01E05: activity初步 Activity启动基本流程 AndroidManifest.xml里面指定, 程序运行后执行MainActivity.java MainActivity.java的onCreate()函数里setContentView(R.layout.activity_main); 读取activity_main.xml文件并显示 Activity与布局文件 一一对应 (eclipse编辑器里有图形化预览界面) 在Activity当中获取代表控件对象 首先在activity_main.xml里定义控件时, 为其指定一个id: android : id = \"@+id/textView1\" ( \"+id\" 表示新建一个 id ) → 自动在R.java里生成一个id: public static final class id { public static final int textView1=0x7f080003; } → 之后在MainActivity.java里, 先声明成员变量: private TextView tv → 在onCreate()中, 通过id获得这个控件对象: tv = (TextView) findViewById(R.id.textView1);//有向下转型: TextView是View的子类 → 之后就可以调用tv的各种方法了,具体见SDK里的文档i(android.widget.TextView), 从而实现再java程序里动态修改控件属性. S01E06: View View的基本概念 View是一个控件?... 为一个View绑定监听器 (我就认为View是一个控件了) java程序里获取控件代表的对象: findViewById()方法 bt = (Button) findViewById(R.id.button1); 定义一个 内部类 实现监听器接口: // **使用一个内部类定义监听器** // Button的监听器 实现OnClickListener接口, OnClickListener接口是处理点击事件的 class ButtonListener implements OnClickListener { @Override // 实现该接口的抽象方法onClick public void onClick(View v) { //do something...... } }// end 内部类ButtonListener onCreate()里生成一个监听器对象, 并为控件绑定该监听器 ButtonListener bl = new ButtonListener();// 生成一个监听器对象 bt.setOnClickListener(bl);// **这句话把监听器和事件联系在一起了** 点击这个按钮 就会执行它的onclick方法 一个监听器可以绑定给多个控件 另法 可以直接写在setOnClickListener参数里, 不用给内部类起名字: bt.setOnClickListener(new OnClickListener() { @Override public void onClick(View v) { //do something...... } }); S01E07: 布局 所谓的控件布局方法,就是指控制控件在Activity当中的位置、大小、颜色以及其他控件样式属性的方法。 布局可以用xml布局文件(ex. /res/layout/activity_main.xml) 也可以在java文件里完成控件布局 最常用: 线性布局LinearLayout, 相对布局RelativeLayout 先暂时用线性布局LinearLayout, 最简单 S01E08: 距离单位; 边距 距离单位: px, dp, sp px =像素点 使用px指定控件大小, 则不同分辨率手机显示结果不同, 非常麻烦. dpi计算公式: dp =dip(Device Independent pixels) 是设备无关的像素单位 换算公式 px = dp * (dpi / 160) 在320*480的屏幕上, dp与px相等 总之一般指定控件大小就用dp sp =scaled pixels 可改变大小的像素单位 当用户修改手机显示字体时,sp会随之改变 sp单位通常用于指定字体的大小 内外边距: margin, padding margin: 控件离其他控件的距离 padding: 控件内容离控件边框的距离 设置内边距与外边距: layout_margin, layout_marginTop, ... padding, paddingTop, ... S01E09: CheckBox多选框 效果: 布局文件里使用 标签 OnClickListener与OnCheckedChangeListener监听器 注意 这俩监听器是有一定区别的, OnClickListener必须是用户点击才能触发, OnCheckedChangeListener则是只要状态改变了(即使是由程序里改变的)就会触发 可以几个CheckBox绑定上同一个监听器 OnClickListener接口的方法: public void onClick(View v) 有一个 参数View v , 指的是是哪个控件被点击了, 在onClick()中要处理这个控件时, 使用: CheckBox cb = (CheckBox) v; 得到这个控件对象, 或者通过其 id: v.getId() 也可以 S01E10: 单选框RadioButton 效果: RadioGroup 单选按钮RadioButton需要放在一个RadioGroup中 xml代码: <RadioGroup android:id= \"@+id/radioGroupId\" android:layout_width= \"0dp\" android:layout_height= \"wrap_content\" android:layout_gravity= \"center\" android:orientation= \"vertical\" android:layout_weight= \"1\" > <RadioButton android:id= \"@+id/radioButton_h\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:text= \"homme\" /> <RadioButton android:id= \"@+id/radioButton_f\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:text= \"femmee\" /> </RadioGroup> <!-- 像RadioGroup这样写的xml标签可以拥有子标签, 而像RadioButton这种则不能 → RadioGroup的OnCheckedChangeListener 有俩参数... (RadioGroup group, int checkedId) 直接上代码吧: class RadioGroupListener implements android . widget . RadioGroup . OnCheckedChangeListener { @Override // 注意看这俩参数 ! public void onCheckedChanged ( RadioGroup group , int checkedId ) {// checkedId是被选择的哪个RadioButton的Id ( 而不是什么序号 ) if ( checkedId == R . id . radioButton_h )// 用两种方法得到想要的id: 1 . 使用R中定义的 { tv3 . setText ( \"Homme!\" ); rb_h2 . setChecked ( true ); } else if ( checkedId == rb_f . getId ())// 2 . 使用getid () 方法 { tv3 . setText ( \"Femme!\" ); rb_f2 . setChecked ( true ); } } } S01E11: 显示图片ImageView 插图的方法 把图片放在./res/drawable 里面(有好几个分辨率的drawable, 随便一个...), ex. 放在了./res/drawable-hdpi/pic.jpg → 会在R.java里生成其 id: pic 在xml中使用 标签 引用放好的图: android:src=\"@drawable/pic\" ScaleType ScaleType属性控制图片缩放的尺寸, 有几个可选: fitcenter等比例缩放并居中显示; fitstart靠上显示; center 若图片较大则把中央部分截取出来显示,不缩放若图片小则直接居中显示, 也不缩放 centerCrop 缩放并进行裁剪以适应ImiageView centerInside 要是图片大就缩放放进去, 图片小就不缩放直接放进去 Fit_XY缩放成和ImageView一样大 代码片段: <ImageView android:id=\"@+id/imageView1Id\" android:layout_width=\"60dp\" android:layout_height=\"70dp\" android:background=\"#FF0000\" android:scaleType=\"fitCenter\" android:layout_weight=\"1\" android:src=\"@drawable/pic\" /> <!-- \"@drawable/pic\"是对应于R.drawable.pic, 实际上图片也可以放在assets文件夹 或者网络上 或者SD卡上 都可以, 不过以后再用 --> S01E12: 线性布局深入 线性布局嵌套 在LinearLayout里面再加入LinearLayout \"直接父/子控件\" 嵌套层数没有限制 layout_weight 子控件并未占满父控件的所有空间时才有用 layout_weight的值用于指定 空闲空间的 分配__比例__: weight都是1的话--是按比例平分父控件的__剩余(!!!)__空间, 而不是整个父控件被按比例分配! 如果想让父控件按比例分配: 很简单, __把宽度改为0dp__即可(那么父控件剩余空间=父控件总空间!) 使用了线性布局嵌套以及weight属性, 已经可以制作一些相对较复杂的布局了 S01E13: 相对布局-I 相对布局def 相对布局是通过指定当前控件__与兄弟控件或者是父控件之间的相对位置__,从而达到控制控件位置的目的  实现同样界面, 用相对布局比用线性布局简单-- UI性能 更好些 UI性能: 布局嵌套越多, 性能越差!!... 基本思路 未指定位置时: 默认往左上角放(可能会重叠!) 可以先放一个控件, 然后第二个控件指定其相对位置 实现方法 android:layout_below/layout_above等等等等... 放置在其左(右)边; 属性的值都是其他控件的id android:id=\"@+id/tvv1\"是创建一个新id; android:layout_toRightOf=\"@id/tvv1\"则是引用已有的id(没有加号) android:layout_alignLeft/Right等等等等..... 左(右)对齐; 属性的值都是其他控件的id  S01E14: 相对布局II 对齐到基准线 基准线:为了保证印刷字母的整齐而划定的线 第三条线就是所谓的基准线(baseline) ex. 两个TextView__的基准线__相互对齐 作用: 当俩TextView的字体大小不相同时... 看图: 和父控件对齐 android:layout_alignParentLeft/Right等等... 属性的值为true/false(因为只有一个直接父控件) android:layout_centerInParent/layout_centerHorizontal等等... S01E15: 相对布局III RelativeLayout布局的新属性(Android 4.2) android:layout_alignStart/End等 值是其他控件的id, 头部和尾部对齐 android:layout_alignParentStart/End, 值是true/false 相对布局小练习 代码片段: <RelativeLayout android:id= \"@+id/RL0\" android:layout_width= \"fill_parent\" android:layout_height= \"fill_parent\" > <TextView android:id= \"@+id/tv1\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_centerHorizontal= \"true\" android:gravity= \"center\" android:text= \"login...\" android:textSize= \"16sp\" /> <EditText android:id= \"@+id/editText1\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_below= \"@id/tv1\" android:layout_centerHorizontal= \"true\" android:ems= \"10\" android:hint= \"username\" > <requestFocus /> </EditText> <EditText android:id= \"@+id/editText2\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_below= \"@id/editText1\" android:layout_centerHorizontal= \"true\" android:ems= \"10\" android:hint= \"password\" android:inputType= \"textPassword\" /> <Button android:id= \"@+id/button1\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_alignParentRight= \"true\" android:layout_below= \"@+id/editText2\" android:text= \"login\" /> <Button android:id= \"@+id/button2\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_below= \"@+id/editText2\" android:layout_toLeftOf= \"@+id/button1\" android:text= \"cancel\" [ /> ](file:///%3E) </RelativeLayout> S01E16: 时间和日期 TimePicker/DatePicker 标签 文档: android.widget.TimePicker/android.widget.DatePicker OnTimeChangedListener的onTimeChanged函数: public void onTimeChanged(TimePicker view, int hourOfDay, int minute) 仨参数 getCurrentHour(); getCurrentMinute(); getMonth(); getDayOfMonth()等函数, 同理有set函数...... 注意月份是从零算起!! setIs24HourView() 切换24小时显示 S01E17: 进度条ProgressBar 各种进度条 进度条的属性 风格Style: 转圈圈: 垂直风格(默认); 一条线的进度条: Horizontal 最大值: max 当前进度:progress 次要进度的值:SecondaryProgress (ex.次要进度: 解压缩文件时, 在线看视频时...) isIndeterminate()断进度条时不时不明确的(打酱油的, 比如转圈圈), 同理有set方法, 不过只能对水平风格适用. S01E18: SeekBar和RatingBar SeekBar 属性: max/progress/ 监听器接口OnSeekBarChangeListener有三个函数要重写: class SeekBarListener implements OnSeekBarChangeListener { /** * seekBar 该对象指的是触发了监听器的SeekBar对象 * progress 指的是当前SeekBar的进度 * fromUser 指是不是用户手动改变的进度 */ @Override public void onProgressChanged ( SeekBar SeekBar , int progress , boolean fromUser ) { System . out . println ( \"progress:\" + progress + \",fromUser:\" + fromUser ); } @Override public void onStartTrackingTouch ( SeekBar seekBar ) { System .out.println ( \"onStart\" ); } @Override public void onStopTrackingTouch ( SeekBar seekBar ) { System .out.println ( \"onStop\" ); } } RatingBar 属性: numStars(星星个数), progress, stepSize(步进) 监听器代码: class RatingBarListener implements OnRatingBarChangeListener { @Override public void onRatingChanged ( RatingBar ratingBar , float rating , boolean fromUser ) { System . out . println ( \"rating:\" + rating + \",fromUser:\" + fromUser ); }","tags":"tech","title":"android开发视频教程(重制版) 第一季 课程笔记"},{"url":"http://x-wei.github.io/我的ubuntu10.04配置总结.html","text":"ubuntu已经出到了13.04, 我之所以坚持使用ubuntu10.04的版本(到现在已经三年了, 现在10.04已经过了支持周期而我还在继续用它), 主要是因为ubuntu出的unity界面以及gnome3的界面实在是用起来不爽(吐槽不已!!)... 目前我的lucid(ubuntu 10.04)经过我的配置, 在我用来已经十分顺手了.. 不过长久这么下去也不是办法...... 在换新的主力系统之前, 把我目前这个系统的配置写下来. 安装的软件 写一下用起来特别爽的一些软件: geany 万能IDE , java/python/Cpp/matlab(octave)... 甚至tex都是用它写的, 轻巧强大. zim桌面维基 神器 , 用来写笔记整理思路, 这篇就是在zim下写的. 自从2012年发现zim这个东西以后, 到现在写了上百条笔记了, 确实方便. GNU octave matlab的开源替代, 语法和matlab完全兼容, 毕设就是用它做的, 如果不用matlab工具箱的话, 这个是很好的选择, 而且比较小巧, 启动很快. ......不过法国人貌似比较喜欢他们自己搞出来的scilab. 音乐播放器audacious 界面和简洁, 用起来蛮好, 不过可能其他播放器也差不到哪里去... 词典goldendict 这个还不是最新的版本(我用的是0.9.0), 已经相当顺手了. 可以自己加stardict的词典文件, 从网上可以找到很多(包括法语的词典), 用起来比stardict方便, 屏幕取词什么的也很好用. GNOME Mplayer 感觉这个界面简洁些, 可能其他播放器(VLC之类)也差不到哪里去. QQ: wine-TM2009 地址在 这里 , 根据我的经验, 别再折腾了, 这个就是最好的解决方案... 配置 zip乱码解决 之前 介绍过 . 外观 globalmenu+windowbuttons 安装这两个插件以后, 可以真的 最大化 利用屏幕空间, 很科学...(貌似是跟苹果学的?) 自定义的主题 Faenza图标主题; elementary窗口主题; ComixCursors鼠标主题; clearlooks控件... 总之这大概就是我觉得最舒服的外观主题了... 边缘触发动作 compiz的设置, 把鼠标移动到左下角会显示桌面, 很方便...(貌似是跟苹果学的?) compiz特效 立方体特效, 开四个桌面(这个其实很实用, 尤其是开十个以上窗口时......); 动画特效, \"对所有事件使用随机动画\"(这个开了以后效果惊艳, 而且即使老电脑开了这个一点也不会卡, 完爆win...); 其他特效还没全开...... 护眼背景色(gnome+chrome) gnome的设置 ; chrome的设置 ; 快捷键 总结一下配置的快捷键: 显示桌面 win+D 打开浏览器 win+B (取\" B rowser\"之意) 关联的命令是 google-chrome 打开文件管理器 win+F (取\" F ile\"之意) 命令: nautilus --browser /home/your_username 打开goldendict ctrl+alt+D 截图 win+S 命令是: gnome-screenshot -ia 这个之前也 写过 .","tags":"tech","title":"我的ubuntu10.04配置总结"},{"url":"http://x-wei.github.io/ssh远程登录学校机房电脑.html","text":"X 机房的电脑 配置还是很高的, 所以... 远程登录的命令是: ssh -X nom.prenom@truite.polytechnique.fr (-X命令表示允许使用X程序.) 登录进去以后, 可以在终端里输入命令, 比如查看系统板本: $ lsb_release -a LSB Version: :core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch Distributor ID: n/a Description: CentOS Release: n/a Codename: n/a 机房是centOS, 高效稳定. 然后可以在终端输入命令来启动程序, 比如 eclipse &, 就会在你这边的电脑显示出来eclipse的窗口 (加\"&\"是可以使多个程序同时启动). 另外, 远程拷贝到本地: scp xing.wei@france.polytechnique.fr:/users/eleves-b/x2012/xing.wei/ /home/wx 几个常用的程序: eclipse & scilab & firefox & 可惜不知道咋能显示远程的桌面, 不过这也差不多够了, 以后用eclipse或者scilab这类比较\"大\"的程序, 直接用机房的工作站做好了...","tags":"soft","title":"ssh远程登录学校机房电脑"},{"url":"http://x-wei.github.io/网页图片自动适应浏览器宽度.html","text":"眼看三月又要结束了, blog还没有更新, 拿来以前写在zim里的一条笔记充数... 有了zim以后, 几乎不再用office了, 常用它编辑一些富文本, 比如加粗, 下划线, 斜体什么的, 有快捷键操作, 很方便, 插图也很简单. zim是我认为的神器(神器=小巧+强大)之一. 有时候用zim写好的笔记给别人看时, 直接按\"ctrl+p\"就可以打印成网页的格式了, 非常方便(而且得承认, 单论富文本的显示效果, 网页格式 > pdf格式 > doc格式). 只是有一点不太好, 就是有时插入的照片很大, 在浏览器里查看的时候显示的还是原尺寸图像, 非常方便... 查了一下, 其实只要在html文件关于插入图片那一行加入 width=\"100%\" height=\"100%\" style=\"z-index:-100\" 就可以了. 比如, 在zim生成的html文件里, 这样替换就可以了: <img src=\"./xxx网页_files/IMG_1186.JPG\" alt=\"\" ><br> ⇒ <img src=\"./xxx网页_files/IMG_1186.JPG\" alt=\"\" width=\"100%\" height=\"100%\" style=\"z-index:-100\"><br> 即进行替换: \" alt=\"\" ⇒ \" alt=\"\" width=\"100%\" height=\"100%\" style=\"z-index:-100\" 这个我倒是时常用到, 比如给家人发最近活动的照片时 , 可以用zim写上一些文字作为照片的注释, 然后生成网页文件然后打包发掉... OK, 水文结束...","tags":"tech","title":"网页图片自动适应浏览器宽度"},{"url":"http://x-wei.github.io/passesimple时态变位总结.html","text":"passé simple 其实一点也不simple... 在Cherbourg时老师直接就没讲, 一是因为这个时态的变位很复杂; 二是现在这个时态几乎不用了, 除了出现在童话故事或人物传记中, 替代passé composé的位置... 可是回到X之后的法语课上却学了... 这周一还有针对passé simple的考试... 这是上周末总结的passé simple 的一些变位规则, 发现其实这个时态的变位也不是很难(原因见下文), 只不过一般不用, 所以容易忘吧~ I. 词尾 passé simple 的变位词尾可以分为种四类型, 我把它们叫做\"A型\", \"I型\", \"U型\", 和\"IN型\", 别看四种类型, 其实很容易记的. 四种\"类型\" 先把这四种类型的词尾写下来: A型 ai - as - a -â mes -â tes - è rent I型 i s -i s -i t -î mes -î tes -i rent U型 u s -u s -u t -û mes -û tes -u rent IN型 in s -in s -in t -în mes -în tes -in rent 看看还是很有规律的嘛, 用X代表\" X类型 \"的词尾的话, 那么词尾可以统一写成: X s -X s -X t -X(加&#94;) mes -X(加&#94;) tes -X rent 简记法: s-s-t是第二组动词的présent变位词尾, 对于nous的变位, 正好是\"som mes \"的词尾, vous正好是\"ê tes \"的词尾, ils的话... 没找到记忆的办法, 就记住吧, 是\"rent\" 而对于A型词尾, 有一些特例, 比如前三个人称是-ai-as-a, 这个其实好记, 因为是动词\"avoir\"的présent变位词尾(与future simple一样), 另外得记住A型的ils变位时不是-Xrent而是-èrent. II. 四种词尾对应的单词 如果能把这四种词尾记住, 而且已经记住了这个单词的过去分词形式(p.p.) , 那么记忆哪个单词对应哪一种词尾其实蛮简单的. 下面看看这四种词尾都对应那些动词: 一). A型(ai-as-a-âmes-âtes-èrent) 对应于: TOUS les verbes au p.p. en -é (sauf être). // 所有过去分词以-é结尾的动词(être除外). 语法书里忘记了naître了... 不过这个应该属于特殊情况... 即对应1er groupe的单词, c'est le plus normal. 例子太多了不列了... 二). I型(is-is-it-îmes-îtes-irent) 对应于以下四类(其实可以说是三类... 其实很好记的...)... 1. la majorité des verbes au p.p. en -i. // 大部分p.p.以-i结尾的动词. 这就是我叫它\"I型\"的另外一个原因!!! 下面的\"U型\"也是同样的! 所以只要记住动词的p.p.(过去分词形式), 改写成passé simple也不麻烦的!~ 这里, 语法书里说的是\"la majorité \", 但是我觉得把它改成\"TOUS\"也没有问题... 2. certain nombre de verbes au p.p. en -u // 一些p.p.以-u结尾的动词 语法书里举出的这些动词有： 1) verbes en -endre, -ondre, -ompre, -ordre, -erdre 总之就是一些 -re 结尾的... 例如: répondre , entendre, éteindre... 2) voir 其实是不规则动词, 在后面还会提到 vis-vis-vit-vîmes-vîtes-virent 3) vaincre vainquis-vainquis-vainquit-vainquîmes-vainquîtes-vainquirent 这个不常用, 干脆可以不记住... 倒不如记一下naître也是I型的: naquis-naquis-naquit-naquîmes-naquîtes-naquirent 3. offrir, ouvrir offrir和ouvrir在p.p.里也属于比较特殊的... 它们的p.p.分别是offert, ouvert: offris-offris-offrit-offrîmes-offrîtes-offrirent 还有特例是-(a)indre, -oindre的, 比如eteindre, 但是这个词变位的词根也变了, 是 eteign- ... 太特殊了又不常用, 所以干脆别记了... 4. p.p.以-is/it结尾的动词 (这一条是我总结的 不一定对) 我发现如果这个动词的p.p.含有- i t/- i s词尾的话, 那么它的passé simple变位就是 I 型的了(所以\"I型\" \"U型\"这个名字起的好啊....). 例子: dire, mettre, prendre 其实第四条可以和第一条合并 : 如果p.p.结尾是-i或-is或-it的话, 词尾就是I型的了~ 唉, 又发现例子的这些单词也都是-re结尾的, 不过它们的p.p.的词尾不是-u, 所以第二条不能合并进来... 三). U型(us-us-ut-ûmes-ûtes-urent) 1. la majorité des verbes au p.p. en -u// 大部分p.p.以-u结尾的动词 这里的\"la majorité \"不能改成\"tous\"了, 在I型里就有几个动词的p.p.是-u结尾的(-re的,以及voir). 见到p.p.是-u结尾的, 然后原型又不是那几个-re结尾的动词, 就是U型了! 例子: courir, connaître, recevoir 2. 再加一个特例: mourir mourus-mourus-mourut-mourûmes-mourûtes-moururent 其实还有个特例是être, 后面不规则动词里讲... 四). IN型(ins-ins-int-înmes-întes-inrent) 这个可以当作特例记了, 只有俩: venir, tenir(当然还有它们的派生词...), 这俩词的p.p.是-u结尾的... 好吧U型又多了俩特例... vins- vins-vint-vînmes-vîntes-vinrent III. 一些不规则动词 虽然词尾只有那四种, 比较有规律, 但是有时候一些不规则动词的 词根 不是很有规律( 一般来说, 词根是是p.p.的词根 ). 这些常用的不规则动词有: être: p.p.=été ; 词根f-; 词尾U型 faire: p.p.=fait ; 词根f- ; 词尾I型 voir: p.p.=vu ; 词根v- ; 词尾I型 naître: p.p.= né ; 词根naiqu- ; 词尾I型 avoir: p.p.=eu ; 词根e- ; 词尾U型 mourir: p.p.=mort ; 词根mour- ; 词尾U型 offrir, ouvrir: p.p.=offert, ouvert ; 词根 offr-, ouvr- ; 词尾I型 venir, tenir: p.p.=venu, tenu ; 词根 v- , t-; 词尾IN型 这些词比较常用, 所以最好记一下. 我发现passé simple也不是很麻烦的另一个原因是: 这些词都是1base的!! 变位只有一个词根, 和那有两三个词根的présent时态相比, 确实是simple多了... IV. 小结 这样总结下来, passé simple也不是特别困难, 有不少规律可循... 不过话又说回来了, 这个时态用得实在是很少... 另外, linux下用goldendict查法语单词+verbiste查动词变位效果蛮好的! :)","tags":"misc","title":"passé simple时态变位总结"},{"url":"http://x-wei.github.io/aurevoir-cherbourg.html","text":"任何一种环境或一个人, 初次见面就预感到离别的隐痛时, 你必定爱上他了. 我是在来Cherbourg的第二天突然想起这句话的. 那天是29-09-2012, 在德先生和赛先生的带领下, 我们在Cherbourg市中心悠闲地溜达了一个下午, 那天阳光很好. 时间过得好快, 四个月过去了, 明天早上就是道别的时候了. 四个月留下了不尽的回忆...... Cherbourg的海风, 雨水, 长堤... centre-ville的街道, 以及停泊了许多小船的港口 EFQ的海鸥 Guichen 喇叭里每天的通知(\"dans 5 minutes, appelle du personnage de service et des gens sonctionne\", \"attention pour la couleur...\") 每天在楼下等我们的Devailly先生, 我猜想年轻时一定很帅的Stricot先生 les prof: 有N件毛衣的Sylvain, 漂亮的Stéphanie, 美食专家以及\"动词变位女王\"Brigitte, 搞怪的Lydia, 家庭美满幸福的Nolwenn...... 在Cherbourg的海边公路上慢跑, 一跑就是四十分钟 Foule de la press长跑 窗外的海滩, 悠闲的奶牛, 远处小丘上的教堂... 几乎每周的体育课都不一样 餐桌礼仪介绍, 葡萄酒介绍, 布列塔尼风情介绍... 数不清的sorties: Mont-Saint-Michel, 出海, Manche的城堡, Hague, AREVA, 报社...... Florence和她的孩子们, 圣诞和新年的美好回忆 Atelier Théâtre, 以及许多的spectacle Atelier Cinéma, <<瑟堡的雨伞>> AND THAT'S THE END...... 明天早上就要走了, 我觉得在最后几天里, 我已经很好地向Cherbourg以及这里的可爱的人们道别... 世界很大, 也许我们有一天还会再相遇...... bon courage à vous tous, et à bientôt , Cherbourg! Vous allez me manquer... 相遇又离别总是难免的, 不过我要好好的和他们道别... AU REVOIR, CHER BOURG!... 下一站: Palaiseau!","tags":"misc","title":"Au revoir, Cherbourg"},{"url":"http://x-wei.github.io/读当我谈跑步时我谈什么.html","text":"上个月读的这本书, 后来简单写了点东西(其实主要是摘抄啦...). 圣诞来到住家, blog很久没更新了, 放上来充数... 嗯, 这本书不错, 心乱的时候读一两章蛮好. 感觉村上春树绝对是一个精神上强大的人, 虽然他的语气平和而谦逊. 这不是一本很文艺的书(所以我很快就读完了), 都是很朴实的语言, 像坐在火炉边和你谈话一样, 喜欢. 一百千米超级马拉松太恐怖了... 跑步时心理的变化过程: 跑前决定一定要跑个好成绩; 开始跑觉得很轻松; 跑到半成以为会是一个好成绩; 跑到最后心里只剩下厌恶--该死的路程怎么还不结束!! 摘抄 下面这话颇类狡辩，更令人惶恐：尽管这是一部谈论跑步的书，却不是谈论健康方法的书。我并非要在这里高谈阔论、振臂一呼：\"来呀!让我们每天跑步，永葆健康吧!\"归根结底，这些都不过是思索片段，抑或自问自答——对我个人而言．坚持跑步究竟有何意味。仅此而已。 即使不足以称为\"哲学\"，然而我以为，这里面含有_些类似经验法则的东西。一些无甚大不了的玩意儿，却是我通过实实在在地运动自己的躯体，通过作为选择的磨难，极其私人地感悟到的东西。也许并不值得推而广之，但无论如何，这，就是我这个人。 跑步对我来说，不独是有益的体育锻炼，还是有效的隐喻。我每日一面跑步，或者说一面积累参赛经验，一面将目标的横杆一点点地提高，通过超越这高度来提高自己。至少是立志提高自己，并为之日日付出努力。我固然不是了不起的跑步者，而是处于极为平凡的——毋宁说是凡庸的——水准。然而这个问题根本不重要。我超越了昨天的自己，哪怕只是那么一丁点儿，才更为重要。在长跑中，如果说有什么必须战胜的对手，那就是过去的自己。 跑步时浮上脑际的思绪，很像天际的云朵，形状各异，大小不同。它们飘然而来，又飘然而去。然而天空犹自是天空，一成不变。云朵不过是匆匆过客，它穿过天空，来了去了。唯有天空留存下来。所谓天空，是既在又不在的东西，既是实体又不是实体。对于天空这种广漠容器般的存在状态，我们唯有照单收下，全盘接受。 我基本是如此思考，并依循着这样的思考度过人生。就结果而言，在某种程度上，我也许是主动地追求孤绝。对于操我这种职业的人来说，尽管有着程度上的差异，这却是无法绕道回避的必经之路。这种孤绝之感，会像不时从瓶中溢出的酸一般，在不知不觉中腐蚀人的心灵，将之溶化。这是一把锋利的双刃剑，回护人的心灵，也细微却不间歇地损伤心灵的内壁。这种危险，我们大概有所体味，心知肚明。唯其如此，我才必须不间断地、物理性地运动身体，有时甚至穷尽体力，来排除身体内部负荷的孤绝感。说是着意如此，毋宁说凭着直觉行事。 让我说得更具体一点。 当受到某人无缘无故(至少我看来是如此)的非难时，抑或觉得能得到某人的接受却未必如此时，我总是比平日跑得更远一些。跑长于平日的距离，让肉体更多地消耗一些，好重新认识自己乃是能力有限的软弱人类——从最深处，物理性地认识。并且，跑的距离长于平日，便是强化了自己的肉体，哪怕是一点点。发怒的话，就将那份怒气冲着自己发好了。感到懊恼的话，就用那份懊恼来磨炼自己好了。我便是如此思考的。能够默默吞咽下去的东西，就一星不剩地吞咽进体内，在小说这一容器中，尽力改变其姿态形状，将它作为故事的一部分释放出去。我努力做到这一点。我并不认为这样一种性格讨人喜爱，恐怕有极少人赏识，却难得讨大众欢喜。对于这样一个缺乏协调性的人，一遇上事情就想独自躲进壁橱里的人，有谁会抱有好意呢?一个职业小说家讨人喜爱这种事，难道真有可能么?不得而知。或许在世界某个地方有，但恐怕很难推而广之。至少我很难想象，自己作为一个小说家，成年累月不断地写小说，同时又能为人私下里喜爱。为人嫌恶、憎恨、轻蔑，似乎倒是更为自然的事情。我也并不打算说：这样的话，我反而感到放心。即便是我，也没有赏玩他人的嫌恶的爱好。那是另外的事，还是来谈谈跑步吧。 人生基本是不公平的。此乃不刊之论。即便身处不公之地，我以为亦可希求某种\"公正\"。许得费时耗力；甚或费了时耗了力，却仍是枉然。这样的\"公平\"，是否值得刻意希求，当然要靠各人自己裁量了。 时隔许久重读这篇文章，我发现一个事实：二十多年已经逝去，我也跑过了几乎与年数相等的全程马拉松赛次，可是跑四十二公里后感受到的，与最初那一次相比，似乎没有多大变化。现在依然如故，每次跑马拉松，我大体都会经历相同的心路。跑到三十公里，总觉得\"这次没准儿会出好成绩呢\"。过了三十五公里，体内的燃料便消耗殆尽，开始对各种事物大为光火。到了最后，则生出\"揣着空空如也的汽油箱不停行驶的汽车\"般的心情。然而跑完之后少顷，曾经的痛苦、可悲的念头眨眼间忘得一千二净，还下定决心：\"下次我要跑得更好!\"任凭积累了多少经验，增添了几岁，还是一再重复相同的旧事。 是的，这种模式无论如何都不接受改变，我以为。如若必须同这种模式和平共处，我只能通过执著的反复，改变或是扭曲自己，将它吸收进来，成为人格的一部分。 哈哈。 倘若一连几天都不给它负荷，肌肉便会自作主张：\"哦，没必要那般努力了。啊呀，太好了。\"遂自行将承受极限降低。肌肉也同有血有肉的动物一般无二，它也愿意过更为舒服的日子，不继续给它负荷，它便会心安理得地将记忆除去。想再度输入的话，必得从头开始，将同样的模式重复一遍。休息是必要的。然而，比赛迫在眼前的重要时期，要严肃地给肌肉下达最后通牒，将毫不含混的信息传达给它：\"这可是一丝一毫也马虎不得的!\"当然不能让它超负荷，但一定得与它维持着绝不松懈的紧张关系。处理个中的勾心斗角，有经验的跑者自然得心应手。 世上时时有人嘲笑每日坚持跑步的人：\"难道就那么盼望长命百岁?\"我却以为，因为希冀长命百岁而跑步的人，大概不太多。怀着\"不能长命百岁不打紧，至少想在有生之年过得完美\"这种心情跑步的人，只怕多得多。同样是十年，与其稀里糊涂地活过，目的明确、生气勃勃地活当然令人远为满意。跑步无疑大有魅力：在个人的局限性中，可以让自己有效地燃烧——哪怕是一丁点儿，这便是跑步一事的本质，也是活着(在我来说还有写作)一事的隐喻。这样的意见，恐怕会有很多跑者予以赞同。 跑全程马拉松时，到了最后关头，脑子里充溢的全是一个念头：赶快跑过终点，赶快结束!此外什么都无法考虑。此时此刻，我却不曾想过这一点。我觉得，所谓结束，不过是暂时告一段落，并无太大的意义。就同活着一样。并非因为有了结束，过程才具有意义。而是为了便宜地凸显过程这玩意儿的意义，抑或转弯抹角地比喻其局限性，才在某一个地点姑且设置一个结束。相当地哲学。不过当时我一点也没觉得这很哲学。这不是通过语言，而是通过身体感受到的，不妨说是整体性地感受到的。跑进了最后的漫长的半岛状原生花园跑道，这种心情变得尤其强烈。跑法近似进入冥想状态。海边的景色十分美丽，可以感受到鄂霍次克海的气息。天色已近黄昏(出发是在清晨)，空气呈现出独特的清澄来，发出夏初深深的青草气味。还看见几只狐狸在原野中结集成群。它们好奇地望着参赛者。仿佛十九世纪英国风景画一般意味深长的云朵，沉稳地遮蔽了天空。风儿一丝也无。在我的周遭，许多人只是默默向着终点奔去。身处其中，我拥抱着异常静谧的幸福感。吸气，再吐气，听不出呼吸中有丝毫紊乱。空气非常平静地进入体内，再走出体外。我那寡言的心脏按照一定的速度重复着舒张与收缩。我的肺好似勤劳的风箱，规规矩矩将新鲜的氧气摄入体内。我能够目睹它们工作的身影，能够听见它们发出的声响。一切都顺畅无误地运转着。沿道的人们对着我们大声呼唤：\"加油啊!马上就到终点啦!\"声音像透明的风，穿透了我的身体逝去。我感觉，人们的声音就这般穿透而过，直达身体另一面。 我是我，又不是我。这是一种异常沉稳而寂静的心情。意识之类并非多么重要的东西。固然，我是一个小说家，在工作上，意识这东西自是十分重要。没有它，主体性的故事便无缘诞生。尽管如此，我还是禁不住感到：意识之类并非大不了的玩意儿。 我已经到了一定的年纪，时间自会拿走它那份额度，怨不得任何人。这就是游戏规则，就如同河水向着大海源源不断地流去一样。自己这种形象，我们只能当作自然光景的一部分，原封不动地接受。这也许不是令人愉快的事，从中发现的，或许也非值得欣喜若狂的东西。不过，这难道不是无可奈何的事情么?至此为止的人生，我好歹也大致——即便不能说是充分——享受了其中的乐趣。 我仰望天空。能看到一丝一毫的爱心么?不，看不到。只有太平洋上空悠然飘来浮去、无所事事的夏日云朵。云朵永远沉默无语。它们什么都不对我说。或许我不该仰望天空，应当将视线投去我的内部。我试着看向自己的内部，就如同窥视深深的井底。那里可以看到爱心么?不，看不到。看到的只有我的性格。我那个人的、顽固的、缺乏协调性的，每每任性妄为又常常怀疑自己的，哪怕遇到了痛苦也想在其中发现可笑之处的性格。我拎着它，就像拎着一个古旧的旅行包，踱过了漫长的历程。我并不是因为喜欢才拎着它。与内容相比，它显得太沉重，外观也不起眼，还到处绽开了线。我只是没有别的东西可拎，无奈才拎着它徘徊彷徨的。然而，我心中却对它怀有某种依依不舍的情感。 不管有无效能，是否好看，对我们至关重要的东西，几乎都是肉眼无法看见，然而用心灵可以感受到的。而且，真正有价值的东西，往往通过效率甚低的营生方才获得。即便这是虚妄的行为，也绝不是愚蠢的行为。我如此认为，作为实在感受，作为经验法则。 成绩也好，名次也好，外观也好，别人如何评论也好，都不过次要的问题。对于我这样的跑者，第一重要的是用双脚实实在在地跑过一个个终点，让自己无怨无悔：应当尽的力我都尽了，应当忍耐的我都忍耐了。从那些失败和喜悦之中，具体地——如何琐细都没关系——不断汲取教训。并且投入时间投入年月，逐一地累积这样的比赛，最终到达一个自己完全接受的境界，抑或无限相近的所在。嗯，这个表达恐怕更为贴切。 就这般，在长达四分之一个世纪里，日日都坚持跑步，各色各样的思绪从心底涌起。 这书里还有很多其他句子值得一抄... 喜欢上了在Cherbourg的海边低头跑步的感觉...","tags":"misc","title":"读 <<当我谈跑步时我谈什么>>"},{"url":"http://x-wei.github.io/图片合并为pdf, 合并mp3, 批量压缩图片.html","text":"前一阵遇到的三个小功能, linux下有简单的命令可以实现... 多张图片合并为pdf 这个在网上搜一般找到的结果是: convert *.jpg xx.pdf 但是这么做的问题是, 运行起来超级慢, 电脑直接卡死!!! 后来看了 这里 , 知道了可以用pdfjam来做. 先要安装pdfjam, 然后: 先将所有jpg文件重命名为pdf： rename 's/\\.jpg$/\\.pdf/' *.jpg 合成刚重命名的pdf文件为一份： pdfjoin $(ls *.pdf|sort -n) --outfile xx.pdf 后来看到pdfjam其实是在用latex, 想到其实也可以先自动生成一个tex文件然后再调用tex生成pdf... 不过既然有现成的软件就直接用吧!! 合并mp3 超级简单的一条命令: cat *.mp3 > output.mp3 只要预先把文件按照想要的顺序编号即可 这个操作只是把这些文件前后连接起来, 可能是由于mp3文件格式的原因吧, 只要这么做了就和并完成了!! 而且速度快得惊人!!! 不过有一点问题: 合并出来的mp3文件的信息(歌名, 艺术家之类)会是最后一个文件的信息... 当然这基本不影响使用~ 批量压缩图片 使用convert命令, 好像是在ImageMagick里面. 参考 这里 : #!/bin/bash images = ` ls *.JPG ` echo \"resize images begining...\" for image in $images do convert $image -resize 50% $image ; echo \"resize $image to %50\" ; done exit 然后就OK了... 顺便吐槽下gmail的附件大小限制!!...","tags":"soft","title":"多张图片合并为pdf, 合并mp3, 批量压缩图片"},{"url":"http://x-wei.github.io/Les-fetes-chinoises-en-automne.html","text":"赶在十月最后一天发一篇水文... 每年X新生的国际学生中会有一些人来到Cherbourg的 Ecole des Fourriers 学习四个月的法语. 这四个月的时间也许是X四年里最轻松的一段时间了吧...... 今年有19名学生来Cherbourg, 历史最多. 我们每两周要自己出一份法语刊物, 算是促进写作水平吧, 基本上没啥人看...... 我们这年把杂志取名\"MelangeXChange\", 取各国文化交汇之意... 第一期的杂志封面是这样的... 把我和QY写的关于中秋和重阳的文章放上来吧, 很水...... Les Fêtes Chinoises en Automne La fête de la lune Origins La fête de la lune procède du mythe de Chang'e(嫦娥). Chang'e est la femme de HouYi (后羿), un héros qui est très fort. HouYi obtient un élixir de longue vie de dieu. Un jour, 'un étudiant de HouYi qui voulait voler cet elixir à Chang'e quand HouYi n'était pas chez lui, pour protéger cet élixir, Chang'e prend cet elixir. Et puis elle s'exile sur la lune. On croit qu'elle habite désormais dans la lune, et donc on célébre une fête le 15 août du calendrier lunaire, car cette jour, la pleine lune est la plus ronde et la plus lumineuse de l'année, Les festivité Aujourd'hui, la fête de la lune est une de ces fêtes traditionnel les plus important en Chine et dans les des pays de l'est de l'Asie. La lune très ronde représente la reunion de la famille. Pendant la fête de la lune, on se rassemble à la maison pour célèbrer la fête avec les familles. En Chine, on a des vacances pour cette fête. La fête de la lune est une fête de réunion , donc il y a beaucoup de poèmes sur ce thème. Quelques poèmes sont eextrêmement connus par les Chinois. Quand la famille se réunit , on regarde la lune ronde et on prépare des gâteaux de lune. < >. C'est un gâteau spécial pour la fête de la lune, il est aussi rond que la lune, et symbolise le rassemblement de la famille. Et parce que le 15 août du calendrier lunaire est usuellement très près de la fête nationale, il y a des vacances de 7 jours. Il y a un pic du tourisme et dans les transports public à chaque année à cette période. La fête du Chongyang La fête du Chongyang a lieu le neuvième jour du neuvième mois dans le calendrier lunaire(le 23 octobre 2012 dans le calendrier grégorien). Il y a deux 9 et le 9 est un chiffre ‘yang'(solaire et positif) donc particulièrement vénéré dans la tradition chinoise. C'est pourquoi on l'appelle ‘Chongyang'(Double-Yang) ou ‘Chongjiu'(Double-Neuf). L' origine La fête du Chongyang est une fête très importante pour les chinois, mais l'origine de la fête est obscure. Il y a une légende qui raconte que les habitants d'un village attrapaient une maladie et que beaucoup de personnes mouraient. Un jour, un jeune est parti consulter les sages pour trouver le remède contre la maladie. Après son retour au village, il a demandé aux villageois de monter au sommet d'une montagne, afin de collecter des plantes médicinales et boire du vin de chrysanthème. Les gens se sont rétablis peu après les soins. Donc le jeune a été considéré comme un héros. Les festivités Pendant la fête du Chongyang, il y a des activités diverses, comme l' alpinisme, la dégustation du vin de chrysanthème et l'admiration des fleurs. Ces activités célèbrent la légende. D'ailleurs, on mange le gâteau de Chongyang, un dessert très sucré et délicieux. Le respect des vieillards a été attribué aujourd'hui à la fête de Chongyang, parce que le chiffre neuf, homonyme du mot ‘longtemps' en chinois, est un symbole de longévité. Les jeunes profitent de l'occasion pour faire une excursion avec des vieillards ou leur préparer des plats savoureux. Recette de Gâteau de Lune ( http://www.marmiton.org ) Temps de préparation : 240 minutes Temps de cuisson : 70 minutes Ingrédients (10 Portions) : Pour la pâte : - 300 g de farine - 750 g de pâte de graine de lotus - 10 jaunes d'oeuf salés - 7 petite cuillère d'huile d'arachide - 1 petite cuillère de thé - 1/2 petite cuillère de levure de boulanger - 100 g de sucre Pour la décoration : - 2 jaunes d'oeuf - 1 petite cuillère de sucre - 2 petites cuillères d'eau Préparation : Mélangez le thé avec la levure. Enveloppez les jaunes d'oeuf salé dans un papier d'aluminium huilé. Faites les cuire au four pendant 20 minutes (90°) Coupez le pain de farine de graine de lotus en 10, faites un creux au centre de chaque bloc et, placez-y un jaune d'oeuf salé. Roulez-les pour en faire une boule. Tamisez la farine et mélangez avec le thé et l'huile. Laissez reposer la pâte sous une serviette mouillée pendant 4 heures. Coupez en 10. Enveloppez chaque boule avec la pâte et mettez-la dans un moule à gâteau de lune. Aplatissez bien, puis retirez du moule et, placez les gâteaux sur un plateau huilé. Faites cuire au four à 200ºC pendant 15 minutes. Puis enlevez les gâteaux et arrosez-les d'eau. Faites cuire encore à 180ºC pendant 5 minutes. Sortez et brossez les gâteaux avec l'oeuf battu, le sucre et l'eau. Faites cuire encore 10 minutes jusqu'à ce que les gâteaux prennent une belle couleur dorée.","tags":"misc","title":"Les fêtes chinoises en automne"},{"url":"http://x-wei.github.io/PT-summery.html","text":"这个总结早就应该写了, 拖延到今天实在惭愧...... 赶在今年申请的同学还没笔试之前把它写出来吧... 一. 法国大学校&巴黎高科集团介绍 交大的法国项目很多, 最主要的有巴黎高科(9+9), 中央理工(4+4), 法国高旷联盟(GEM), 以及N+I项目. 但是感觉交大的同学们对于法国项目知道的很少, 我是大三考完托福后才知道有巴黎高科这个项目(感谢HY跑步时告诉我T_T......) 当然也不能全怪同学们, 教务处不知为啥, 总把法国的项目放到很不显眼的位置. 今年交大刚刚和巴黎高科成立了 联合学院 , 按说交大应该会大力宣传下吧, 可是今年的报名通知竟然放在了一堆游学项目里!!...... OK, 吐槽完毕, 开始正题... 法国教育制度 法国的教育体系非常奇葩... 它有着世界上独一无二的教育体系(and我记得那次中央理工的校长说他们很为此骄傲==...). 法国的高等教育体系分为大学教育(普通教育)和大学校教育(精英教育), 两套体系完全并行. 见下图(盗图自QYQY): 简单讲, 就是法国的学生高中毕业后拿到BAC文凭, 之后可以选择进入大学学习或进入预科学校学习. 进入大学的条件很宽松, 法国的大学教育强调平等接受教育的权利, 宽进严出. 而如果选择进入预科学校, 则要经历两年的密集的数学物理课程培训(难度比国内的大一大二只高不低). 预科学校学习两年后, 学生经过激烈的竞考(难度很大, 淘汰率很高), 通过的学生才可以进入法国的大学校(Grandes Écoles)学习. 我记得只有不道10%(大约2%??)的法国高中毕业生有机会进入法国大学校学习. 法国的大学校主要分为工程师学校和高等商学院. 工程师学校注重于培养工业界人才, 很重视实践和与企业的合作, 在法国享有很高声望. 工程师学院毕业生拿到的是工程师文凭(Diplôme d'Ingénieur, BAC+5). 听学长说虽然法国的硕士也是BAC+5, 但是工程师学位含金量更大... 关于工程师学位可以参考 这里 . 巴黎高科集团及中法9+9项目简介 巴黎高科集团由法国最有声望的12所大学校组成, 这些学校可以说是法国精英教育最出色的学校. 学校的规模很小, 如巴黎高等矿业学校(Mines)每年全球大概只招收120人, 规模最大的巴黎综合理工学校(École Polytechnique)每届也不过招收500人. 中法9+9(50名工程师)项目是中法两国为了培养一批熟悉中法文化的工程师而建立的合作项目. 首先\"9+9\"不是指9年加9年(囧...)...... 之所以叫\"9+9\"是因为一开始巴黎高科集团是9个成员, 中国方面是9所高校(), 每年大概有50人赴巴黎高科学习. 但是... 现在高科集团现在扩充为12所高校, 而在中国, 2011年浙大加入该项目, 貌似2012年武大和华科也要加入, 而且每年高科集团一共录取的人数大约为80人, 所以这项目名字该改成\"12+12\"或者\"80名工程师\"才比较靠谱... 关于这个项目, 罗列一些优点和缺点: 优点 不要求GRE或toefl成绩; 申请时没有法语要求, 申请流程较简单, 周期较短; 掌握法语的优势; 学费全免(据说学费每年2万欧??), 奖学金机会多且基本都足够生活费(他们认为\"money should not be a discrimination\"); 强调通识教育, 很容易转专业; 重视实习, 上学期间会有两到三次的企业实习(对找工作很有帮助), 没有奖学金的话实习基本上也能赚回来了; 工程师学位在法国认可度很高, 比较容易找工作(法国或国内法企); 缺点 工程师学校重实践不重科研(可能EP是个例外吧...), 想搞科研的同学不要来; 学校太小, 国际知名度不高(不过在欧洲还是很知名的, 高科属欧洲 IDEA联盟 之一); 通识教育, 没有很深入学习某一方面, 他们认为只要你数学物理足够好干啥都没问题... 总结一下, 就是去高科和去北美读硕读博是两条很不一样的路. 选择高科的话未来大多成为工程师, 进入企业, 基本不会进入实验室从事科研. 所以建议大家想好以后要干什么再做出选择. 而且高科的结果出来的很早, 拿高科保底不太合适(而且很伤RP 有木有...). 巴黎综合理工学校简介 Écoles Polytechnique (昵称X) 是法国最有名的大学校, 也是一所世界知名的学校. 这是一所具有悠久军事传统的法国名校, 和ENS(巴黎高师)同为法国最出色的Écoles之一. 关于它的具体介绍可以见文后的附件\"交大法国留学手册\". 我只在此列举它 wiki页面 提供的一些校友, 这些名字时常出现在物理和数学书中... 雪铁龙, 庞加莱, 柯西, 毕奥(毕奥-萨伐尔定律), 马吕斯, 盖-吕萨克, 泊松, 安培, 菲涅耳, 科里奥利, 卡诺, 克拉伯龙, 刘维尔, 约当, 勒沙特列, 亨利·贝克勒...... 我大概就是看了这个名单才决定把EP作为申请的第一选择的... 二. 网申&材料准备 学校报名 九月份教务处会贴出来高科的报名通知, 可能放在很隐蔽的地方... 所以大家要多关注下教务处或者游学网的通知... 高科基本上不会在合作学校之外招人, 所以学校这关一定要过...... 大概每年学校会推荐50人大名单参加笔试, 学校推荐的名额基本上是看成绩排名吧. 排在TOP15%的同学基本上都可以进入学校的推荐名单. 我们那年申请的竞争有点激烈, 电院有四五个排在TOP5%的同学申请了, 可能高科项目的知名度在提高吧... 网申 学校推荐的同时, 自己还要填写高科的网申页面. 注意如果要申请巴黎综合理工(École Polytechnique, 简写EP或X)的话, 还要专门填写EP的网申页面. 这里再说一句, 不通过高科项目(比如学校不在高科的合作学校之列)也是可以申请EP的, 不过那就需要足够好的硬件才能进入EP的面试. 网申可以选择多个学校, 我选择了四所学校: EP, Mines, Telecom, ENSTA. Mines差不多排名仅次于EP, 而规模超小(120人), 每年在中国招生只有3-5人; Telecom电信方面法国最强, 毕业生很好找工作, Telecom在法国大致相当于交大在中国的地位, 而且和交大关系很好; ENSTA是除了EP之外的另一所国防部直属学校, 机械电子能源啥的都很强... 巧合的是, 这四所学校恰好是2012年交大-高科联合学院的四所法国学校...... 网申页面需要上传一些文件和图片. 身份证复印件, 护照复印件, 照片什么的就不说了, 按照要求准备就可以. 下面说一下比较重要的SOP, CV, RL. SOP 动机信在网申和面试中非常重要, 需要好好对待. 在动机信里, 一是要表现出你对未来的规划, 显示你是经过了深思熟虑后才选择的高科; 二是列出你选择高科的理由, 注意最好不要写自己想从事科研(而且如果你想从事科研的话高科真的不太适合你); 三是表明你对法国教育制度的了解以及对高科的向往, 让他们相信你是真的想去而不是仅仅拿高科垫底...... 动机信用英文写就可以(当然如果会法语的话可以严重加分...), 动机信要早写, 早早写出初稿后让多个人看, 参考他们的意见进行改写. 当时我参加了GAUnion小组, 小组同学的意见让我受益匪浅... 高科的网申页面可以重复提交的, 你可以先上传一个版本, 修改后再覆盖上传. 我最后定稿的动机信是在9月30号, 也就是高科网申结束的前一天, 这之前修改了N(N>10)次...... CV 简历也是很重要的文件, 关于简历的模板我用的是一个latex模板, 不要太花哨 , 把最重要的信息写出来, 一页即可 , 决不可超过两页. 简历最好加上一项Objective, 写上你想申请高科的工程师, 表明这份简历是专门为高科写的. 申其他学校也是这样, 写上Objective(\"我想申请你们学校\")比较好...... 在CV里罗列的内容大概有: 姓名, 专业, 联系方式; GPA和排名; 获奖情况; 所学课程; GT成绩(如果有); 科研经历; 实践经历. 科研和实践经历大家可以把那些看起来比较NB的大作业或者科创课程拿出来, 每项实践经历用两三句话进行介绍... CV也是他们参考的重要因素, 在高科的面试时他们就一直在拿着我的CV在问问题... 我的硬件条件较为突出(GPA90+, rank top 3%... 但是硬伤是基本没有科研经历), 所以后来都比较顺利的进入了笔试和面试. 如果硬件不是很突出(比如TOP10%-TOP20%之间的同学), 则需要在笔试中有较为出色的发挥才能进入面试... RL 高科和EP的网申都要求两位老师的推荐信, 我也不知道推荐信可以起多大作用, 但是最好还是认真对待吧... 我找了自动化系两位有德国留学背景的老师(席裕庚老师和袁景淇老师, 两位大牛啊...)写推荐信. 这里赞一下自动化系的老师们, 对学生超级好的!!~ 席老师人超级和蔼, 有求必应; 袁老师是只要大三下他那门过控考得好, 就会给你推荐~ 推荐信一般是自己写好了一个初稿然后交给老师, 老师再进行修改后提交给高科. 一般字数二三百就可以了吧, 尤其是我这种没有跟老师做过项目的人, 更没什么可以写的. 在RL的写作中GAUnion小组也提供了很大的帮助... 还要注意的一点是, 高科面试时 需要提交纸质推荐信 . 你需要把推荐信打印后放在信封里, 然后封口, 让老师们在信封上签字. 其他东西 EP的申请除了网申还需要邮寄, 具体的材料清单可以看 EP的网站 , 有个checklist. 我们那年还要出生证明, 今年好像不要求了. EP的网申填写很繁琐, 还要写每年的排名, 不过这些东西自己估着填一下就可以了. EP还要求动机信和简历, 可以从高科的PS/CV修改. 另外如果提供奖学金证明(奖学金复印件即可吧)可以免除申请费. 托福成绩可以提供(作用不大), 不必让ETS送分, 把纸质的托福成绩单复印下即可. 不建议提供GRE成绩, 这会让他们怀疑你有心申请美国... 我的GRE算是白考了... 有一样 比较纠结的东西是排名证明 ...... 我当时在学院和学校的教务处跑了N次后, 学院终于给开了一个证明. 不过后来教务处又给每人开了一份排名证明...... 总之这个事情虽然纠结但是一届届大家还都能够最后搞定...... 三. 笔试 巴黎高科的笔试一般在十月中旬. 会收到邮件通知, 以及 网上通知 . 我是10月10号收到通知, 10月15号考试...... 所以需要提前复习一下啊否则来不及复习都不会就囧了... 关于高科的笔试, 我觉得他们还是比较看重CV和SOP, 如果排名较高的话, 只要笔试成绩中等水平就应该可以进入高科的面试. 如果GPA不高, 则需要在笔试中表现得出色一点, 以进入面试. 高科的笔试安排在同济的瑞安楼, 交大同学过去要俩小时...... 所以打好提前量...... 笔试的内容是数学物理以及专业课, 全部是选择题, 对一道得3分, 错一道扣1分...... 这里是高科的笔试考试范围: 巴黎高科笔试范围参考.pdf 第一场是数学, 分数学1和数学2, 要申EP的同学需要做数学2. 难度大概比GRE sub稍难一些吧, 但应该超不过交大高数考试的难度...... 高数内容比较简单, 线代当时我是下了比较大功夫复习的(因为忘的比较干净...), 记得考试时有好几道关于特征值特征向量和矩阵对角化的题目... 数学2比较难, 有好多没学过的内容就直接没做...... 第二场是物理和专业课, 物理我感觉蛮难的, 一共十几道题, 每一题的题干都很长...... 而且算出来结果没有出现在选项里的话就很崩溃...... 物理做完后选择自己的专业课, EE的题目比较简单, 有一些信号的内容, 还有些编程以及数电模电的东西, 但都是很简单的, 这些课程不必费太大力气复习. 准备笔试时拿了点GRE sub做了两套: sub.zip 我觉得词汇才是重点, 英语的题干要是看不懂就囧了, 可以带词典, 我嫌词典太重, 就自己打印了份sub词汇表, 用那种一页两面的打印, 刚刚好... sub词汇.zip btw, 在同济监场的是俩帅哥, 后来才知道他们是Telecom来交大交流的学生... 四. 高科的面试 笔试完了之后面试大约在10月底, 在 这里 会有面试通知, 2011年我们是30号面试的. 还是在同济, 不过还算比较人性化, 把交大的面试安排在了下午, 否则还得提前一天晚上过去(闵大荒村民伤不起!!). 交大那年有三十来人进了面试, 笔试淘汰率不太高的. 这次高科的面试不是技术面, 主要还是询问动机. 着装的话有正装可以穿去, 不过其实也无所谓的, 别太邋遢即可. 我就牛仔裤衬衫去了(因为没租到正装)...... 面试在同济的中法中心进行, 一共有三四个房间, 基本上是你报了哪几所学校, 这几所学校的老师就回来面试你. 不过EP的人一个也没来, 因为他们有专门的面试... 面试时拿着纸质版简历和推荐信以及护照什么的. 建议面试前把动机信再好好看看, 另外再了解下各个学校的特点, 那些方面比较强, 这样面试的时候显得你已经对他们研究很久了... 这里有个简单的介绍: Relations_domains_schools_ParisTech.pdf 另外我把BBS上学长学姐们的面试回忆放在了一起打印了出来: 面经.pdf 面我的是一个ENSTA的老师(帅哥), 一个Mines的老师(大胡子, Julien), 还有一个Telecom的老师(大鼻子...). 主要是ENSTA的老师在问问题(难道是看到我是自动化系的所以比较亲切??...). Juien询问了我选择高科的原因, 这是面试时都会问的一个问题, 最好能够有条理地说出来, 我说我进行了一个简单的SWAT分析啊, 觉得高科最适合我啊, blablabla...... 他们听了笑而不语...... 然后ENSTA的老师又拿着我的简历问我为啥选择电院的自动化专业啊, 做的科创以及电设都是干了点什么东西啊之类的... 我给他说了说, 然后又强调了我没有做过研究, 对实际应用比较感兴趣...... 然后ENSTA的老师让我把我申请的这四所学校按照优先级排序...... 没办法, 我只好实话实说了, 我说第一选择是EP, 因为知名度最高; 第二选择是Mines, 因为Mines的毕业生很多进入了法国企业的高管, 然后我又补充说EP和Mines都是很想去的学校, 要是都录取了的话我得好好考虑才知道到底去哪里; 第三选择是ENSTA, 因为他们的机电还有机器人什么的很棒(ENSTA的老师很得意); 第四选择是Telecom, 他们的电子信息方面是法国最好的(Telecom的老师对我把他们排在第四位很不满, 表情严肃...... 最后Telecom果然没录我...). 最后, 他们问我是否有问题问他们(这也是个保留项目), 我问他们觉得中国学生和法国学生有啥区别, 他们说没有, 他们一视同仁...... 然后我就撤了... 面试大概持续了二三十分钟吧, 建议提前准备下如何回答他们的问题以及如何表达自己想去高科的愿望, 让他们相信你是真想去而不是拿来保底....... 高科面试之后, 如果没有申请EP或Mines的话, 高科的申请就结束了, 接下来就是耐心的等待, 录取结果大约在十二月中旬出来... 五. Mines的面试 高科面试结束后, Mines和EP会进行单独的面试, 单独面试的表现才会决定最终的录取 . 所以如果想进入Mines或者EP的话, 面试完了还要继续苦逼的看高数和大物... 2011年11月17日收到Mines的面试, 面试在11月21日. Mines的面试是视频面, 在图书信息楼的7楼进行, 不用跑同济了... 交大一共有三个同学进入了面试, 每人面大约半小时. 额, Mines的面试啊, 这事不能说得太细...... 之前李哲学长(X2010)在BBS上写了他当时 面试回忆 , 学长把面试时做的题目是啥都写了, 于是我复习时就打印出来做了做... 结果面试时遇到的题目和李哲学长那年的一样!!...... 有点太幸运了... Mines的面试题目可以参见学长的文章. 面试持续半小时, 感觉Mines的面试注重的是应用, 比如求二阶偏导, 比如解微分方程...... 而EP的面试则注重分析推导. 面试时拿到题目不会做没有关系, 老师会给你提示, 如果能够快速反应过来按他们的提示做出来题目, 他们会很高兴... 面试是面对一个摄像头和一个显示屏, 那边的老师给你念出题目你在黑板上做. 网络条件很不好啊, 时不时的就\"sorry, the image is frozen AGAIN\"... 另外强烈建议 看一些国外数学公开课 , 我看的是MIT的一个老师讲的微积分重点(Course Highlights of Calculus), 葡萄上有下载. 这个课程内容很简单(介绍微积分的最基本内容), 我只看了七八集. 倒不是为了复习高数, 而是学会那些数学用语用英语怎么说... 比如次方, 求导, 积分, 对数之类的术语... 因为面试是向他们用英语讲题嘛... 六. EP的面试 11月17日收到邮件, 通知我12月2日去同济进行EP的面试. 进入EP面试的人不多, 电院只有三个人... 关于谁进入EP的面试可能主要还是看GPA吧, 如果高科的笔试表现很出色的话也应该会进入EP面试. 据说一旦进入了EP的面试, 会不会录取就 完全取决于面试时的表现 , 和GPA或排名无关了... EP的入学竞考是法国历史最久, 难度最大的竞考之一. EP的面试分三场, 一场数学一场物理一场综合知识(比重貌似是6:4:4?), 大概会持续四个半小时...... 会有三个教授对你进行面试, 他们期望在这四个半小时里全面地了解你, 不仅是考察数理基础, 还包括言谈举止(比如主动擦黑板...)各个方面. EP的考试范围: EP-考试范围.zip 个人感觉EP的面试难度比Mines高很多!! 比如这是EP网上提供的面试题样题: examples-eng.pdf 这个题我和zbf同学在自习室捣鼓了很久才差不多搞出来...... 而且EP的老师会根据你动机信里的内容, 对每个人出不同的题目. 比如有学长写他的试验能力很强, 教授就给他出了一道基于实验的题目. zbf说他电子方面很感兴趣结果老师给出了道电子模型的题目, 瞎了...... 对于数学系物理系的同学, 他们的题目必然要难很多, 工科学生的话题目就简单一些... 幸亏我没在SOP里写我有什么特长, 他们给的题目还算中规中矩, 否则肯定跪了...... 准备EP的面试的过程非常痛苦, 不知道他们会出什么难度的题目, 又把高数线代大物好好看了一遍. 并且我把BBS上学长们的考试题回忆全部打印下来做了做: EP面经.pdf EP的面试会持续一星期, 因为他们一上午只能面试四个人(两组老师). 我是12月2号下午面试, 先是数学, 老师给了两道题, 自己做30分钟. 一道是数列的极限以及收敛的速度, 前两问比较简单可以用数学归纳法做出来, 第三问收敛速度不知道咋做就跳过了. 第二道是线代的题目, 定义了两个矩阵的一种运算(非常SORRY没带日记本来, 所以题目的内容忘了!!...), 然后让你证明一些结论... 由于我理解有误所以也不大会做... 然后去给老师讲, 第一道题前两问讲完后, 第三问在老师的提示下......还是没做出来(弱爆了)... 还有十分钟时老师让我讲下一题... 我对题目理解错了, 不过前两问比较简单, 一会就做出来了(其实前两问就是铺垫, 为了得出第三问的结论). 第三问不会做, 老师提示: 先假设矩阵是对角阵? 然后我捣鼓出来了时间也到了. 然后老师说你看, 对角阵有这个结论吧? 非对角阵只要对它进行对角化我们就能得到这个结论了blablabla...... 我其实也没听懂就出来了...... 休息一会后, 第二场是物理. 还是两道题, 一道是卫星怪象(高中就学过的, 卫星受到阻力之后速度反而增加), 第二道是运放工作在饱和区...是的, 之前学长写过这道题, 我又踩狗屎了... 然后给老师讲题, 第一题做完后老师又让我进一步思考: 假设卫星收到的阻力和速度成正比, 该是什么情况, 要是和速度的平方成正比又是什么情况...... 第二题我一开始做错了(亏我之前看过学长的面经==......), 不过还好在老师的帮助下总算做出来了, 解一个微分方程就可以...... 物理面完再休息一会, 第三场就比较轻松了. 那个数学老师给我一篇文章, 是太阳能发电的维基百科页面, 让我看一下然后跟他们讲讲这篇文章的主要内容. 我讲完后他又问了问我关于这篇文章相关的一些东西, 比如说为啥太阳能板要设计成抛物面... 之后就开始问动机了, 这时就和高科的面试差不多. 他拿着我的动机信, 问我为啥选择申请EP,我就说了说那几条理由. 然后他补充说, \"而且你在动机信里也写了, 我们学校可以提供多个学科的教育\", 看来EP很以此为傲...... 我又再次向他们表忠心, 说目前我只申请了高科的项目, EP是我最想去的学校没有之一, 如果去不了的话我可能再申香港...... 最后教授问我有啥问题没, 我问了问奖学金的事又问了问EP毕业生的就业情况, 然后就结束了...... 感觉我的面试是很幸运的, 没有给我出太难的数学题, 数学题再稍难一点我就挂了...... 另外英语口语好一些的话也有比较大的帮助... 七. 录取通知 面试完之后这几天非常纠结, 很害怕没有被录取上, 所以我抽时间写了申请港中文的PS...... 在EP面试前, 高科的网上出现了\"admissibility\"这么一个东西, 意思是这些学校会考虑录取你, 然后要你按照心目中的优先级进行排序. 这么做估计是为了防止一个人被多个学校录取的情况发生. 我的页面里有admissibility的是EP, Mines以及ENSTA. 于是我很纠结如果把EP排在第一位而最后面试没过的话, 会不会排在二三位的Mines和ENSTA不要我... 不过还是下决心把EP排在了第一位... 12月14日下午, 收到EP的邮件, 被录取了!! EP的网申页面终于走到了最后一步...... 好开心啊!!... 不过我们这年的意外情况比较多... 2011年是交大的小年, 只录取了18人(往年都是20+的). 和交大(尤其是和电院)关系最好的Telecom只在电院录取了3个人, 电子系很强的zbf同学没有被录取(绝对是他们的损失啊!!). 我最后能够如愿进入EP实在是有很大的运气成分...... 八. 录取之后 记得有学长说过, 一旦你被录取, \"everything is orgnized\"... 所以录取之后只要时常查一下邮件按照邮件要求的做就是了... 奖学金 EP给学生提供720欧/月的奖学金(由La Fondation提供), 足够生活的开支(每月总生活费开支大约600-700欧). 而EP也会帮学生申请著名的埃菲尔奖学金, 该奖学金的好处参考 这里 ... 总之是各种爽... 只需要填一个EP的表格(大概是把SOP的内容改改粘上去), 然后EP会帮你申请埃奖. 之前有传闻埃奖是随机分配的, 或者是按照姓名首字母分配的(额...), 不过这次的结果似乎还是排名高一点的同学有较大机会拿到埃奖. 我们这年交大5个埃奖(2EP, 1Mines, 1 Telecom, 1ENSAE). 除了埃奖, 还可以申请国奖, 数额和埃奖相当只是申请过程比较繁琐... EP的人因为有学校奖保底所以一般不会申请. 国奖的申请流程见附件\"交大法国留学手册\"... 除了埃奖国奖, 还有一些企业奖和地区奖, 今年campusfrance也增加了一个奖学金. 即使没有奖学金也问题不大, 因为实习时就会有工资拿了... 法语学习 同学们要好好学习法语啊! T_T... 虽然EP第一年会有半年的学法语时间, 我现在也后悔当初没有好好上法语课, 这边分班分到了水平底的班级...... 上海的法语培训机构主要是凯育和芳赛, 我们当时两边询问比价, 最后把价格减少了很多...... 一半人去凯育一半人去芳赛, 我在凯育上课, 凯育的何春燕老师上课很赞, 可惜我没好好学...... 高科要求的TCF/TEF成绩是B1, 最后还是很容易达到的, 其实即使考了A2也不会有任何问题. 其他 签证 GEM项目有时会有签证悲剧的情况, 但是高科项目的历史签证通过率是100%...... 按照campusfrance上的通知一步步走即可. 关于签证的材料见附件: 签证材料.zip 疫苗 EP会发一封邮件通知你去接种疫苗以及体检, 在上海出入境检疫中心, 有的人可能会打两针(间隔一个月), 这样的话需要提前去打否则到了法国还要再打... 九. 法国留学参考资料 交大法国留学手册.pdf 这个手册很给力, 介绍了交大所有法国项目的情况, 强烈推荐认真阅读... 交大欧洲留学手册.pdf 这个是比较早的一个手册了, 介绍了交大的欧洲项目 上海交通大学goabroad2012大型出国留学讲座PPT汇总.pdf GAUnion的PPT很给力, 让你发现美国之外的其他选择~ 另外强烈建议大家加入GAUnion, 氛围很好帮助很大! 饮水思源BBS-french版精华区 有很多学长的总结贴, 推荐认真看一看. OKOKOK... 写这么多行了... 祝大家申请顺利!!","tags":"misc","title":"2011巴黎高科(ParisTech)申请总结"},{"url":"http://x-wei.github.io/git push使用代理.html","text":"来到X之后, 上外网全部要用代理的, 非常不爽... 而且ubuntu的所谓的全局代理设置(首选项-->网络代理)好像并不管用... 设置了之后apt-get命令可以用, 但是常用软件(最常用莫过于chrome了)都要单独设置才可以... 然而极为不爽的是git, 这边可以clone, 但是一到push的时候就报错: $ git push ssh: connect to host github.com port 22: Network is unreachable fatal: The remote end hung up unexpectedly 前一篇帖子把\"Toefl\"写成了\"Tofel\"...... 囧大了, 然后想改过来发现没法push... 不过今天终于弄好了, 虽然不太明白是怎么弄好了的... 这里记一下. 参考了 这篇文章 , 不过好像又不大一样(我实在是不懂这个东西是什么原理, 只要求能用就好...). 首先, 设置代理地址和端口: $ git config --global http.proxy = yourproxyserver:theport 然后好象就好了...... 不过push的时候要指定用户名和https的地址, 根据提示输入github密码才能使用. $ git push https://x-wei@github.com/X-Wei/x-wei.github.com/ Password: Counting objects: 234, done . Delta compression using up to 4 threads. Compressing objects: 100% ( 153/153 ) , done . Writing objects: 100% ( 153/153 ) , 28.73 KiB, done . Total 153 ( delta 141 ) , reused 0 ( delta 0 ) To <https://x-wei@github.com/X-Wei/x-wei.github.com/> 7ec6f3f..957ede7 master -> master 好吧暂时就这样用吧, 虽然原理是什么我完全不知道......","tags":"soft","title":"git push使用代理"},{"url":"http://x-wei.github.io/GT-summery.html","text":"这个暑假马上结束, 再不写这篇文章就再也没心情写了...... 2011年3月到6月, 三个多月的时间我完成了托福和GRE的考试, 并且取得了让我比较满意的分数(toefl 105+, AW 3.5, GRE 13500+). 在此分享一些经验, 希望能够帮助大家在尽量不影响GPA的前提下, 用尽量短的时间, 考到一个足够的GT分数... 一. 为什么要\"用尽量少的时间考一个够用的分数\" \"尽少的时间\" GT这两样几乎是申请出国必须的敲门砖(请原谅我这么功利...). 但是这些考试难度较大, 即使英语基础比较好的也需要相当大的精力准备. 现在人们喜欢拖延, 所以经常又把GT的备考战线拉得很长, 为了考一个好成绩, 提前好几个月甚至半年开始准备, 耗费了很多时间和精力同时效果却不一定好. \"足够的分数\" 申请下来我的感觉是, 对于申请的学校, GT的分数要求仅仅是一个 门槛 . 虽然一个优异的GT分数会给你的申请加分, 但大多数学校只关心你是否qualify, 你的GT成绩再好也不会比一个美国本土的本科生流利 , 所以GT的分数在考虑录取是不会是占有很高权重的一项. 而学校更看重的是\"干货\": GPA和Research, 这两样才会在申请中起到决定作用. 如果在GT上耗费太多的时间和精力, 势必会对GPA和科研造成影响, 得不偿失. It's possible! GRE和托福虽然考试难度大, 但是它们的题型固定, 有很强的规律性, 一两个月就可以总结出考试的一些技巧. 再加上机经之类的\"特殊手段\", 做到\"速战速决\"是完全可能的. 我就做到了一个月考T, 一个月AW, 一个月考G, 也许有人还可以更快呢!! 但是要做好困难准备, 备考的这几个月确实非常艰苦... 二. 多少分是\"够用的分数\" 首先说一下, 最后申请中我其实没有用到GT的分数(提交了一个toefl分数, 但是学校根本不强制要求也不关心, GRE分数完全没有用到...), 可能下面说的和每个学校的具体情况不尽相同, 仅供参考吧... 不过我个人觉得这些分数应该是足够用来申请的... toefl 我觉得托福考试的重要性要大于GRE, 大概是因为托福的适用面更广吧, 像澳洲, HK, 欧洲这些地方的学校似乎没有GRE也是可以申请的, 但是要求托福(或雅思)成绩. 欧洲或香港的学校对托福的要求不高, 90以上就达到了不少学校的要求. 不过当然了, 分数多多益善~ 而申请美国的学校则一般要求在100分以上甚至更高, 如果要申请TA的话口语分数最好高于23分. 综上, 一个\"够用\"的toefl分数大概是: \"基本配置\": 总分95+, 口语20+; \"推荐配置\": 总分100+, 口语22+; \"发烧级配置\": 总分110+, 口语24+. GRE 两个月的艰辛的GRE备战过程让我得出了这样的结论: 尼玛GRE绝对是美帝耗费外国学生时间与精力的阴谋啊!!! 太坑爹了!!! 不论是作文还是阅读还是词汇都TMD好难好难啊!!! 吐槽不已...... 不过既然它是申请北美众多学校的必需的敲门砖, 还是要硬着头皮考下来... 要求GRE分数的学校一般是北美, 由于我没有申请北美的学校, 所以这里给的参考数据可能不够准确... 我的感觉是, 如果申请MS的话, GRE的要求可能会低一些, 总分1300+, 作文3.0大概是个基本够用的分数了; 如果要申请phd, 总分1350+, 作文3.5的要求基本可以满足大部分学校; 如果申请更好的学校(就是大家耳熟能详的那几个牛校), 必须要更高GRE分数...... 综上, 我给出的\"够用\"的GRE分数为: \"基本配置\": 总分1300+, AW3.0+; \"推荐配置\": 总分1350+, AW3.5+; \"发烧级配置\": 总分1400+, AW4.0+. 三. 我的英语基础&备考经历 我的英语底子应该算不错, 但也不是超级好的水平. 考了两次六级, 一次599一次600. 我觉得六级成绩在580以上的同学都有能力在一个月的时间内托福考到100+ . 另外, 我在备考GT前的一个学期比较系统地背了一遍六级单词(没有坚持到底, 大概背了70%吧), 并且把新概念英语第三册看了一遍(前20课背下, 后40课熟读). 这大概就是我备考GT之前的英语基础吧. 我拖到大三的下学期才开始考GT, 大三下是非常疯狂的一个学期: 用三个月的时间把GT搞定, 然后最后半个月疯狂复习准备期末考试... 二月末开学来到学校, 准备了一个月, 3.20考了toefl. 考完歇了十天左右, 开始准备AW, 4.26去上财考了AW. 考完AW休息了一个星期, 开始了最暗无天日的GRE备考... 6.11考完了GRE, 然后马上开始了期末考试的复习. 备考这三个月来我没有旷课, 上课也基本在听讲, 而不是完全不听课去背单词. 所以经过半个月的突击, 期末考试的成绩没有受到GT的影响... 回想起来, 那三个月实在是目前我经历的最充实的一段时间, 是非常难忘的奋斗经历: 最后一个月(前半个月GRE+期末考试), 我每天在自习室待到12点才回到寝室, 匆匆洗漱后第二天早上七点半起床去图书馆, 天天如此...... 如果想做到用尽少的时间考个足够的分数, 一定要做好困难准备, 做好连续一个月起早贪黑的准备...... 四. toefl备考经验 关于托福的入门介绍, 看饮水思源EnglishTest版的介绍, 看了这个可以不看那本厚厚的OG: https://bbs.sjtu.edu.cn/bbstopcon?board=EnglishTest&file=T.1177935865.A 强烈建议提前把考试题型弄清楚 , 我前面说了, GT的题型很固定很呆版很有规律性, 掌握了每一类题的题型和每一类题的特点可以事半功倍. 考试题型可以看上面链接的介绍, 然后自己做几套题体会. 我觉得托福的单词不是重点, 六级词汇全部掌握的话应付托福问题不大. 当然, 有时间看看分类的单词书还是有帮助, 可以重点看看地理学, 生物学等等的分类词汇. \" 得听力者得托福 \", 这句话是很对的. 因为托福的听说读写四部分, 除了阅读没有听录音的环节, 其他三个环节都有听力参与进来! 所以听力的加强是最重要的! 我的托福的备考技巧, 简单总结就是: 听力阅读靠TPO, 口语听力靠机经 ...... 下面分别说明: 关于听力阅读以及TPO 听力和阅读确实需要真功夫, 因为听力阅读的机经没法看...... 所以我的备考重点也一直是听力和阅读的锻炼. 网上有很多托福的模考软件, 什么巴朗, Delta,Longman...... 我刚开始把这些全部下载下来, 不过幸好只做了两套的巴朗... 因为后来才发现这些模考软件除了界面和实际考试比较相似, 实际题目的难度和考试时相差较大, 使用这些模考来练习的话效果不好. 所以我推荐TPO, TPO是什么? 就是\"TOEFL® Practice Online\", 托福在线模考. 为什么推荐TPO呢? 因为这些题都是以前托福考试的原题! 它们最能够反映托福考试的难度! 官方售价是一套TPO题目44.95美元, 好像新东方和网上也有买的, 会便宜很多. 我使用的是网上下载的PPS格式的TPO, 用Powerpoint打开放映就可以使用, 115下载地址, 有19套题目: http://115.com/file/aq24vbdz 大家也可以去下载最新的TPO, 貌似已经出了二十几套了...... 这些PPS放映虽然界面不大好, 而且最后要自己对答案算分, 但是题目都是货真价实的, 非常珍贵. TPO做一套少一套, 我建议在做了二三套模考软件的题目, 熟悉了题型之后再开始做TPO. 精做, 一天就做一套, 做完了好好对答案, 把不明白的句子弄明白, 没听清的对话听清楚. 备考期间我也就做了10套左右的TPO, 感觉提高很快, 一开始算下来听力和阅读分别也就是二十一二分, 到后来基本上能保证28分以上(压缩包里有一个表格, 把你每一部分对了几道题错了几道题输进去就会出来得分). 那些文件放映时有计时功能, 一开始做题老是超时, 后来渐渐提高了阅读的速度. 另外, 有人建议先看一遍题目再去读文章, 我的经验是 最好先把文章读完再看题目 . 因为题目也比较长, 而且选对需要对文章意思的正确理解, 看看题目和选项直接能选出来基本不可能. 这样做需要快速读懂文章, 一开始确实比较痛苦, 练的多了掌握做题进度就比较有经验了. 听力的重要性怎么言也不为过, 好好做TPO可以很快地提高听力水平...... 托福的听力每一题都是固定的题型, 在了解了每一题的题型和特点后, 要做好及时的记录, 同时别因为记录上一句而导致下一句没听进去... 关于英语的速记, 可以用一些小技巧, 比如: 有的单词可以忽略元音字母, 如\"book\"写成\"bk\"也认得出来; 一些表示\"提高\", \"上升\"之类的单词可以使用箭头代替; 用符号代替常用词尾: -ing词尾--->\"~\", -tion/-sion词尾--->\"ʃ\", th-开头--->\"θ\"(them--->θm)...... 这样的技巧可以提高一些记录的速度. 不过最主要的还是听力能力的锻炼, TPO做完后觉得哪些对话没有听懂要 反复听 , 听懂每一句. 由于听力题型(甚至是话题)的固定性, 这样做进步会很快!~ 另外, 当时做TPO的听力和阅读时感觉就是很有趣(这和背GRE单词和做GRE阅读的心情形成鲜明对比...), 托福的阅读一般是一些科普文章(生物学, 地质学, 人类学啥的), 一些人做的研究, 得到的结论...... 读来很有意思, 比如有一片文章介绍鸡在面临危险或者面临选择时居然会突然睡觉, 这是XXX原因...... 总之让我很开眼界...... 托福的听力则有的是课堂上的对话, 老师和学生的交流什么的, 很有意思...... TPO的口语和写作我不建议做了, 因为...... 因为机经有很大的机会命中, 而TPO的这些题肯定是不会考到的了...... 关于口语写作以及机经 我得承认, 口语和写作这部分我偷懒了, 并没有下功夫训练... 如果要提高这两部分的能力, 恐怕要下比阅读和听力多得多的功夫. 所以我抱着侥幸的心态使用了机经, 并且幸运地中了... 机经是什么? 机经就是以前考托福的人考完后对他那场考试内容的回忆. 托福每周都有考试, 而实际上ETS每年只会出十来套新题目, 所以很多的考试会有重复! 而且, \"ETS出题太有规律了\"(王京竹语), 基本上新东方的老师都可以预测准确! 既然有机经, 为什么听力和阅读不能靠机经呢? 因为所谓的机经都是考完后的回忆, 就像交大的考经一样, 只言片语, 总不会把听力和阅读的全文回忆出来吧? 所以听力和阅读的机经非常鸡肋, 只能大概回忆出文章的主题以及单词那个题考的啥, 不会有太实质的帮助, 所以听力和阅读还是要好好用TPO练... 而口语和写作的机经由于只需要记住一个主题或者题目即可, 所以很有价值...... 我准备口语和作文的时间也就是10天左右, 因为我寄希望于机经命中, 而机经必须要等到考前一两周才回出来...... 实际上机经命中的概率蛮大的, 尤其是我在听了王京竹点题班的录音之后... 托福的机经可以在淘宝上买到, 也就几十块钱. 小马过河或者王京竹的鸡精网上也会有好心人共享机经出来. 各个版本的机经相差不大, 基本上都是那十来套题目. 机经会给出可能考的十几套题目, 按照可能性大小分为三类, 每类大概有六七套题目. 我当时把机经的A类和B类题目好好看了一遍, 一共10套题目左右. 和TPO一样, 我准备得非常详细, 对口语的前两题都准备好了要说的话, 对作文的题目都准备好了三个论点, 因为我知道考试时很可能就会碰到原题!! 关于口语, 我觉得最难的是前两题, 需要在45秒的时间内就一个主题说30秒钟的话. 强烈建议大家去听一听王京竹点题班的录音, 不一定是最近的机经, 他的随便一期点题班录音都可以. 他的方法我觉得短期应付考试很管用: 对于口语, 每个题目说出三个论点 , 每个点一两句话, 再加上前面的引入和后面的总结, 差不多刚好把时间说完. 这是短期内提高口语分数的很好的方法. 准备机经时, 我就对每个题目列出三个论点(有时想出这三个论点需要很多时间的, 所以如果机经没有中的话我肯定会考得很差), 差不多是三句话, 写在纸上. 考试时, 先第一句话表明自己的观点, 然后firstly......secondly......finally......把三个论点讲出, 最后再来一句\"That's why I think......\", 时间就差不多到了~ 可以自己总结出适合自己的模板, 引入话题和最后总结时用固定的句式(i.e. \"In my view, XXXX......., I have this point mainly because of 3 reasons: firstly......secondly......finally....... So that's why I think XXXX......\"), 不要变来变去. 我当时听的王京竹点题班录音: http://115.com/file/dpghlbbk 另外, 口语的前两题一定要大声说出来! 声音越大越好, 否则到后面更加说不出来...... 备考口语的时候, 我在寝室里对这电脑, 看着写好的论点大声说. 说完后听自己的录音, 感觉不好的地方就改正(一开始我说得特别快, 不道20秒已经把三个论点说完了...), 渐渐的就能够比较好的控制时间. 至于口语的后面几题, 实际上是听力题, 只要能够把对话的内容听明白, 接下来表达出来不是问题了, 况且听力的题型和主题也是那么的固定...... 写作的第一题实际上是阅读+听力, 找出这两篇文章中三个不同的观点(一定会是三个点, 找仔细, ETS出题太有规律了...)即可, 然后可以自己总结出来个模板, 很快写完这篇. 关于阅读的第二题, 我也推荐使用八股文式的\"五段式\": 引入-论点1-论点2-论点3-总结. 这个是看一篇 太傻上的文章 学到的, 不要担心因为结构的平庸影响分数, 考官最喜欢这种格式!! 所以对于每个作文题目, 只需想好3个论点(同样, 想出这三个论点需要一番功夫...), 并依次进行展开即可!! 关于考试 我在电信群楼考的, 主场作战就避免了路上的颠簸, 托福一考就是四个小时, 早晨要早起, 多吃点, 一定得带点巧克力啥的, 在考完听力后的十分钟里吃一些...... 考试时我倒数第三个进场, 抽到的是听力加试. 晚入场的好处是, 在我做着听力加试的时候已经听见了别人说口语--而那口语的题目正是我准备过的!! 心中窃喜, 考完听力有十分钟休息时间, 我就边吃点东西边把那一套机经的口语和写作好好看了一遍! 不过晚进场的缺点是, 当你说口语时, 别人几乎都说完了, 没有了大家乱哄哄说话的气氛, 只能听见自己的话在考场里回响...... 不过我脸皮厚, 比较大声地把口语考完了...... 差不多这些就是我的托福备考经验, 另外大家可以关注太傻或小马过河的托福频道, 其中的一些文章介绍考试技巧什么的还是很给力的. 五. GRE备考经验 我考的是最后一次老G, 笔试, 现在GRE变成了网络考试, 可能情况有变, 以下的经验可能参考性不如托福的强, 我也少些一些... GRE真是要人命... 前面说六级580的水平准备toefl问题不大, 但是不论原来英语底子多么好, 备考GRE都要花很大一番功夫的. 我觉得我备考GRE比备考托福多花费了一倍的时间和投入... 做好困难准备, 天无绝人之路...... 关于作文 说托福作文和六级难度比较相似应该基本准确, 然而GRE的作文就完全是高一个层次了... 不论是Argu还是Issue, 都对思维的逻辑性和表达的流畅性提出了非常高的要求... 我的AW经验是: Argu看孙远, Issue看题库+RP...... Argu和孙远 强烈推荐新东方的孙远的AW教学视频, 一共有几十讲, 我只把Argu的部分好好看了. 边看视频边做笔记, 孙远讲得确实不错, 告诉我们如何找出文章立论的漏洞, 常见的漏洞有哪些, 如何进行反驳(Argu貌似都是要反驳的对吧......). 整个教程看完花了一个多星期的时间, 这段时间没怎么实际写文章练过, 但是收获颇多, 后面写Argu也比较顺利了... 需要特别强调, Argu得到高分还是比较容易的. Issue写得差一点也基本能拿个3分, 拿4分的Issue比较困难, 而4分的Argu却完全有可能做到. 所以在Argu上一定要全力写好, 否则很难保证作文能拿到3.5+的分数!... Issue和题库 Issue实在就得靠实力+运气了...... 我AW考了3.5, 运气成分很大... AW的题库是开放的, 考试时会从中抽出一道. 论坛上有人总结每次考试的题目, 然后按照频率来排序, 当时时间不够, 我只做了最高频的几个题目--然后考试时Argu是高频第一题, Issue也是昨晚简单准备过的!...... 耗费了比较多的RP, 考前实在也是没底...... 关于Issue的写作, 我觉得还是要像托福那样准备好三四个论点, 但是要挖掘的更深, 写得更长... 别的经验就谈不上了, 有空的话还是多做些题库的高频题目吧, 但是不要抱托福机经那样大的指望... 关于词汇 词汇是GRE最让人恶心的部分, 也是没法跳过的部分...... 词汇的准备是最痛苦的, 一开始我想要把红宝书背下来, 一天一个list还需要四十多天那! 我坚持了一个星期就放弃了....... 词汇太多了!TT...... 然后果断转用要你命三千, 这本书三千个单词, 30个list, 但至少比红宝书要少, 而且每个list又分成10个小部分, 可以抽几分钟的零碎时间背10个单词...... 要你命3k绝对不是浪得虚名, 真的快要了我的名了... 开始背要你命3k时, 我算了算, 必须每天背两个list才能赶上进度, 来得及背第二遍. 那两个星期我决定不把要你命3k背完不刮胡子...... 当我终于把3k个单词背完一遍时, 胡子已经两三公分长了...... 要你命3k上的单词是从过去十几年的GRE考试中提取出来的常考词汇, 最终考试时我也非常幸运地碰到了好几个3k里的单词(还有要你命3k增补的几百个单词也要背, 这些是近几年考试中出现的词汇). 另外, 每一个list前面都有一个GRE牛人(其中有2011年北大著名的\"横扫姐\")的一些话, 很励志的. 让我最感到受益的是一句很简单的话: \" 背GRE单词的秘诀就是: 重复, 重复, 再重复 \". 基于这样一句话, 我每天除了背新单词之外, 还会花半个小时的时间复习昨天和一周前背的单词, 不断的重复, 再重复...... 背完第一遍后, 3k里面的单词我差不多能够记得85%了, 然后迅速开始复习第二遍和第三遍. 由于第一遍背单词时就注意及时回顾, 所以二三遍的速度快了许多. 另外, 在我回顾单词的时候, 发现很多单词是近义词/反义词/同一类别的词, 把这些词汇放在一起再看, 了解他们的区别与联系, 非常有好处. 所以我花了一天时间把3k里出现的这些有联系的单词写在了一起, 背完3k再来看看这些单词有助于强化记忆. 我总结的同类词汇 关于阅读 如果说得听力者得托福的话, 我觉得可以说得阅读者得GRE... 因为在词汇部分, 大家都能够拿到差不多的分数, 拉分的项目就是阅读了. GRE的阅读, 长, 难, 偏!(尼玛GRE的项目就没有一个正常的...) 需要多多锻炼以提高阅读能力. 那么用什么练呢? 和托福一样, 用真题! 网上有卖所谓的\"小白书\"的, 是比较早的几十套GRE真题, 这些题目可以反映出GRE阅读的真实难度, 是非常好的训练材料! 我在要你命3k背的差不多时开始做小白书的题, 一天一套或两套, 掐时间做, 精做. 开始做题就发现, 3k里的词汇果然出现在了题目里, 不再有一开始看到GRE词汇题那种一个也不会的感觉. 做题主要目的是练阅读的速度和准确度, 一开始也会遇到时间不够的情况(一定要掐时间做, 否则没有意义), 但是后来就会知道如何分配时间了. 我基本没有放弃阅读的任何一道题, 时间刚刚好. 做完后对照答案, 把不认识的单词和看不懂的句子弄懂. 极品的GRE, 有时把原文的简单结构的句子故意弄成长得吓人的长难句! 所以要在这方面下点功夫, 网上有GRE长难句解析, 写得不错, 可以看看. 我看了二三十句长难句后就开始做真题了. 这样做上十来套真题, 到后面我差不多可以做到阅读只错二三个的程度! 这样的水平让我在考前就比较放心了...... 关于数学 GRE的数学其实是考的词汇... 那些幼稚的算数几乎不会犯错误的 累的时候可以做做小白书的数学题休息一下...... 网上有人整理的GRE数学词汇倒是值得看一看, 有些词汇不明白啥意思就傻眼了. 比如我考试时不明白一组数据的\"range\"是啥意思--原来是指一组数据的最大值减去最小值...... 不过总的来说数学一般不会阴沟里翻船的...... 六. 总结 我个人非常建议有出国意向的人考一下托福, 准备周期较短, 适用面比GRE广, 就算最后不出国, 准备托福的过程也会有很大收获. 当然如果目标是北美的话, GRE就是一道不得不越过的坎了... 如果GT都不想考的话, 其实也有别的出路, 比如德国和法国的项目... 另外建议常到网上看看, 太傻, 寄托天下什么的, 上面有一些人的心得体会, 有时他们的一句话就能够点醒你. 还可以到bbs的EnglishTest板上逛逛, 交流心得, 看看精华区的文章... 以上说了这么多的技巧和经验, 但实际上最重要的还是要坚持每天做下去, 就算方向再准确, 计算得再精确, 不迈出前进的脚步仍然没用. 还是那句话, 既然选择了这条路, 就要 做好困难准备 . 最后, 送上我在准备GT时看到的几句话, 共勉: 第一句: Unless a man undertakes more than he possibly can do, he will never do all that he can. 第二句: Sloth, like rust, consumes faster than labor wears. 懒惰像生锈一样, 比操劳更能消耗身体. 第三句: 每当我们对未来充满了各种美好的期望与幻想时, 就该反思一下自己现在的努力 是否配得上这幻境中的将来. Bon courage!","tags":"misc","title":"用尽量少的时间考一个够用的分数--一点Toefl/GRE备考经验"},{"url":"http://x-wei.github.io/google_youku_host_20120706.html","text":"之前两篇帖子介绍了如何通过修改host文件达到无鸭梨 访问google服务 以及 屏蔽优酷土豆广告 的目的, 虽然不明白这东西到底是啥原理, 但一直用得很爽...... 在学校里使用那一个hosts文件一直很顺利, 没啥毛病, 有人抱怨说那个方法不给力, 我也没管... 后来回家发现原来的host确实不给力了, 优酷广告可以屏蔽, 但gmail的附件预览不能...... 今晚决定搞一搞这个问题... 原先的文件在学校管用的原因, 我猜测是google的host有不少是ipv6的, 回家后这些行都不行了...... 于是上网搜索, 想改改新的host. 搜了一大堆都是2011年贴出来的, 不知能不能用... 边搜边想, 这样每隔一段时间去搜host的方法貌似有点笨...... 然后我发现了两个比较给力的host项目...... smarthosts 一个是 smarthosts 项目, 在云端不断更新(最近一次是07.03, 两天前)host文件, 而且也提供了各种客户端~ for linux的其实就是 一个python文件 , 功能就是把云端的文件(地址: https://smarthosts.googlecode.com/svn/trunk/hosts )copy到本地覆盖原先的文件... 所以(对我来说)也用不着什么客户端, 需要时去copy那个云端文件来本地就是了. 把那个文件中的host行copy到我本地的hosts文件里, 顺便把原先的那些行删了, 然后google就可以顺利访问了~ gmail附件预览木问题, googlesites啥的也能上了~ hostx 之前用的host加上那个修改host屏蔽视频网站的的方法, 只是对优酷有效, 对土豆/奇异貌似都无效, 我猜是因为host的原因. 所以继续搜屏蔽广告的host. 然后我就搜到了hostx这个项目(网址是: http://orztech.com/softwares/hostsx 这个值得吐槽的域名==), 和smarthosts项目类似... 它提供给linux的是shell代码, 很短: #!/bin/bash sudo mv /etc/hosts /etc/hosts.bak wget <http://hostsx.googlecode.com/svn/trunk/HostsX.orzhosts> sudo mv hosts /etc/hosts sudo gedit /etc/hosts sudo /etc/init.d/networking restart 啊... 原来这个也是个googlecode上的项目, 和smarthosts很类似... (使用googlecode/github来共享文件确实是不错的办法唉) 不过它的特点是可以提供屏蔽视频网站广告的host, 我把这些host加入本地的hosts文件之后, 果然土豆/奇艺的广告也去除了(当然之前要按 这个帖子 设置一下), 给力啊!~ 总结 本来使用hozstx也有关于google的行, 只是实测貌似不给力...... 于是我现在把这俩host内容都放在我本地的hosts文件里(看来host这东西还是多多益善...), 另外, 这两个都提供了youtube等网站的host, 但是实测都不给力...... 不过反正一般也不上什么youtube/twitter(想上的同学自己折腾下\"goagent\"......), 目前的效果( 访问google所有服务, 上优酷/土豆/奇艺没广告 )已经很满意了~~ 最后贴上 我的host文件 ... 我已经把最开头locoalhost那两行删了(否则可能有问题), 下载下来后把这些内容添加到本地的host文件中即可~ 本地的hosts文件位于: linux: /ect/hosts windows: c:/windows/system32/drivers/etc/hosts 如果又不能用了, 可以自己去把 https://smarthosts.googlecode.com/svn/trunk/hosts 和 http://hostsx.googlecode.com/svn/trunk/HostsX.orzhosts 这俩文件的内容粘贴进本地的hosts文件里就可以了. 我估计换一次host能撑很长时间吧... 当然如果真的很懒的话也可以用他们提供的小软件来搞~ P.S. 码字+修改花了好长的时间啊啊... 大概是因为昨天看了篇文章, \"为什么你应该（从现在开始就）写博客\" ......","tags":"soft","title":"[更新]访问google服务和优酷去广告功能的host列表"},{"url":"http://x-wei.github.io/步入七月.html","text":"面临一个早就很渴望的暑假, 两个多月的自由时间, 可以做很多事情, 还有很多雄心勃勃的暑假计划希望在这个暑假充实自己. 可是回家来一周多基本上只在吃饭+睡觉+上网发呆╮(╯▽╰)╭...... 额, 所以得打起精神来做点事情, 像这样的暑假以后估计不多了. 其实 必须做的 / 该做的 / 想做的 事情还是蛮多的, 比如一些以前想研究的小东西, 比如之前喊着要写的一些post... 所以写这篇水文冒个泡, 算是个宣言吧... 我也希望能把这个小blog慢慢写下去~ Allez!~","tags":"misc","title":"步入七月"},{"url":"http://x-wei.github.io/水源PPP板图片下载器.html","text":"这个其实是三月份的时候做的, 当时刚刚学会用urllib和正则表达式做一些爬虫, 于是结合人民群众的需要, 写了个小脚本(福利~) 不过现在我还只是会照葫芦画瓢那样用urllib, 没什么长进... github地址: https://github.com/X-Wei/yssy_ppp_pic_downloader 1. 功能就是下载水源ppperson板里帖子的图片, 并且每个帖子一个文件夹放好. 通过修改main函数可以选择下载最近一页的帖子还是下载全部帖子(或者最近几页的帖子) 原理很简单, 分析网页的html代码, 用正则表达式找出图片的地址然后下载到本地. 当时我已经写了两三个简单的爬虫, 所以这个写得蛮快, 而且只用50行就搞定了... 不会用多线程, 只能一张一张下载, 帖子数目实在太多了, 我让它跑了一晚上, 第二天跑完, 下载了8个G的图, 几千个文件夹(囧)...... 2. 不过还是遇到了一些问题, 比较老的帖子会有些图片404, 这时或者这个帖子对应的文件夹为空, 或者里面的图片其实不是图片, 而是出错信息的html代码(虽然看后缀是个图片). 我需要把那些不是图片的文件删掉, 而且要删掉所有的空文件夹. 删除不是图片的文件(其实应该是删除纯文本文件), 在水源发贴问, 用shell命令(perl)做到了(虽然不明白为什么这样写...): find yssy_ppp/ -type f | perl -ne 'chomp;unlink \"$_\" if -T $_' 关于删除空目录, 发现 rmdir 命令就已经可以了, 会删除空文件夹, 非空文件夹不会删除(虽然会显示警告). python里面调用shell命令只需要: os.system(\"shell_command\") 所以, 只需要在程序的最后加上两行: os.system('''find yssy_ppp/ -type f | perl -ne 'chomp;unlink \"$ \" if -T $ ' ''') os.system('rmdir yssy_ppp/*') 虽然终端里运行时最后会因为那个 rmdir 命令出一堆警告, 但是既然功能实现了就懒得改了... 3. 还写(改写)过一个人人相册下载的脚本, 不过需要改进, 不知毕业前能不能搞定......","tags":"tech","title":"水源PPP板图片下载器"},{"url":"http://x-wei.github.io/用pandoc自由转换markdown与html格式.html","text":"markdown虽然写起来方便, 但是要预览的话还要用ReText打开, 而且ReText好像是Qt程序, 打开文件时不如别的编辑器那么流畅. 所以想找一个可以把markdown文件变成html格式的工具. 我甚至搜了很久\"markdown2html\"(github上居然可以搜到好几个项目...) 而没有注意到, 在终端输入\"html2markdown\"时显示的警告: $ html2markdown 程序\"html2markdown\"尚未安装。 您可以使用以下命令安装： sudo apt-get install pandoc 后来安装了pandoc( sudo apt-get install pandoc ), 其实只要看看帮助就知道咋用了: $ pandoc -h pandoc [ OPTIONS ] [ FILES ] Input formats: native, markdown, markdown+lhs, rst, rst+lhs, html, latex, latex+lhs Output formats: native, html, html+lhs, s5, docbook, opendocument, odt, latex, latex+lhs, context, texinfo, man, markdown, markdown+lhs, plain, rst, rst+lhs, mediawiki, rtf Options: -f FORMAT, -r FORMAT --from = FORMAT, --read = FORMAT -t FORMAT, -w FORMAT --to = FORMAT, --write = FORMAT -s --standalone -o FILENAME --output = FILENAME -p --preserve-tabs --tab-stop = TABSTOP --strict --reference-links -R --parse-raw -S --smart -m [ URL ] --latexmathml [= URL ] , --asciimathml [= URL ] --mathml [= URL ] --mimetex [= URL ] --jsmath [= URL ] --gladtex -i --incremental --xetex -N --number-sections --no-wrap --sanitize-html --email-obfuscation = none | javascript | references --id-prefix = STRING --indented-code-classes = STRING --toc, --table-of-contents --base-header-level = LEVEL --template = FILENAME -V FILENAME --variable = FILENAME -c URL --css = URL -H FILENAME --include-in-header = FILENAME -B FILENAME --include-before-body = FILENAME -A FILENAME --include-after-body = FILENAME -C FILENAME --custom-header = FILENAME -T STRING --title-prefix = STRING --reference-odt = FILENAME -D FORMAT --print-default-template = FORMAT --data-dir = DIRECTORY --dump-args --ignore-args -v --version -h --help 真是more than I've expected! markdown/rst/html/latex之间可以互转! 使用pandoc命令就可以在随便转换了, 示例, 把demo.md输出成demo.html: $pandoc -f markdown -t html -o demo.html demo.md 或者直接: $pandoc -f markdown -t html -o demo.html demo.md 我觉得这个实在是很有用的一条命令~","tags":"soft","title":"用pandoc自由转换markdown与html格式"},{"url":"http://x-wei.github.io/TheSoundofSilence.html","text":"所有事都完成, 开始写歌评. intro引子 本来我是计划从老鹰的一首歌开始写的, 昨天甚至计划好了第一首就写Hotel California. BUT SOMEHOW, 决定第一首写它, 寂静之声(SoS). 如果把硬盘里的歌全部删了, 只能留十首歌的话, 我是肯定会留下这一首. 歌名: The Sound of Silence 专辑: The Graduate 歌手: Simon&Garfunkel 年代: 1967 不必多说, 这首歌应该知名度很高的吧, 西蒙与加芬科的成名作. Simon&Garfunkel组合凭借电影<<毕业生>>的两首配乐被许多人所熟知(另一首是Scarborough Fair). 以下摘自百度百科: 《寂静之声》旋律飘缓低迷，歌词充满了一种幻觉般的意境。细细听来，仿佛在诉说着年轻无助的一种宣泄。眼前似乎看到一个懵懂无知的女孩，独自一人行走在铺着鹅卵石的狭窄、清冷的小巷里，喧嚣的人群在她身后渐渐远去，前面是没有尽头的黑夜……歌曲如果说是属于民谣的话似乎太深邃，如果说随着强劲的乐曲而震荡的节奏是摇滚的话，却太细腻，音乐上它造就了类的中和体，这应该是它成功的最大原因。 lyrics歌词 Hello darkness, my old friend, I've come to talk with you again, Because a vision softly creeping, Left its seeds while I was sleeping, And the vision that was planted in my brain Still remains Within the sound of silence. In restless dreams I walked alone Narrow streets of cobblestone, Neath the halo of a street lamp, I turned my collar to the cold and damp When my eyes were stabbed by the flash of a neon light That split the night And touched the sound of silence. And in the naked light I saw Ten thousand people, maybe more. People talking without speaking, People hearing without listening, People writing songs that voices never share And no one deared Disturb the sound of silence. \"Fools\" said I,\"You do not know Silence like a cancer grows. Hear my words that I might teach you, Take my arms that I might reach you.\" But my words like silent raindrops fell, And echoed In the wells of silence And the people bowed and prayed To the neon god they made. And the sign flashed out its warning, In the words that it was forming. And the signs said, The words of the prophets are written on the subway walls And tenement halls. And whisper'd in the sounds of silence. 黑暗，老朋友，我来了 又想同你谈谈 突然有一种幻觉悄悄来临 趁着我入睡时播下了种子 那些个在我脑海中不停滋生的幻觉啊 还仍然滞留在寂寞之声中 在永无休止的梦里 我孤独地行走在 狭窄的布满圆石的路上 行走在路灯的光晕下 感到有点寒冷潮湿 于是我翻竖起衣领 那盏刺痛我双眼的霓红灯 划破了夜晚 触到了寂寞的音符 在那无遮蔽的灯光下 我看见成千上万的人 也许更多 人们无声地诉说着 人们无语地倾听着 他们写出一首首歌曲 却没有一个人去唱 没有人敢 惊扰寂寞沉睡时的轻梦 我说：你们这群傻子啊 难道不知道寂寞的癌细胞正在扩散吗 好好记住我教你们的话 握紧我伸给你们的手臂 可惜我的话只如同雨滴无声地落下 回荡在寂静的深井里 看人们朝着象征神圣的氖灯低头祈祷 而霓红灯广告牌却在讽刺地一闪一闪 发出这样的警告： \"先哲的预言就在地道下的墙上 就写在贫民们的住房里\" 还依旧在一片祷告的寂静中喃喃地低语 remark简评 这首歌的主题好像是\"人与人之间交流的障碍\", 但是我听出来的却是年轻人关于未来的迷茫. 在电影<<毕业生>>的开头, 随着清脆的吉他前奏, 达斯汀霍夫曼一脸迷离的坐在回家的飞机上, 下飞机拿行李向我们走来...... 不过我是先听的歌后看的电影, 也许是歌太好听了, 电影没我预期的好看... 歌一开头, \"我\"去找darkness谈话, 这首歌的内容就是\"我\"向黑暗讲述\"我\"的一个\"vision\", 一个诡异的幻觉或一种感觉: In restless dreams I walked alone Narrow streets of cobblestone, Neath the halo of a street lamp, I turned my collar to the cold and damp 这也是我比较喜欢的几句歌词, 几句话就描述了一个让人难忘的典型的restless dream, 一种压抑无助的的气氛. 狭小的鹅卵石街道, 路旁昏暗的路灯, 一个人不安的独行, 禁不住竖起领子, 试图抵御让人难受的cold and damp... 就在这时, 出现了一束刺眼的neon light, 划过长空. 这是\"我\"看到了人, 成千上万的人, 像一种仪式一样, 他们在做什么? People talking without speaking, People hearing without listening, People writing songs that voices never share And no one deared Disturb the sound of silence. \"talking without speaking\", \"hearing without listening\", 我觉得翻译成\"说而不言, 听而不闻\"比较好, 人们是怎么了? 这是一种什么状态? \"我\"有一种\"众人皆醉我独醒\"的感觉. 但当我要试图劝阻人们的时候, 却发现: But my words like silent raindrops fell, And echoed In the wells of silence 结果就是\"我\"的声音回荡在寂静的深井中... 接下来更是诡异的一幕: 人们像被催眠一般, 对neon god顶礼膜拜, 此时牌子上却闪现出了一行warning: The words of the prophets are written on the subway walls And tenement halls. And whisper'd in the sounds of silence. 然后歌曲就以这样一句让人迷惑的句子戛然而止. 整个歌曲就是一个人的喃喃自语, 向darkness慢慢道来他的诡异见闻. 不过我觉得这种诡异和eagles歌曲里的诡异是两种完全不同的感觉, 这里的\"诡异\", 没有恐怖的气息, 有的只是 迷茫和无助 ...... 所以, 突然觉得这首歌是不是很合现在毕业的气氛呢? 人生的又一段重头戏就这样结束了, 下一段的路是什么样子, 会发生什么事会遇到什么人? 都是未知的, 已经年纪不小的我们, 还没有找到人生的目标和事业的轨迹...... 于是, 有时在独行的时候, 想着一些事情, 没来由地, Sound of Silence就会从耳边响起...... 所以这首歌一直在我心目里占据一个特殊的位置. versions版本 关于SoS, 版本实在太多了, 包括各种翻唱以及他们早年的表演, 我就只写几个让我印象深刻的吧... 电影原声配音 这个是把整个电影剪辑一下, 配上的电影原声. 剪辑的很好, 但是我不觉得西蒙与加芬科这一版唱的很好... \"那个\"版本的寂静之声 这个版本我没有查到到底是谁唱的(原文件里没有这个信息?), 当时妈从单位上一个人那里拷来一些英语歌, 这首就在里面. 也许是先入为主吧, 总是觉得这首才最好听, 把那种迷茫的诡异的感觉演绎得淋漓尽致. And the people bowed and prayed, to the neon god they made. 简直就是一种宗教的仪式... (优酷上这个视频的配乐刚好就是\"那个\"版本, 就拿来了) Yao Shi Ting版 一个中国人的翻唱版, 总觉得这个版本和其他的翻唱版有点区别, 姚斯婷的声音很空灵, 别有一番滋味. concert in central park 1981年9月19日，在美国纽约中央公园上演了一场摇滚史上着名的音乐会，那就是Simon & Garfunkel举行的免费音乐会，这也是两人各自单飞11年后的首度合作，约50万歌迷前来捧场，场面蔚为壮观，虽然当时两人都已年过40，但是和声依旧美妙。 当年非常激动下载到了这个演唱会, 找了很久才找到...... 可以说这个演唱会的Sound of Silence以及Scarborough Fair是最最经典的版本. 那时的Simon&Garfunkel都四十几岁, 和声依旧美妙无比, 后来(2002?)格莱美颁奖时, 他们不仅苍老, 嗓子也已经变得沙哑... SoS是演唱会快结束是才唱到的, 西蒙的吉他响起, 全场顿时一阵骚动, 但又很快安静下来. 只有西蒙的吉他伴奏, 寂静之声就这样慢慢蔓延开来...... 有几个镜头给了台下观众, 有的低头并把手插在口袋里默默的听, 有的情侣相互依偎着跟着哼唱. 大家都像被催眠了一样, 沉浸在这寂静之声中... 有一个细节, 其实在他们唱的时候, 观众是在拍手打节拍的, 而其实我看了很多次也没有注意到. 然后, 高三时某天晚上放学回家的路上, 脑海里回放这首歌的时候, 突然, 那些掌声变的异常清晰, 仿佛那些掌声也是伴奏的一部分... 觉得观众的掌声和喊声实在是和西蒙的吉他配合得天衣无缝, 恰到好处, 堪称完美(另一首我觉得现场观众的喊声与歌声配合完美的歌, 大概是94年老鹰的木吉他加州旅馆)...... 这也许就是为什么这一个版本的SoS让我百看不厌的原因吧. 第一篇... 终于写完了, 写得好累啊, 以后可能不能每首都写那么多了, 我觉得有可说的就说, 不想说的就不写那么多, 不能让写这些东西变成一种负担...","tags":"music","title":"寂静之声The Sound of Silence[Simon&Garfunkel;]"},{"url":"http://x-wei.github.io/beamer_template.html","text":"毕设完了, 把答辩时用的beamer模板拿出来分享下. github项目地址: https://github.com/X-Wei/aBeamerTemplate4SJTU 我是tex菜鸟, 基本是遇见什么问题然后上网搜一通找到解决方案... 这个模板自然也是参考的别人的了... 参考自 Yixf's blog , 我只是修改了一下主题, 换了一下交大的图标, 并且做了一些常用功能的例子... 效果截图: 注意 使用xelatex编译生成, latex估计不行, xelatex的配置参考 这里 ; 编辑内容直接修改beame_body.tex即可, 改变设置一般在beamer_header.tex里 我用的是文泉驿的字体, 可以修改beamer_header.tex改变字体设置, 查看已安装的中文字体的命令为: fc-list :lang=zh-cn 个人一点感觉: 用tex写ppt有时也会因为少些括号或者什么地方没注意老编译不过, 所以用tex写也不一定能比用powerpoint方便, 不论啥工具, 只要能 get things done ,就是好工具~","tags":"tech","title":"交大beamer模板"},{"url":"http://x-wei.github.io/github上两个比较有用的小项目.html","text":"github上的好东西不少, 最近发现了两个比较有用的python程序, 这俩功能都是我比较想要的, 有需求就会有牛人去实现~ 1. 视频下载器youku-lixian https://github.com/iambus/youku-lixian 可不止支持下载优酷的视频奥, 土豆, 奇艺, 新浪, 酷6...... 通吃~ 而且每个都只是一个小小的py文件, 直接就可以运行, 比起什么优酷客户端, 奇艺客户端小多了! 太赞了!~ 2. 115网盘自动摇奖 https://gist.github.com/2698830 这个功能我曾经想要实现, 但是关于网络通信方面知道的太少了, 搞了一通也没有成功. 现在有人把它共享出来, 代码居然还不到100行, 强大啊~","tags":"soft","title":"github上两个比较有用的小项目"},{"url":"http://x-wei.github.io/给网页添加交互的flash小老鼠.html","text":"昨天晚上, 这个帖子 突然火了, 号称是交大技术男给女朋友做的... 亮点在于那只小老鼠: 我点进去看, 也很吃惊, 不过为什么那是一个flash? 而且美工做得这么好... 然后今天有人爆料, 这个其实是用的现成的材料, 网站是: http://abowman.com/ 这个网站提供了很多gadgets, 而且都做得好厉害, 还提供了对应的html代码, 只要粘贴进文件就能够看见了... 所以我把那个小老鼠搬到了侧边栏... 那个网站的一些小工具展示 Penguins: Tree Frog: Newton's Cradle: Fish: 不过貌似我用的模板的宽度和这个flash有点不匹配... 可能过几天就把它从侧栏撤掉了...","tags":"misc","title":"给网页添加交互的flash小老鼠"},{"url":"http://x-wei.github.io/关于用pelican写博客的三点tips.html","text":"1.插入视频 效果就像校内网日志那样, 可以内嵌的视频. 其实很简单, 只需要把html代码放进markdown源文件就行了! 而视频的html代码在视频网站上一般都会提供: 复制下来放进源文件即可 2.删除线 markdown不支持删除线? 反正我没有在教程里找到... 但是删除线确实是个有用的功能, 在zim里记笔记的时候我就经常使用. 但是好像听说markdown是支持html内容的, 那么, 是不是直接加html的删除线代码就行了呢? 果然~! <s> 文本 </s> or <strike> 文本 </strike> 嗯, 更复杂的html样式如果markdown没有的话也可以用这种方法弄~ 3.给博客加入分享按钮 这个也是用网上找的html代码, 然后修改了一下主题(pelican-themes/bs5)中的一个html文件, 不过我水平太菜, 改了好久也没能让分享按钮处于标题下方... 2012-05-31补充 原先那个分享的按钮不好看也不很好用, 我借鉴了 ubuntusoft 网站上的分享按钮和回顶部按钮, 查看了下网页代码, 原来是用的百度分享以及友荐按钮, 修改主题文件 ./pelican-themes/bs6/templates/base.html ,在 <body> 后面加上这几行: <!-- Baidu Button BEGIN --> <script type= \"text/javascript\" id= \"bdshare_js\" data= \"type=slide&img=6&pos=right\" ></script> <script type= \"text/javascript\" id= \"bdshell_js\" ></script> <script type= \"text/javascript\" > var bds_config = {\"bdTop\":289}; document.getElementById(\"bdshell_js\").src = \"http://bdimg.share.baidu.com/static/js/shell_v2.js?cdnversion=\" + new Date().getHours(); </script> <!-- Baidu Button END --> <!-- UJian Button BEGIN --> <script type= \"text/javascript\" src= \"http://v1.ujian.cc/code/ujian.js?type=slide\" ></script> <!-- UJian Button END --> 网页就变成了现在的样子, 嗯, 现在就比较满意了...","tags":"soft","title":"关于用pelican写博客的三点tips"},{"url":"http://x-wei.github.io/TODOlist-20120511.html","text":"好吧最近太废了... 主观原因是心不静, 懒惰拖延, 客观原因(理由)是毕设和法语... 仔细分析一下, 貌似目前还是要专心把毕设跟法语搞定. 但是我还是想在毕业前多做一点事情, 列一个清单, 等忙完这一阵就开始做一下这些事情: 写歌评 Eagles, Simon&Garfunkel, Brothers Four, 主要是这三个乐队的, 大概会有20首吧... 可能毕业前写不完, 但是一定要开始写, 我觉得关于某些歌我会有很多要写的. 写写我的毕设 可能我做的毕设没什么意思, 但是至少可以写写那些学到的matlab技巧. 写(所谓的)总结贴 一个是GT的一些经验总结, 一个是PT申请的一些经验总结. 这两件事大概就是我的2011的主线. 唉, 要是前者在2011年7月写掉, 后者在2011年12月写掉该多好!...... 行了, 还是好好对付法语考试以及毕设论文去吧...","tags":"misc","title":"TODO list--忙完这一段之后要做的事情"},{"url":"http://x-wei.github.io/打乱文本的行.html","text":"今天cbl问了我一个问题: 怎样打乱一个文本文件的所有行?? 仔细一想, 确实有难度... 因为那个文本文件居然有1G, 用python读进内存再打乱的思路估计不行啊... 那么awk, shell什么的有没有解决方法? 搜到了一些帖子, 但是稍微复杂一点的shell脚本我也看不懂(弱爆了)... 我甚至想大概vim会提供这个功能吧, 没想到在搜的时候居然搜到了一个现成的shell命令: shuf !! $ shuf --help 用法： shuf [ 选项 ] ... [ 文件 ] 或者: shuf -e [ 选项 ] ... [ 参数 ] ... 或者: shuf -i LO-HI [ 选项 ] ... 把输入行按随机顺序输出到标准输出。 长选项必须使用的参数对于短选项时也是必需使用的。 -e, --echo 将每个参数视为输入行 -i, --input-range = LO-HI 将LO 到HI 的每个数字视为输入行 -n, --head-count = 行数 最多输出指定的行数 -o, --output = 文件 将结果输出到指定文件而非标准输出 --random-source = 文件 从指定文件获得随机比特 -z, --zero-terminated 以0 结束行而非新行 --help 显示此帮助信息并退出 --version 显示版本信息并退出 如果没有指定文件，或者文件为 \"-\" ，则从标准输入读取。 [ 请向bug-coreutils@gnu.org ]( mailto:请向bug-coreutils@gnu.org ) 报告shuf 的错误 GNU coreutils 项目主页： <<http://www.gnu.org/software/coreutils/>> GNU 软件一般性帮助：<<http ://www.gnu.org/gethelp/>> 请向 <<http://translationproject.org/team/zh_CN.ht ml>> 报告shuf 的翻译错误 太好了吧! 用一个小的文本文件一试, 果然是可以的! shuf [filename] 另外, 加上-o参数, 可以指定输出到一个新文件(估计用shell的输出重定向方法也可以吧): shuf [filename] -o [output_filename] 对了, 我搜到的网址是 这里 , 介绍了一些非主流的命令. 比如另一条命令: tac ( cat 的反转), 作用是先输出文件的末端, 也很有意思.","tags":"soft","title":"打乱文本的行"},{"url":"http://x-wei.github.io/在bolg页面上加入ubuntu发布倒计时图标.html","text":"明天ubuntu12.04 LTS 就要发布了! 然后今天下课回来在各种网站上闲逛, 突然发现了这个页面: 给网页添加ubuntu发布倒计时 . 很厉害的样子, 介绍说只要把那一段代码加入网页的html文件就可以了. 我试了一下, 直接加在index.html上面--还真的可以唉~~ 不过, pelican每次都是自动生成和更新index.html的啊, 难道每次都要手动加入这一行代码?? 难道还要自己修改pelican的代码??...... 此时我想到了farseerfc学长的配置文件, 其中我把他的微博秀那几行注释掉了: #~ SIDEBAR_CUSTOM = r\"\"\" #~ <li class= \"nav-header\" ><h4><i class= \"icon-list-alt\" ></i> Weibo </h4></li> #~ <iframe width= \"100%\" height= \"550\" class= \"share_self\" frameborder= \"0\" scrolling= \"no\" #~ src= \"<http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=1&ptype=1&speed=0&skin=2&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=1862842353&verifier=b193b9de&dpc=1>\" > #~ </iframe> #~ \"\"\" 今天仔细一看, 我靠, SIDEBAR_CUSTOM 不就是可以自定义的侧边栏么??!! 赶紧, 写上这样一行: SIDEBAR_CUSTOM=r\"\"\" <a href= \"<http://www.ubuntu.com/>\" ><img src= \"<http://www.ubuntu.com/countdown/banner1.png>\" border= \"0\" width= \"180\" height= \"150\" alt= \"The next version of Ubuntu is coming soon\" ></a> \"\"\" 运行一下, 果然好了!!~~ 需要注意的是, 这个设置只有使用farseerfc制作的bootstrap2主题时才有用(再次感谢farseerfc!)~ 那么怎么添加多个小工具呢? 很简单, 只要在字符串SIDEBAR_CUSTOM里面罗列copy来的代码就可以啦!~(比如, 还可以 添加gtalk 的小工具.) 所以, google自定义搜索引擎其实也可以这样添加的~ BTW 今天才知道原来pelican是支持中文文件名的啊啊啊啊啊... 以后没必要取那些dt的英文文件名了... 另外今天又自己修改了一点bootstrap2主题(把存档页面换成了bootstrap主题的...), 越来越喜欢pelican这个工具了~","tags":"soft","title":"在bolg页面上加入ubuntu发布倒计时图标"},{"url":"http://x-wei.github.io/chrome_flashblock.html","text":"以前搜索怎么加速打开网页的速度, 有人会推荐flashblock插件. 安装之后, 所有的flash(视频也好, 广告也好)都不会自动播放, 只有自己去点击一下才会播放. 当时觉得这样挺好啊, 因为flash肯定占用了不少带宽以及cpu嘛~ 后来, 发现cbl同学没有安装插件也实现了这样的效果, 如下图: cbl教我把chrome设置成插件点击播放, 个人感觉这样挺有用的~ 大概是两步: 第一步 , 扳手菜单-->首选项-->高级选项-->隐私设置-->内容设置 然后找到\"插件\"这一项, 选择\"点后运行\" 这样设置之后, chrome就不会自动播放flash, 然后再设置点击运行flash. 第二步 , 在地址栏输入: , 找到\"点后运行\"一行, 选择\"启用\". OK, 到此为止, 就可以不装插件实现flashblock效果了~~","tags":"soft","title":"不用安装插件, 设置chrome点击播放flash"},{"url":"http://x-wei.github.io/add-google-custom-search.html","text":"虽然我的blog点击人数可能还是个位数的, 我还是每天都想折腾一下它... bootstrap2模板里提供了很丰富的内容(可以看farseerfc学长的页面), 其中的google站内搜索我觉得很有用, 于是也自己去弄了一下... 使用google自定义搜索 要登录google 自定义搜索 , 的页面, 用google帐号登录, 然后选择新建一个自定义搜索引擎, 会看到这样的界面: 第一项的名称和描述啥的随便填就行, 关键是第二项\"要搜索的网站\", 可以点击\"了解详情\"看一下应该怎么写. 比如我的网站是 x-wei.github.com , 而且我想是在这个网站的所有子页面中搜索, 于是这里就填写: x-wei.github.com/* 即可~ 第三项当然是免费版, 然后下一步. 下一步是一个测试, 可以在搜索框里尝试一下能不能得到想要的结果( 我就是这里有问题的, 待会说 ). 如果没问题, 点击下一步, 下一步是给出了一段html代码, 把这些代码加入网页就可以添加google自定义搜索栏了(不过使用pelican写博客的话就不用这样了, 见后文). 嗯, 这个过程还是非常简单的吧!~ 如何把自定义搜索栏加入pelican生成的页面 首先, 可能只能使用bootstrap2这个主题... 然后, 在 settings.py 文件里加入这一行: GOOGLE_CUSTOM_SEARCH_SIDEBAR = \"001578481551708017171:axpo6yvtdyg\" 注意, 引号里的那一串字符是你刚才申请的自定义搜索引擎的id, 这个id在哪里? 再次登录google自定义搜索, 这次点\"管理现有引擎\", 点击你刚才创建的那个引擎的\"控制面板\": 在基本信息里面就会看到\"搜索引擎的唯一 ID\", 把那一串数字赋值给GOOGLE_CUSTOM_SEARCH_SIDEBAR即可~ 让google收录你的网站 但是我没有那么顺利的完成以上, 因为我在测试自定义引擎的时候总是搜不到任何我的网站的东西... 还以为是设置搜索的网页格式写错了呢... 非常崩溃... 后来在zyb同学的提醒下意识到可能是google没有收录我的网页... 果然, google怎么也搜不到我的网站的内容唉... 那么怎么才能让google收录自己的网页?? 难道要坐等几个月后google发现我? 呵呵, 其实是可以主动申请让搜索引擎收录自己的网站的, 我看的 这里 . 原来就是很简单的一个工作, 登录http://www.google.com/addurl/?hl=zh-CN&continue=/addurl (要是想被百度收录就登录 http://www.baidu.com/search/url_submit.html ) 填上你的网址以及验证码(google的验证码太难辨认... 而百度的有点太简单了吧...)就可以了, 我昨天填的, 今天一试发现就可以搜索到了! 然后又发现google自定义搜索也好使了!~~","tags":"soft","title":"使用google自定义搜索以及让google收录自己的网站"},{"url":"http://x-wei.github.io/host_youkuqiyi.html","text":"优酷现在的广告已经是半分钟长了?? 所以屏蔽这些广告还是很有必要滴~ 之前, 马阳同学给我一个修改host的方式, 用了几个月之后, 发现不好使了: 虽然不会显示广告, 但是不会直接跳过去, 而是显示\"广告不能正常播放...\" 然后还是要等待半分钟才能看... 后来, 看了 奶牛的博客 , 终于找到了解决办法, 至少到目前还是好使的~ 第一步 首先, 添加屏蔽广告的host. 关于host的修改, 直接参考 这篇博客 , 把里面host的全部内容粘贴进对应的文件中. 第二步 然后, 按照奶牛的办法: linux用户 找到: ~/.macromedia/Flash_Player/#SharedObjects/某某名字文件夹/ 这里, 可能会有两个文件夹: www.iqiyi.com 以及 static.youku.com , 删除之, 然后新建两个空白文件, 名字就取这两个文件夹的名字(要是没有这俩文件夹, 则直接新建这两个空白文件). windows用户 和linux用户一样, 只是那个文件夹在: (xp) C:\\Documents and Settings\\Administrator\\Application Data\\Macromedia\\Flash Player\\#SharedObjects\\某某名字文件夹 (呵呵, 略长略长...) (win7) C:\\Users\\用户名\\AppData\\Roaming\\Macromedia\\Flash Player\\#SharedObjects\\某某名字文件夹 注意啊, 新建的空白文件就是叫 www.iqiyi.com 和 static.youku.com , 我的意思是...别加上.txt的后缀之类的... 这样做好之后, 看优酷奇艺的视频就没有广告了, enjoy~ 另外, 我的host文件里好像提供了土豆的屏蔽规则, 但是貌似不大好使...","tags":"soft","title":"修改host去除优酷奇艺网站广告"},{"url":"http://x-wei.github.io/google_host.html","text":"google的服务(mail, doc, site, code, project...)很多都实在很方便(个人感觉QQ, 网易什么的和它绝对不是一个档次). 但是比较悲剧的是, google服务在国内不很稳定, 时不时上不去(比如gmail), 而有的服务(比如site)居然完全上不去... 自己的经验, 使用修改host的方式可以比较好的解决这个问题(而youtube啊, facebook啊什么的修改了不一定好使...) 这里贴一下.. 其实就是修改一个文件...linux用户修改 /ect/hosts , windows用户修改 c:/windows/system32/drivers/etc/hosts . 2012-07-06更新 下面这些host不给力了, 新的host见 我的日志 关于google的host, 网上到处都是, 比如 这里 , 不过我没有试过这里的host可不可以(应该可以吧...), 还是把我的host贴出来吧. 这里说一下, 我同时还加了去优酷广告(这里修改完了还要 再处理一下 ), 上youtube(貌似有点问题). 把这些东西粘贴进hosts文件, 即可访问google的服务, gmail也不会抽风了~~ (要是嫌复制粘贴麻烦, 直接 下载hosts文件 ) #优酷屏蔽广告规则如下： 127.0.0.1 stat.youku.com 127.0.0.1 static.lstat.youku.com 127.0.0.1 static.atm.youku.com/crossdomain.xml 127.0.0.1 valb.atm.youku.com 127.0.0.1 valc.atm.youku.com 127.0.0.1 valf.atm.youku.com 127.0.0.1 valo.atm.youku.com 127.0.0.1 valp.atm.youku.com 127.0.0.1 vid.atm.youku.com 127.0.0.1 walp.atm.youku.com #土豆屏蔽广告规则如下： 127.0.0.1 adextensioncontrol.tudou.com 127.0.0.1 adplay.tudou.com 127.0.0.1 adcontrol.tudou.com 127.0.0.1 iwstat.tudou.com 127.0.0.1 nstat.tudou.com 127.0.0.1 stat.tudou.com 127.0.0.1 stats.tudou.com 127.0.0.1 at-img1.tdimg.com 127.0.0.1 at-img2.tdimg.com 127.0.0.1 at-img3.tdimg.com 127.0.0.1 *.p2v.tudou.com* #PS: 127.0.0.1 at-img1.tdimg.com 127.0.0.1 at-img2.tdimg.com 127.0.0.1 at-img3.tdimg.com #会导致部分图片无显示（与广告挂钩），建议保留at-img1.tdimg.com，其余两条前打个\"#\"号去掉，最大化的去除广告和显示图片，具体算法自己选择吧。 #去迅雷看看广告 127.0.0.1 pubstat.sandai.net 127.0.0.1 mcfg.sandai.net 127.0.0.1 biz5.sandai.net 127.0.0.1 float.sandai.net 127.0.0.1 recommend.xunlei.com 127.0.0.1 cl.kankan.xunlei.com #去56广告 127.0.0.1 acs.56.com 127.0.0.1 acs.agent.56.com 127.0.0.1 acs.agent.v-56.com 127.0.0.1 bill.agent.56.com 127.0.0.1 union.56.com 127.0.0.1 v16.56.com #去搜狐高清广告 127.0.0.1 images.sohu.com #去新浪视频广告 127.0.0.1 dcads.sina.com.cn #去酷6广告 127.0.0.1 1.allyes.com.cn 127.0.0.1 analytics.ku6.com 127.0.0.1 stat0.888.ku6.com 127.0.0.1 stat1.888.ku6.com 127.0.0.1 stat2.888.ku6.com 127.0.0.1 stat3.888.ku6.com 127.0.0.1 ku6afp.allyes.com #去凤凰网广告(不包括直播部分) 127.0.0.1 img.ifeng.com #去pptv.com广告 127.0.0.1 pp2.pptv.com #去cntv广告 127.0.0.1 d.cntv.cn #去乐视广告 127.0.0.1 pro.letv.com #去奇艺广告 127.0.0.1 afp.qiyi.com 127.0.0.1 focusbaiduafp.allyes.com #去6间房广告(还有一点点残留) 127.0.0.1 simba.6.cn 127.0.0.1 pole.6rooms.com 127.0.0.1 shrek.6.cn 127.0.0.1 union.6.cn #去激动网广告 127.0.0.1 86file.megajoy.com 127.0.0.1 86get.joy.cn 127.0.0.1 86log.joy.cn #dropbox 208.43.202.50 www.dropbox.com 174.129.11.212 dl.dropbox.com 184.73.163.57 dl-web.dropbox.com 127.0.0.1 localhost Hasee 127.0.1.1 Hasee # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 127.0.0.1 atm.youku.com 127.0.0.1 Fvid.atm.youku.com 127.0.0.1 html.atm.youku.com 127.0.0.1 valb.atm.youku.com 127.0.0.1 valf.atm.youku.com 127.0.0.1 valo.atm.youku.com 127.0.0.1 valp.atm.youku.com 127.0.0.1 lstat.youku.com 127.0.0.1 speed.lstat.youku.com 127.0.0.1 urchin.lstat.youku.com 127.0.0.1 stat.youku.com 127.0.0.1 static.lstat.youku.com 127.0.0.1 valc.atm.youku.com 127.0.0.1 vid.atm.youku.com 127.0.0.1 walp.atm.youku.com 127.0.0.1 valf.atm.youku.com 127.0.0.1 valb.atm.youku.com 127.0.0.1 vid.atm.youku.com ##Google.com Google.com 2404:6800:8005::68 www.google.com #主页 #2404:6800:8005::68 www.l.google.com 2404:6800:8005::c1 m.google.com #Google移动版 2404:6800:8005::54 accounts.google.com #帐户 2404:6800:8005::62 id.google.com #帐号登录 #2404:6800:8005::62 id.l.google.com # 2404:6800:8005::62 gg.google.com # #2404:6800:8005::62 csi.l.google.com 2404:6800:8005::62 safebrowsing.clients.google.com #安全浏览客户端服务器 #2404:6800:8005::62 clients.l.google.com 2404:6800:8005::62 ns1.google.com #域名系统服务器ns-soa/ns 2404:6800:8005::62 ns2.google.com #域名系统服务器ns 2404:6800:8005::62 ns3.google.com #域名系统服务器ns 2404:6800:8005::62 ns4.google.com #域名系统服务器ns 2404:6800:8005::65 services.google.com #服务申请 #2404:6800:8005::65 www3.l.google.com 2404:6800:8005::76 feedproxy.google.com #Feed代理 #2404:6800:8005::76 www4.l.google.com 2404:6800:8005::d2 jmt0.google.com #未知 2404:6800:8005::62 googlemashups.l.google.com #位置 ##Google.com.hk 谷歌香港 2404:6800:8005::2e www.google.com.hk 2404:6800:8005::2e images.google.com.hk 2404:6800:8005::2e video.google.com.hk 2404:6800:8005::2e maps.google.com.hk 2404:6800:8005::2e news.google.com.hk 2404:6800:8005::2e translate.google.com.hk 2404:6800:8005::2e blogsearch.google.com.hk 2404:6800:8005::2e picasaweb.google.com.hk 2404:6800:8005::2e toolbar.google.com.hk 2404:6800:8005::2e desktop.google.com.hk 2404:6800:8005::2e id.google.com.hk ##Google.cn 谷歌中国（启用此地址无法正常使用谷歌音乐） #2401:3800:c001::2c www.google.cn #主页 #2401:3800:c001::2c g.cn #主页 #2401:3800:c001::2c google.cn #主页 #2401:3800:c001::2c ipv6cn.l.google.com #IPv6：ipv6.google.cn 2401:3800:c001::84 music.googleusercontent.cn ##Google.com.tw Google台湾 2404:6800:8005::2f www.google.com.tw #主页 2404:6800:8005::2f picasaweb.google.com.tw #picasaweb ##Google.co.jp Google日本 2a00:1450:8006::30 www.google.co.jp #IPv6：ipv6.google.co.jp ## 2404:6800:8005::20 www.google.com.tr 土耳其 2404:6800:8005::21 www.google.com.au 澳大利亚 2404:6800:8005::22 www.google.com.vn 越南 2404:6800:8005::23 www.google.com.pk 巴基斯坦 2404:6800:8005::24 www.google.com.my 马来西亚 2404:6800:8005::25 www.google.com.pe 2404:6800:8005::26 www.google.co.za 2404:6800:8005::27 www.google.co.ve 2404:6800:8005::28 www.google.com.ph 2404:6800:8005::29 www.google.com.ar 2404:6800:8005::2a www.google.co.nz 2404:6800:8005::2b www.google.lt #2404:6800:8005::2c www.google.cn 中国（已死） 2404:6800:8005::2d www.google.com.sg 2404:6800:8005::2e www.google.com.hk 香港 2404:6800:8005::2f www.google.com.tw 台湾 2404:6800:8005::30 www.google.co.jp 日本 2404:6800:8005::31 www.google.ae 2404:6800:8005::32 www.google.co.uk 英国 2404:6800:8005::33 www.google.com.gr 2404:6800:8005::34 www.google.de 2404:6800:8005::35 www.google.co.il 2404:6800:8005::36 www.google.fr 法国 2404:6800:8005::38 www.google.it 2404:6800:8005::39 www.google.lv 2404:6800:8005::3a www.google.ca 2404:6800:8005::3b www.google.pl 2404:6800:8005::3c www.google.ch 2404:6800:8005::3d www.google.ro 2404:6800:8005::3e www.google.nl 2404:6800:8005::3f www.google.com.ru 2404:6800:8005::40 www.google.at 奥地利 2404:6800:8005::42 www.google.be 2404:6800:8005::44 www.google.co.kr 2404:6800:8005::45 www.google.com.ua 2404:6800:8005::48 www.google.fi 芬兰 2404:6800:8005::49 www.google.co.in 2404:6800:8005::4a www.google.pt 2404:6800:8005::4b www.google.com.ly 2404:6800:8005::4c www.google.com.br #Web 网页 2404:6800:8005::68 www.google.com #主页 2404:6800:8005::68 www.l.google.com 2404:6800:8005::62 www0.l.google.com 2404:6800:8005::62 www1.l.google.com 2404:6800:8005::62 www3.l.google.com 2404:6800:8005::62 suggestqueries.google.com #搜索建议 2404:6800:8005::62 suggestqueries.l.google.com #搜索建议 2404:6800:8005::62 clients0.google.com #客户端服务器 2404:6800:8005::62 clients1.google.com #客户端服务器 2404:6800:8005::62 clients2.google.com #客户端服务器 2404:6800:8005::62 clients3.google.com #客户端服务器 2404:6800:8005::62 clients4.google.com #客户端服务器 #Images 图片 2404:6800:8005::68 images.google.com #主页 2404:6800:8005::68 images.l.google.com # 2404:6800:8005::62 tbn0.google.com 2404:6800:8005::62 tbn1.google.com 2404:6800:8005::62 tbn2.google.com 2404:6800:8005::62 tbn3.google.com 2404:6800:8005::62 tbn4.google.com 2404:6800:8005::62 tbn5.google.com 2404:6800:8005::62 tbn6.google.com #Video 视频 2404:6800:8005::62 video.google.com #主页 #2404:6800:8005::62 video.l.google.com 2404:6800:8005::62 0.gvt0.com 2404:6800:8005::62 1.gvt0.com 2404:6800:8005::62 2.gvt0.com 2404:6800:8005::62 3.gvt0.com 2404:6800:8005::62 4.gvt0.com 2404:6800:8005::62 5.gvt0.com 2404:6800:8005::62 video-stats.video.google.com 2404:6800:8005::74 upload.video.google.com 2404:6800:8005::74 sslvideo-upload.l.google.com 2404:6800:8005::62 vp.video.google.com 2404:6800:8005::62 vp.video.l.google.com 2404:6800:8005::62 qwqy.vp.video.l.google.com 2404:6800:8005::62 nz.vp.video.l.google.com 2404:6800:8005::62 nztdug.vp.video.l.google.com 2404:6800:8005::62 pr.vp.video.l.google.com 2404:6800:8005::62 ug.vp.video.l.google.com 2404:6800:8005::62 vp01.video.l.google.com 2404:6800:8005::62 vp02.video.l.google.com 2404:6800:8005::62 vp03.video.l.google.com 2404:6800:8005::62 vp04.video.l.google.com 2404:6800:8005::62 vp05.video.l.google.com 2404:6800:8005::62 vp06.video.l.google.com 2404:6800:8005::62 vp07.video.l.google.com 2404:6800:8005::62 vp08.video.l.google.com 2404:6800:8005::62 vp09.video.l.google.com 2404:6800:8005::62 vp10.video.l.google.com 2404:6800:8005::62 vp11.video.l.google.com 2404:6800:8005::62 vp12.video.l.google.com 2404:6800:8005::62 vp13.video.l.google.com 2404:6800:8005::62 vp14.video.l.google.com 2404:6800:8005::62 vp15.video.l.google.com 2404:6800:8005::62 vp16.video.l.google.com 2404:6800:8005::62 vp17.video.l.google.com 2404:6800:8005::62 vp18.video.l.google.com 2404:6800:8005::62 vp19.video.l.google.com 2404:6800:8005::62 vp20.video.l.google.com #Map 地图 2404:6800:8005::68 maps.google.com #主页 2404:6800:8005::68 maps.l.google.com 2404:6800:8005::62 maps-api-ssl.google.com #2404:6800:8005::62 clients.l.google.com 2404:6800:8005::62 map.google.com 2404:6800:8005::62 kh.google.com 2404:6800:8005::62 kh.l.google.com 2404:6800:8005::62 khmdb.google.com 2404:6800:8005::62 khm.google.com # 2404:6800:8005::62 khm.l.google.com 2404:6800:8005::62 khm0.google.com #Satellite View 2404:6800:8005::62 khm1.google.com #Satellite View 2404:6800:8005::62 khm2.google.com #Satellite View 2404:6800:8005::62 khm3.google.com #Satellite View 2404:6800:8005::62 cbk0.google.com #Street View 2404:6800:8005::62 cbk1.google.com #Street View 2404:6800:8005::62 cbk2.google.com #Street View 2404:6800:8005::62 cbk3.google.com #Street View 2404:6800:8005::62 mw0.google.com 2404:6800:8005::62 mw1.google.com 2404:6800:8005::62 mw2.google.com 2404:6800:8005::62 mw3.google.com 2404:6800:8005::62 mw-small.l.google.com 2404:6800:8005::62 mt.l.google.com 2404:6800:8005::62 mt0.google.com 2404:6800:8005::62 mt1.google.com 2404:6800:8005::62 mt2.google.com 2404:6800:8005::62 mt3.google.com 2404:6800:8005::62 mlt0.google.com 2404:6800:8005::62 mlt1.google.com 2404:6800:8005::62 mlt2.google.com 2404:6800:8005::62 mlt3.google.com #News 资讯 2404:6800:8005::68 news.google.com #主页 2404:6800:8005::68 news.l.google.com 2404:6800:8005::62 nt0.ggpht.com 2404:6800:8005::62 nt1.ggpht.com 2404:6800:8005::62 nt2.ggpht.com 2404:6800:8005::62 nt3.ggpht.com 2404:6800:8005::62 nt4.ggpht.com 2404:6800:8005::62 nt5.ggpht.com #Gmail 邮箱 2404:6800:8005::11 mail.google.com #主页 2404:6800:8005::53 googlemail.l.google.com 2404:6800:8005::11 googlemail.l.google.com 2404:6800:8005::12 googlemail.l.google.com 2404:6800:8005::13 googlemail.l.google.com 2404:6800:8005::bd chatenabled.mail.google.com #Gmail中Gtalk聊天服务 #2404:6800:8005::bd b.googlemail.l.google.com 2404:6800:8005::62 talk.gmail.com #Gmail中Gtalk聊天服务 2404:6800:8005::62 gmail.google.com # 2404:6800:8005::62 gmail.l.google.com # 2404:6800:8005::62 www.gmail.com #Gmail主页 2404:6800:8005::62 gmail.com #Gmail主页 2404:6800:8005::62 pop.gmail.com #pop服务 2404:6800:8005::62 smtp.gmail.com #smtp服务 2404:6800:8005::62 smtp1.google.com 2404:6800:8005::62 smtp2.google.com 2404:6800:8005::62 smtp3.google.com 2404:6800:8005::62 smtp4.google.com 2404:6800:8005::62 smtp5.google.com 2404:6800:8005::62 smtp-out.google.com 2404:6800:8005::62 smtp-out2.google.com 2404:6800:8005::62 smtp-out3.google.com 2404:6800:8005::62 imap.google.com # 2404:6800:8005::62 gmail-pop.l.google.com 2404:6800:8005::62 gmail-smtp.l.google.com 2404:6800:8005::62 gmail-smtp-in.l.google.com 2404:6800:8005::62 gmr-smtp-in.l.google.com #Books 图书 2404:6800:8005::62 books.google.com #主页 #2404:6800:8005::64 www3.l.google.com 2404:6800:8005::62 bks0.books.google.com 2404:6800:8005::62 bks1.books.google.com 2404:6800:8005::62 bks2.books.google.com 2404:6800:8005::62 bks3.books.google.com 2404:6800:8005::62 bks4.books.google.com 2404:6800:8005::62 bks5.books.google.com 2404:6800:8005::62 bks6.books.google.com 2404:6800:8005::62 bks7.books.google.com 2404:6800:8005::62 bks8.books.google.com 2404:6800:8005::62 bks9.books.google.com #Finance 财经 2404:6800:8005::62 finance.google.com #Translate 翻译 2404:6800:8005::62 translate.google.com #Blog 博客搜索 2404:6800:8005::63 blogsearch.google.com #2404:6800:8005::63 www2.l.google.com #Calendar 日历 2404:6800:8005::64 calendar.google.com #2404:6800:8005::64 www3.l.google.com #Photo/Picasa 照片/网络相册 2404:6800:8005::5d photos.google.com #2404:6800:8005::5d picasaweb.l.google.com 2404:6800:8005::63 picasa.google.com #2404:6800:8005::63 www2.l.google.com 2404:6800:8005::be picasaweb.google.com #2404:6800:8005::be picasaweb.l.google.com 2404:6800:8005::62 lh0.ggpht.com 2404:6800:8005::62 lh1.ggpht.com 2404:6800:8005::62 lh2.ggpht.com 2404:6800:8005::62 lh3.ggpht.com 2404:6800:8005::62 lh4.ggpht.com 2404:6800:8005::62 lh5.ggpht.com 2404:6800:8005::62 lh6.ggpht.com 2404:6800:8005::62 lh7.ggpht.com 2404:6800:8005::62 lh8.ggpht.com 2404:6800:8005::62 lh9.ggpht.com #Docs 文档 2404:6800:8005::64 docs.google.com 2404:6800:8005::64 writely.l.google.com 2404:6800:8005::62 spreadsheet.google.com 2404:6800:8005::62 spreadsheets.google.com 2404:6800:8005::62 spreadsheets0.google.com 2404:6800:8005::62 spreadsheets.l.google.com 2404:6800:8005::62 writely.google.com 2404:6800:8005::62 writely.l.google.com 2404:6800:8005::62 writely-com.l.google.com 2404:6800:8005::62 writely-china.l.google.com #Reader 阅读器 2404:6800:8005::68 reader.google.com 2404:6800:8005::68 www2.l.google.com #Sites 协作平台 2404:6800:8005::65 sites.google.com #2404:6800:8005::65 www3.l.google.com #2404:6800:8005::62 ghs.google.com #2404:6800:8005::62 ghs.l.google.com #Group 论坛 2404:6800:8005::62 groups.google.com 2404:6800:8005::62 groups.l.google.com 2404:6800:8005::89 *.googlegroups.com 2404:6800:8005::89 blob-s-docs.googlegroups.com 2404:6800:8005::89 2503061233288453901-a-1802744773732722657-s-sites.googlegroups.com #Scholar 学术搜索 2404:6800:8005::62 scholar.google.com 2404:6800:8005::62 scholar.l.google.com #Tools 工具 2404:6800:8005::62 tools.google.com 2404:6800:8005::62 tools.l.google.com #Code 代码 2404:6800:8005::64 code.google.com #主页 2404:6800:8005::64 code.l.google.com # 2404:6800:8005::52 *.googlecode.com # 2404:6800:8005::52 chromium.googlecode.com # 2404:6800:8005::52 searchforchrome.googlecode.com # 2404:6800:8005::52 android-scripting.googlecode.com #Android Scripting Environment 2404:6800:8005::52 earth-api-samples.googlecode.com # 2404:6800:8005::52 gmaps-samples-flash.googlecode.com # 2404:6800:8005::52 google-code-feed-gadget.googlecode.com 2404:6800:8005::52 china-addthis.googlecode.com # 2404:6800:8005::52 get-flash-videos.googlecode.com #get-flash-videos 2404:6800:8005::52 youplayer.googlecode.com #YouPlayer 2404:6800:8005::52 cclive.googlecode.com #ccLive #Labs 实验室 2404:6800:8005::65 labs.google.com #2404:6800:8005::65 www3.l.google.com 2404:6800:8005::62 www.googlelabs.com 2404:6800:8005::62 browsersize.googlelabs.com #Browser Size 2404:6800:8005::62 storegadget.googlelabs.com #Google Checkout Store Gadget 2404:6800:8005::62 citytours.googlelabs.com #City Tours 2404:6800:8005::62 livingstories.googlelabs.com #Living Stories 2404:6800:8005::62 image-swirl.googlelabs.com #Image Swirl 2404:6800:8005::62 scriptconv.googlelabs.com #Script Converter 2404:6800:8005::62 relatedlinks.googlelabs.com #Related Links 2404:6800:8005::62 fastflip.googlelabs.com #Fast Flip 2404:6800:8005::62 listen.googlelabs.com #Google Listen 2404:6800:8005::62 similar-images.googlelabs.com #Similar Images 2404:6800:8005::62 tables.googlelabs.com #Fusion Tables 2404:6800:8005::62 newstimeline.googlelabs.com #Google News Timeline #Knol 在线百科全书 2404:6800:8005::65 knol.google.com #2404:6800:8005::65 www3.l.google.com #SketchUp 3D建模工具 2404:6800:8005::62 sketchup.google.com #2404:6800:8005::62 sketchup.l.google.com #Pack 软件精选 2404:6800:8005::68 pack.google.com #2404:6800:8005::68 www2.l.google.com 2404:6800:8005::68 cache.pack.google.com #Blogger 博客服务 2404:6800:8005::bf www.blogger.com 2404:6800:8005::bf buttons.blogger.com 2404:6800:8005::bf beta.blogger.com 2404:6800:8005::bf draft.blogger.com #Blogger 测试区 2404:6800:8005::bf status.blogger.com #Blogger 状态 2404:6800:8005::bf help.blogger.com #支持中心 2404:6800:8005::bf buzz.blogger.com #Blogger Buzz博客（英文） 2404:6800:8005::bf photos1.blogger.com 2404:6800:8005::bf bp0.blogger.com 2404:6800:8005::62 blogger.google.com 2404:6800:8005::62 blogger.l.google.com 2404:6800:8005::62 www.blogblog.com 2404:6800:8005::62 www1.blogblog.com 2404:6800:8005::62 www2.blogblog.com 2404:6800:8005::62 img.blogblog.com 2404:6800:8005::62 img1.blogblog.com 2404:6800:8005::62 img2.blogblog.com 2404:6800:8005::62 img.blshe.com #Blogspot 博客服务 2404:6800:8005::62 www.blogspot.com #主页 #2404:6800:8005::62 blogger.l.google.com 2404:6800:8005::62 blogsofnote.blogspot.com #留言博客（英文版本） 2404:6800:8005::62 knownissues.blogspot.com #已知问题 2404:6800:8005::62 1.bp.blogspot.com # 2404:6800:8005::62 2.bp.blogspot.com # 2404:6800:8005::62 3.bp.blogspot.com # 2404:6800:8005::62 4.bp.blogspot.com # 2404:6800:8005::62 googleblog.blogspot.com #Official Google Blog 2404:6800:8005::62 googlesystem.blogspot.com #Google Operating System 2404:6800:8005::62 googlechromereleases.blogspot.com #Google Chrome Releases 2404:6800:8005::62 youtube-global.blogspot.com #YouTube Blog 2404:6800:8005::62 igoogledeveloper.blogspot.com #iGoogle Developer Blog 2404:6800:8005::62 google-code-featured.blogspot.com #Featured Projects on Google Code 2404:6800:8005::62 googlegeodevelopers.blogspot.com #Google Geo Developers Blog 2404:6800:8005::62 googlecustomsearch.blogspot.com #Google Custom Search Blog 2404:6800:8005::62 chinafreenet.blogspot.com #中国自由网 2404:6800:8005::62 gregmankiw.blogspot.com #GREG MANKIW'S BLOG 2404:6800:8005::62 xiangeliushui.blogspot.com #年华似水，岁月如歌 2404:6800:8005::62 chinagfw.blogspot.com #GFW Blog 2404:6800:8005::62 wallpapers-arena.blogspot.com #Wallpapers Arena 2404:6800:8005::62 ggq.blogspot.com #GG圈 2404:6800:8005::62 whiteappleer.blogspot.com #WA＋ER 2404:6800:8005::62 rain-reader.blogspot.com #Nostalgia: Those Who Remain 2404:6800:8005::62 unityteam1.blogspot.com #生活圈 BLOG 2404:6800:8005::62 ipv6-or-no-ipv6.blogspot.com #IPv6 Related Stuff 2404:6800:8005::62 autoproxy2pac.appspot.com # 2404:6800:8005::62 gysj.blogspot.com # 2404:6800:8005::62 szncu.blogspot.com # #2404:6800:8005::62 *.blogspot.com #可以添加你自己的博客地址到这里 #Checkout 买家 2404:6800:8005::73 checkout.google.com #2404:6800:8005::73 checkout.l.google.com #Orkut 网络社区（貌似错误） #2404:6800:8005::62 orkut.google.com #2404:6800:8005::62 orkut.l.google.com #2404:6800:8005::62 www.orkut.com #2404:6800:8005::62 clients1.orkut.com #Toolbar 工具栏 2404:6800:8005::62 toolbar.google.com #2404:6800:8005::62 tools.l.google.com 2404:6800:8005::62 www.gmailnotifier.com #Gmail Notifier #App Engine 2404:6800:8005::64 appengine.google.com #主页 #2404:6800:8005::64 www3.l.google.com 2404:6800:8005::62 appspot.l.google.com # 2404:6800:8005::62 chart.apis.google.com #Google 图表 API 2404:6800:8005::5f *.googleapis.com 2404:6800:8005::5f translate.googleapis.com #Google 翻译 API 2404:6800:8005::5f ajax.googleapis.com #Ajax API 2404:6800:8005::8d *.appspot.com 2404:6800:8005::8d productideas.appspot.com #Google 汇问 2404:6800:8005::8d wave-api.appspot.com #Google Wave API 2404:6800:8005::8d wave-skynet.appspot.com #SkyNet 2404:6800:8005::8d cactus-wave.appspot.com # 2404:6800:8005::8d storegadgetwizard.appspot.com #Google Checkout Store Gadget 2404:6800:8005::8d moderator.appspot.com #Google Moderator 2404:6800:8005::8d haiticrisis.appspot.com #Google Person Finder: Haiti Earthquake 2404:6800:8005::8d mytracks.appspot.com #My Tracks for Android 2404:6800:8005::8d reader2twitter.appspot.com #Reader2Tweet 2404:6800:8005::8d twitese.appspot.com 2404:6800:8005::8d gfw.appspot.com 2404:6800:8005::8d go2china9.appspot.com 2404:6800:8005::8d mirrorrr.appspot.com 2404:6800:8005::8d mirrornt.appspot.com 2404:6800:8005::8d soproxy.appspot.com 2404:6800:8005::8d so-proxy.appspot.com 2404:6800:8005::8d go-west.appspot.com 2404:6800:8005::8d proxytea.appspot.com 2404:6800:8005::8d sivanproxy.appspot.com 2404:6800:8005::8d proxybay.appspot.com 2404:6800:8005::8d ipgoto.appspot.com 2404:6800:8005::8d meme2028.appspot.com 2404:6800:8005::8d autoproxy2pac.appspot.com #Chrome 谷歌浏览器 2404:6800:8005::64 chrome.google.com #Chromium OS 2404:6800:8005::62 goto.ext.google.com #2404:6800:8005::62 ghs.l.google.com #Desktop 桌面 2404:6800:8005::62 desktop.google.com 2404:6800:8005::62 desktop.l.google.com #Google Earth Google地球 2404:6800:8005::65 earth.google.com #2404:6800:8005::65 www3.l.google.com #Google Mars Google火星地图 2404:6800:8005::65 mars.google.com #2404:6800:8005::65 www3.l.google.com #Panoramio 2001:4860:8010::8d www.panoramio.com #2001:4860:8010::8d appspot.l.google.com 2001:4860:8010::8d static.panoramio.com #Keyhole 地理查询软件 2404:6800:8005::62 www.keyhole.com 2404:6800:8005::62 geo.keyhole.com 2404:6800:8005::62 dev.keyhole.com 2404:6800:8005::62 auth.keyhole.com #iGoogle Modules Google小工具 2404:6800:8005::62 gmodules.com 2404:6800:8005::62 www.gmodules.com 2404:6800:8005::62 www.ig.gmodules.com 2404:6800:8005::62 ig.gmodules.com 2404:6800:8005::62 ads.gmodules.com 2404:6800:8005::62 p.gmodules.com 2404:6800:8005::62 1.ig.gmodules.com 2404:6800:8005::62 2.ig.gmodules.com 2404:6800:8005::62 3.ig.gmodules.com 2404:6800:8005::62 4.ig.gmodules.com 2404:6800:8005::62 5.ig.gmodules.com 2404:6800:8005::62 6.ig.gmodules.com 2404:6800:8005::62 maps.gmodules.com 2404:6800:8005::62 img0.gmodules.com 2404:6800:8005::62 img1.gmodules.com 2404:6800:8005::62 img2.gmodules.com 2404:6800:8005::62 img3.gmodules.com 2404:6800:8005::62 skins.gmodules.com 2404:6800:8005::62 friendconnect.gmodules.com 2404:6800:8005::62 mc8tdi0ripmbpds25eboaupdulritrp6.friendconnect.gmodules.com 2404:6800:8005::62 r1rk9np7bpcsfoeekl0khkd2juj27q3o.friendconnect.gmodules.com 2404:6800:8005::62 r1rk9np7bpcsfoeekl0khkd2juj27q3o.a.friendconnect.gmodules.com ##Google其他服务 #Ajax 2404:6800:8005::62 googleapis-ajax.google.com #2404:6800:8005::62 googleapis-ajax.l.google.com #YouTube 203.208.46.29 youtube.com 203.208.46.29 www.youtube.com 203.208.46.29 gdata.youtube.com 203.208.46.29 m.youtube.com 203.208.46.29 help.youtube.com 74.125.71.116 upload.youtube.com 203.208.46.29 accounts.youtube.com 203.208.46.29 insight.youtube.com 203.208.46.29 apiblog.youtube.com 203.208.46.29 clients1.youtube.com 203.208.46.29 s.youtube.com 203.208.46.29 s2.youtube.com 203.208.46.29 s.ytimg.com 203.208.46.29 i1.ytimg.com 203.208.46.29 i2.ytimg.com 203.208.46.29 i3.ytimg.com 203.208.46.29 i4.ytimg.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache1.c.youtube.com 203.208.46.29 dn.net 125.56.199.9 photos-d.ak.fbcdn.net 125.56.199.9 photos-e.ak.fbcdn.net 125.56.199.9 photos-f.ak.fbcdn.net 125.56.199.9 photos-g.ak.fbcdn.net 125.56.199.9 photos-h.ak.fbcdn.neto-o.preferred.sjc07s15.v14.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache1.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache2.c.youtube.com 203.208.46.29 dn.net 125.56.199.9 photos-d.ak.fbcdn.net 125.56.199.9 photos-e.ak.fbcdn.net 125.56.199.9 photos-f.ak.fbcdn.net 125.56.199.9 photos-g.ak.fbcdn.net 125.56.199.9 photos-h.ak.fbcdn.neto-o.preferred.sjc07s15.v8.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache2.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache3.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache4.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache5.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache6.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v24.lscache7.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v1.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v2.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v3.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v4.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v5.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v6.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v7.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v8.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v9.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v10.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v11.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v12.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v13.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v14.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v15.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v16.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v17.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v18.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v19.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v20.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v21.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v22.lscache8.c.youtube.com 203.208.46.29 o-o.preferred.sjc07s15.v23.lscache8.c.youtube.com 203.208.46.29 dn.net 125.56.199.9 photos-d.ak.fbcdn.net 125.56.199.9 photos-e.ak.fbcdn.net 125.56.199.9 photos-f.ak.fbcdn.net 125.56.199.9 photos-g.ak.fbcdn.net 125.56.199.9 photos-h.ak.fbcdn.neto-o.preferred.sjc07s15.v24.lscache8.c.youtube.com 203.208.46.29 r1.pek01s01.c.youtube.com 203.208.46.29 r2.pek01s01.c.youtube.com 203.208.46.29 r3.pek01s01.c.youtube.com 203.208.46.29 r4.pek01s01.c.youtube.com 203.208.46.29 r5.pek01s01.c.youtube.com 203.208.46.29 r6.pek01s01.c.youtube.com 203.208.46.29 r7.pek01s01.c.youtube.com 203.208.46.29 r8.pek01s01.c.youtube.com 203.208.46.29 r9.pek01s01.c.youtube.com 203.208.46.29 r10.pek01s01.c.youtube.com 203.208.46.29 r11.pek01s01.c.youtube.com 203.208.46.29 r12.pek01s01.c.youtube.com 203.208.46.29 r13.pek01s01.c.youtube.com 203.208.46.29 r14.pek01s01.c.youtube.com 203.208.46.29 r15.pek01s01.c.youtube.com 203.208.46.29 r16.pek01s01.c.youtube.com 203.208.46.29 r17.pek01s01.c.youtube.com 203.208.46.29 r18.pek01s01.c.youtube.com 203.208.46.29 r19.pek01s01.c.youtube.com 203.208.46.29 r20.pek01s01.c.youtube.com 203.208.46.29 r21.pek01s01.c.youtube.com 203.208.46.29 r22.pek01s01.c.youtube.com 203.208.46.29 r23.pek01s01.c.youtube.com 203.208.46.29 r24.pek01s01.c.youtube.com 203.208.46.29 tc.v1.cache1.c.youtube.com 203.208.46.29 tc.v2.cache1.c.youtube.com 203.208.46.29 tc.v3.cache1.c.youtube.com 203.208.46.29 tc.v4.cache1.c.youtube.com 203.208.46.29 tc.v5.cache1.c.youtube.com 203.208.46.29 tc.v6.cache1.c.youtube.com 203.208.46.29 tc.v7.cache1.c.youtube.com 203.208.46.29 tc.v8.cache1.c.youtube.com 203.208.46.29 tc.v9.cache1.c.youtube.com 203.208.46.29 tc.v10.cache1.c.youtube.com 203.208.46.29 tc.v11.cache1.c.youtube.com 203.208.46.29 tc.v12.cache1.c.youtube.com 203.208.46.29 tc.v13.cache1.c.youtube.com 203.208.46.29 tc.v14.cache1.c.youtube.com 203.208.46.29 tc.v15.cache1.c.youtube.com 203.208.46.29 tc.v16.cache1.c.youtube.com 203.208.46.29 tc.v17.cache1.c.youtube.com 203.208.46.29 tc.v18.cache1.c.youtube.com 203.208.46.29 tc.v19.cache1.c.youtube.com 203.208.46.29 tc.v20.cache1.c.youtube.com 203.208.46.29 tc.v21.cache1.c.youtube.com 203.208.46.29 tc.v22.cache1.c.youtube.com 203.208.46.29 tc.v23.cache1.c.youtube.com 203.208.46.29 tc.v24.cache1.c.youtube.com 203.208.46.29 tc.v1.cache2.c.youtube.com 203.208.46.29 tc.v2.cache2.c.youtube.com 203.208.46.29 tc.v3.cache2.c.youtube.com 203.208.46.29 tc.v4.cache2.c.youtube.com 203.208.46.29 tc.v5.cache2.c.youtube.com 203.208.46.29 tc.v6.cache2.c.youtube.com 203.208.46.29 tc.v7.cache2.c.youtube.com 203.208.46.29 tc.v8.cache2.c.youtube.com 203.208.46.29 tc.v9.cache2.c.youtube.com 203.208.46.29 tc.v10.cache2.c.youtube.com 203.208.46.29 tc.v11.cache2.c.youtube.com 203.208.46.29 tc.v12.cache2.c.youtube.com 203.208.46.29 tc.v13.cache2.c.youtube.com 203.208.46.29 tc.v14.cache2.c.youtube.com 203.208.46.29 tc.v15.cache2.c.youtube.com 203.208.46.29 tc.v16.cache2.c.youtube.com 203.208.46.29 tc.v17.cache2.c.youtube.com 203.208.46.29 tc.v18.cache2.c.youtube.com 203.208.46.29 tc.v19.cache2.c.youtube.com 203.208.46.29 tc.v20.cache2.c.youtube.com 203.208.46.29 tc.v21.cache2.c.youtube.com 203.208.46.29 tc.v22.cache2.c.youtube.com 203.208.46.29 tc.v23.cache2.c.youtube.com 203.208.46.29 tc.v24.cache2.c.youtube.com 203.208.46.29 tc.v1.cache3.c.youtube.com 203.208.46.29 tc.v2.cache3.c.youtube.com 203.208.46.29 tc.v3.cache3.c.youtube.com 203.208.46.29 tc.v4.cache3.c.youtube.com 203.208.46.29 tc.v5.cache3.c.youtube.com 203.208.46.29 tc.v6.cache3.c.youtube.com 203.208.46.29 tc.v7.cache3.c.youtube.com 203.208.46.29 tc.v8.cache3.c.youtube.com 203.208.46.29 tc.v9.cache3.c.youtube.com 203.208.46.29 tc.v10.cache3.c.youtube.com 203.208.46.29 tc.v11.cache3.c.youtube.com 203.208.46.29 tc.v12.cache3.c.youtube.com 203.208.46.29 tc.v13.cache3.c.youtube.com 203.208.46.29 tc.v14.cache3.c.youtube.com 203.208.46.29 tc.v15.cache3.c.youtube.com 203.208.46.29 tc.v16.cache3.c.youtube.com 203.208.46.29 tc.v17.cache3.c.youtube.com 203.208.46.29 tc.v18.cache3.c.youtube.com 203.208.46.29 tc.v19.cache3.c.youtube.com 203.208.46.29 tc.v20.cache3.c.youtube.com 203.208.46.29 tc.v21.cache3.c.youtube.com 203.208.46.29 tc.v22.cache3.c.youtube.com 203.208.46.29 tc.v23.cache3.c.youtube.com 203.208.46.29 tc.v24.cache3.c.youtube.com 203.208.46.29 tc.v1.cache4.c.youtube.com 203.208.46.29 tc.v2.cache4.c.youtube.com 203.208.46.29 tc.v3.cache4.c.youtube.com 203.208.46.29 tc.v4.cache4.c.youtube.com 203.208.46.29 tc.v5.cache4.c.youtube.com 203.208.46.29 tc.v6.cache4.c.youtube.com 203.208.46.29 tc.v7.cache4.c.youtube.com 203.208.46.29 tc.v8.cache4.c.youtube.com 203.208.46.29 tc.v9.cache4.c.youtube.com 203.208.46.29 tc.v10.cache4.c.youtube.com 203.208.46.29 tc.v11.cache4.c.youtube.com 203.208.46.29 tc.v12.cache4.c.youtube.com 203.208.46.29 tc.v13.cache4.c.youtube.com 203.208.46.29 tc.v14.cache4.c.youtube.com 203.208.46.29 tc.v15.cache4.c.youtube.com 203.208.46.29 tc.v16.cache4.c.youtube.com 203.208.46.29 tc.v17.cache4.c.youtube.com 203.208.46.29 tc.v18.cache4.c.youtube.com 203.208.46.29 tc.v19.cache4.c.youtube.com 203.208.46.29 tc.v20.cache4.c.youtube.com 203.208.46.29 tc.v21.cache4.c.youtube.com 203.208.46.29 tc.v22.cache4.c.youtube.com 203.208.46.29 tc.v23.cache4.c.youtube.com 203.208.46.29 tc.v24.cache4.c.youtube.com 203.208.46.29 tc.v1.cache5.c.youtube.com 203.208.46.29 tc.v2.cache5.c.youtube.com 203.208.46.29 tc.v3.cache5.c.youtube.com 203.208.46.29 tc.v4.cache5.c.youtube.com 203.208.46.29 tc.v5.cache5.c.youtube.com 203.208.46.29 tc.v6.cache5.c.youtube.com 203.208.46.29 tc.v7.cache5.c.youtube.com 203.208.46.29 tc.v8.cache5.c.youtube.com 203.208.46.29 tc.v9.cache5.c.youtube.com 203.208.46.29 tc.v10.cache5.c.youtube.com 203.208.46.29 tc.v11.cache5.c.youtube.com 203.208.46.29 tc.v12.cache5.c.youtube.com 203.208.46.29 tc.v13.cache5.c.youtube.com 203.208.46.29 tc.v14.cache5.c.youtube.com 203.208.46.29 tc.v15.cache5.c.youtube.com 203.208.46.29 tc.v16.cache5.c.youtube.com 203.208.46.29 tc.v17.cache5.c.youtube.com 203.208.46.29 tc.v18.cache5.c.youtube.com 203.208.46.29 tc.v19.cache5.c.youtube.com 203.208.46.29 tc.v20.cache5.c.youtube.com 203.208.46.29 tc.v21.cache5.c.youtube.com 203.208.46.29 tc.v22.cache5.c.youtube.com 203.208.46.29 tc.v23.cache5.c.youtube.com 203.208.46.29 tc.v24.cache5.c.youtube.com 203.208.46.29 tc.v1.cache6.c.youtube.com 203.208.46.29 tc.v2.cache6.c.youtube.com 203.208.46.29 tc.v3.cache6.c.youtube.com 203.208.46.29 tc.v4.cache6.c.youtube.com 203.208.46.29 tc.v5.cache6.c.youtube.com 203.208.46.29 tc.v6.cache6.c.youtube.com 203.208.46.29 tc.v7.cache6.c.youtube.com 203.208.46.29 tc.v8.cache6.c.youtube.com 203.208.46.29 tc.v9.cache6.c.youtube.com 203.208.46.29 tc.v10.cache6.c.youtube.com 203.208.46.29 tc.v11.cache6.c.youtube.com 203.208.46.29 tc.v12.cache6.c.youtube.com 203.208.46.29 tc.v13.cache6.c.youtube.com 203.208.46.29 tc.v14.cache6.c.youtube.com 203.208.46.29 tc.v15.cache6.c.youtube.com 203.208.46.29 tc.v16.cache6.c.youtube.com 203.208.46.29 tc.v17.cache6.c.youtube.com 203.208.46.29 tc.v18.cache6.c.youtube.com 203.208.46.29 tc.v19.cache6.c.youtube.com 203.208.46.29 tc.v20.cache6.c.youtube.com 203.208.46.29 tc.v21.cache6.c.youtube.com 203.208.46.29 tc.v22.cache6.c.youtube.com 203.208.46.29 tc.v23.cache6.c.youtube.com 203.208.46.29 tc.v24.cache6.c.youtube.com 203.208.46.29 tc.v1.cache7.c.youtube.com 203.208.46.29 tc.v2.cache7.c.youtube.com 203.208.46.29 tc.v3.cache7.c.youtube.com 203.208.46.29 tc.v4.cache7.c.youtube.com 203.208.46.29 tc.v5.cache7.c.youtube.com 203.208.46.29 tc.v6.cache7.c.youtube.com 203.208.46.29 tc.v7.cache7.c.youtube.com 203.208.46.29 tc.v8.cache7.c.youtube.com 203.208.46.29 tc.v9.cache7.c.youtube.com 203.208.46.29 tc.v10.cache7.c.youtube.com 203.208.46.29 tc.v11.cache7.c.youtube.com 203.208.46.29 tc.v12.cache7.c.youtube.com 203.208.46.29 tc.v13.cache7.c.youtube.com 203.208.46.29 tc.v14.cache7.c.youtube.com 203.208.46.29 tc.v15.cache7.c.youtube.com 203.208.46.29 tc.v16.cache7.c.youtube.com 203.208.46.29 tc.v17.cache7.c.youtube.com 203.208.46.29 tc.v18.cache7.c.youtube.com 203.208.46.29 tc.v19.cache7.c.youtube.com 203.208.46.29 tc.v20.cache7.c.youtube.com 203.208.46.29 tc.v21.cache7.c.youtube.com 203.208.46.29 tc.v22.cache7.c.youtube.com 203.208.46.29 tc.v23.cache7.c.youtube.com 203.208.46.29 tc.v24.cache7.c.youtube.com 203.208.46.29 tc.v1.cache8.c.youtube.com 203.208.46.29 tc.v2.cache8.c.youtube.com 203.208.46.29 tc.v3.cache8.c.youtube.com 203.208.46.29 tc.v4.cache8.c.youtube.com 203.208.46.29 tc.v5.cache8.c.youtube.com 203.208.46.29 tc.v6.cache8.c.youtube.com 203.208.46.29 tc.v7.cache8.c.youtube.com 203.208.46.29 tc.v8.cache8.c.youtube.com 203.208.46.29 tc.v9.cache8.c.youtube.com 203.208.46.29 tc.v10.cache8.c.youtube.com 203.208.46.29 tc.v11.cache8.c.youtube.com 203.208.46.29 tc.v12.cache8.c.youtube.com 203.208.46.29 tc.v13.cache8.c.youtube.com 203.208.46.29 tc.v14.cache8.c.youtube.com 203.208.46.29 tc.v15.cache8.c.youtube.com 203.208.46.29 tc.v16.cache8.c.youtube.com 203.208.46.29 tc.v17.cache8.c.youtube.com 203.208.46.29 tc.v18.cache8.c.youtube.com 203.208.46.29 tc.v19.cache8.c.youtube.com 203.208.46.29 tc.v20.cache8.c.youtube.com 203.208.46.29 tc.v21.cache8.c.youtube.com 203.208.46.29 tc.v22.cache8.c.youtube.com 203.208.46.29 tc.v23.cache8.c.youtube.com 203.208.46.29 tc.v24.cache8.c.youtube.com 203.208.46.29 v1.lscache1.c.youtube.com 203.208.46.29 v2.lscache1.c.youtube.com 203.208.46.29 v3.lscache1.c.youtube.com 203.208.46.29 v4.lscache1.c.youtube.com 203.208.46.29 v5.lscache1.c.youtube.com 203.208.46.29 v6.lscache1.c.youtube.com 203.208.46.29 v7.lscache1.c.youtube.com 203.208.46.29 v8.lscache1.c.youtube.com 203.208.46.29 v9.lscache1.c.youtube.com 203.208.46.29 v10.lscache1.c.youtube.com 203.208.46.29 v11.lscache1.c.youtube.com 203.208.46.29 v12.lscache1.c.youtube.com 203.208.46.29 v13.lscache1.c.youtube.com 203.208.46.29 v14.lscache1.c.youtube.com 203.208.46.29 v15.lscache1.c.youtube.com 203.208.46.29 v16.lscache1.c.youtube.com 203.208.46.29 v17.lscache1.c.youtube.com 203.208.46.29 v18.lscache1.c.youtube.com 203.208.46.29 v19.lscache1.c.youtube.com 203.208.46.29 v20.lscache1.c.youtube.com 203.208.46.29 v21.lscache1.c.youtube.com 203.208.46.29 v22.lscache1.c.youtube.com 203.208.46.29 v23.lscache1.c.youtube.com 203.208.46.29 v24.lscache1.c.youtube.com 203.208.46.29 v1.lscache2.c.youtube.com 203.208.46.29 v2.lscache2.c.youtube.com 203.208.46.29 v3.lscache2.c.youtube.com 203.208.46.29 v4.lscache2.c.youtube.com 203.208.46.29 v5.lscache2.c.youtube.com 203.208.46.29 v6.lscache2.c.youtube.com 203.208.46.29 v7.lscache2.c.youtube.com 203.208.46.29 v8.lscache2.c.youtube.com 203.208.46.29 v9.lscache2.c.youtube.com 203.208.46.29 v10.lscache2.c.youtube.com 203.208.46.29 v11.lscache2.c.youtube.com 203.208.46.29 v12.lscache2.c.youtube.com 203.208.46.29 v13.lscache2.c.youtube.com 203.208.46.29 v14.lscache2.c.youtube.com 203.208.46.29 v15.lscache2.c.youtube.com 203.208.46.29 v16.lscache2.c.youtube.com 203.208.46.29 v17.lscache2.c.youtube.com 203.208.46.29 v18.lscache2.c.youtube.com 203.208.46.29 v19.lscache2.c.youtube.com 203.208.46.29 v20.lscache2.c.youtube.com 203.208.46.29 v21.lscache2.c.youtube.com 203.208.46.29 v22.lscache2.c.youtube.com 203.208.46.29 v23.lscache2.c.youtube.com 203.208.46.29 v24.lscache2.c.youtube.com 203.208.46.29 v1.lscache3.c.youtube.com 203.208.46.29 v2.lscache3.c.youtube.com 203.208.46.29 v3.lscache3.c.youtube.com 203.208.46.29 v4.lscache3.c.youtube.com 203.208.46.29 v5.lscache3.c.youtube.com 203.208.46.29 v6.lscache3.c.youtube.com 203.208.46.29 v7.lscache3.c.youtube.com 203.208.46.29 v8.lscache3.c.youtube.com 203.208.46.29 v9.lscache3.c.youtube.com 203.208.46.29 v10.lscache3.c.youtube.com 203.208.46.29 v11.lscache3.c.youtube.com 203.208.46.29 v12.lscache3.c.youtube.com 203.208.46.29 v13.lscache3.c.youtube.com 203.208.46.29 v14.lscache3.c.youtube.com 203.208.46.29 v15.lscache3.c.youtube.com 203.208.46.29 v16.lscache3.c.youtube.com 203.208.46.29 v17.lscache3.c.youtube.com 203.208.46.29 v18.lscache3.c.youtube.com 203.208.46.29 v19.lscache3.c.youtube.com 203.208.46.29 v20.lscache3.c.youtube.com 203.208.46.29 v21.lscache3.c.youtube.com 203.208.46.29 v22.lscache3.c.youtube.com 203.208.46.29 v23.lscache3.c.youtube.com 203.208.46.29 v24.lscache3.c.youtube.com 203.208.46.29 v1.lscache4.c.youtube.com 203.208.46.29 v2.lscache4.c.youtube.com 203.208.46.29 v3.lscache4.c.youtube.com 203.208.46.29 v4.lscache4.c.youtube.com 203.208.46.29 v5.lscache4.c.youtube.com 203.208.46.29 v6.lscache4.c.youtube.com 203.208.46.29 v7.lscache4.c.youtube.com 203.208.46.29 v8.lscache4.c.youtube.com 203.208.46.29 v9.lscache4.c.youtube.com 203.208.46.29 v10.lscache4.c.youtube.com 203.208.46.29 v11.lscache4.c.youtube.com 203.208.46.29 v12.lscache4.c.youtube.com 203.208.46.29 v13.lscache4.c.youtube.com 203.208.46.29 v14.lscache4.c.youtube.com 203.208.46.29 v15.lscache4.c.youtube.com 203.208.46.29 v16.lscache4.c.youtube.com 203.208.46.29 v17.lscache4.c.youtube.com 203.208.46.29 v18.lscache4.c.youtube.com 203.208.46.29 v19.lscache4.c.youtube.com 203.208.46.29 v20.lscache4.c.youtube.com 203.208.46.29 v21.lscache4.c.youtube.com 203.208.46.29 v22.lscache4.c.youtube.com 203.208.46.29 v23.lscache4.c.youtube.com 203.208.46.29 v24.lscache4.c.youtube.com 203.208.46.29 v1.lscache5.c.youtube.com 203.208.46.29 v2.lscache5.c.youtube.com 203.208.46.29 v3.lscache5.c.youtube.com 203.208.46.29 v4.lscache5.c.youtube.com 203.208.46.29 v5.lscache5.c.youtube.com 203.208.46.29 v6.lscache5.c.youtube.com 203.208.46.29 v7.lscache5.c.youtube.com 203.208.46.29 v8.lscache5.c.youtube.com 203.208.46.29 v9.lscache5.c.youtube.com 203.208.46.29 v10.lscache5.c.youtube.com 203.208.46.29 v11.lscache5.c.youtube.com 203.208.46.29 v12.lscache5.c.youtube.com 203.208.46.29 v13.lscache5.c.youtube.com 203.208.46.29 v14.lscache5.c.youtube.com 203.208.46.29 v15.lscache5.c.youtube.com 203.208.46.29 v16.lscache5.c.youtube.com 203.208.46.29 v17.lscache5.c.youtube.com 203.208.46.29 v18.lscache5.c.youtube.com 203.208.46.29 v19.lscache5.c.youtube.com 203.208.46.29 v20.lscache5.c.youtube.com 203.208.46.29 v21.lscache5.c.youtube.com 203.208.46.29 v22.lscache5.c.youtube.com 203.208.46.29 v23.lscache5.c.youtube.com 203.208.46.29 v24.lscache5.c.youtube.com 203.208.46.29 v1.lscache6.c.youtube.com 203.208.46.29 v2.lscache6.c.youtube.com 203.208.46.29 v3.lscache6.c.youtube.com 203.208.46.29 v4.lscache6.c.youtube.com 203.208.46.29 v5.lscache6.c.youtube.com 203.208.46.29 v6.lscache6.c.youtube.com 203.208.46.29 v7.lscache6.c.youtube.com 203.208.46.29 v8.lscache6.c.youtube.com 203.208.46.29 v9.lscache6.c.youtube.com 203.208.46.29 v10.lscache6.c.youtube.com 203.208.46.29 v11.lscache6.c.youtube.com 203.208.46.29 v12.lscache6.c.youtube.com 203.208.46.29 v13.lscache6.c.youtube.com 203.208.46.29 v14.lscache6.c.youtube.com 203.208.46.29 v15.lscache6.c.youtube.com 203.208.46.29 v16.lscache6.c.youtube.com 203.208.46.29 v17.lscache6.c.youtube.com 203.208.46.29 v18.lscache6.c.youtube.com 203.208.46.29 v19.lscache6.c.youtube.com 203.208.46.29 v20.lscache6.c.youtube.com 203.208.46.29 v21.lscache6.c.youtube.com 203.208.46.29 v22.lscache6.c.youtube.com 203.208.46.29 v23.lscache6.c.youtube.com 203.208.46.29 v24.lscache6.c.youtube.com 203.208.46.29 v1.lscache7.c.youtube.com 203.208.46.29 v2.lscache7.c.youtube.com 203.208.46.29 v3.lscache7.c.youtube.com 203.208.46.29 v4.lscache7.c.youtube.com 203.208.46.29 v5.lscache7.c.youtube.com 203.208.46.29 v6.lscache7.c.youtube.com 203.208.46.29 v7.lscache7.c.youtube.com 203.208.46.29 v8.lscache7.c.youtube.com 203.208.46.29 v9.lscache7.c.youtube.com 203.208.46.29 v10.lscache7.c.youtube.com 203.208.46.29 v11.lscache7.c.youtube.com 203.208.46.29 v12.lscache7.c.youtube.com 203.208.46.29 v13.lscache7.c.youtube.com 203.208.46.29 v14.lscache7.c.youtube.com 203.208.46.29 v15.lscache7.c.youtube.com 203.208.46.29 v16.lscache7.c.youtube.com 203.208.46.29 v17.lscache7.c.youtube.com 203.208.46.29 v18.lscache7.c.youtube.com 203.208.46.29 v19.lscache7.c.youtube.com 203.208.46.29 v20.lscache7.c.youtube.com 203.208.46.29 v21.lscache7.c.youtube.com 203.208.46.29 v22.lscache7.c.youtube.com 203.208.46.29 v23.lscache7.c.youtube.com 203.208.46.29 v24.lscache7.c.youtube.com 203.208.46.29 v1.lscache8.c.youtube.com 203.208.46.29 v2.lscache8.c.youtube.com 203.208.46.29 v3.lscache8.c.youtube.com 203.208.46.29 v4.lscache8.c.youtube.com 203.208.46.29 v5.lscache8.c.youtube.com 203.208.46.29 v6.lscache8.c.youtube.com 203.208.46.29 v7.lscache8.c.youtube.com 203.208.46.29 v8.lscache8.c.youtube.com 203.208.46.29 v9.lscache8.c.youtube.com 203.208.46.29 v10.lscache8.c.youtube.com 203.208.46.29 v11.lscache8.c.youtube.com 203.208.46.29 v12.lscache8.c.youtube.com 203.208.46.29 v13.lscache8.c.youtube.com 203.208.46.29 v14.lscache8.c.youtube.com 203.208.46.29 v15.lscache8.c.youtube.com 203.208.46.29 v16.lscache8.c.youtube.com 203.208.46.29 v17.lscache8.c.youtube.com 203.208.46.29 v18.lscache8.c.youtube.com 203.208.46.29 v19.lscache8.c.youtube.com 203.208.46.29 v20.lscache8.c.youtube.com 203.208.46.29 v21.lscache8.c.youtube.com 203.208.46.29 v22.lscache8.c.youtube.com 203.208.46.29 v23.lscache8.c.youtube.com 203.208.46.29 v24.lscache8.c.youtube.com #Twitter 199.59.148.84 oauth.twitter.com 199.59.148.84 twitter.com 199.59.148.84 www.twitter.com 199.59.148.84 api.twitter.com 199.59.148.201 search.twitter.com 199.59.148.139 userstream.twitter.com 199.59.148.84 ssl.twitter.com 199.59.148.84 status.twitter.com 199.59.148.84 assets0.twitter.com 199.59.148.84 assets1.twitter.com 199.59.148.84 assets2.twitter.com 199.59.148.84 assets3.twitter.com 199.59.148.84 static.twitter.com 184.29.36.124 platform.twitter.com 219.76.10.138 platform0.twitter.com 199.59.148.206 help.twitter.com 199.59.148.206 support.twitter.com 209.84.4.102 si0.twimg.com 209.84.4.102 si1.twimg.com 209.84.4.102 si2.twimg.com 209.84.4.102 si3.twimg.com 209.84.4.102 si4.twimg.com 209.84.4.102 si5.twimg.com #Facebook 脸谱网（尚未完全部署） 69.63.189.16 facebook.com 69.63.189.16 www.facebook.com 69.63.181.31 m.facebook.com 69.63.181.20 login.facebook.com 69.63.179.70 secure.facebook.com 66.220.146.18 apps.facebook.com 69.63.181.31 touch.facebook.com 118.214.114.110 s-static.ak.facebook.com 66.220.147.47 api.facebook.com 69.63.181.16 zh-CN.facebook.com 202.157.186.28 static.ak.facebook.com 202.157.186.34 b.static.ak.facebook.com 69.63.178.57 secure-profile.facebook.com 69.63.178.57 secure-media-sf2p.facebook.com 69.63.178.15 ssl.facebook.com 69.63.190.18 apps.facebook.com 118.214.190.105 profile.ak.facebook.com #Facebook web 69.63.187.17 fbcdn.net 97.65.135.139 external.ak.fbcdn.net 124.155.222.50 vthumb.ak.fbcdn.net 97.65.135.163 static.ak.fbcdn.net 97.65.135.163 b.static.ak.fbcdn.net 202.157.186.34 creative.ak.fbcdn.net 118.214.190.128 profile.ak.fbcdn.net 69.63.176.21 s-hprofile-sf2p.fbcdn.net 125.56.199.9 photos-a.ak.fbcdn.net 125.56.199.9 photos-b.ak.fbcdn.net 125.56.199.9 photos-c.ak.fbcdn.net 125.56.199.9 photos-d.ak.fbcdn.net 125.56.199.9 photos-e.ak.fbcdn.net 125.56.199.9 photos-f.ak.fbcdn.net 125.56.199.9 photos-g.ak.fbcdn.net 125.56.199.9 photos-h.ak.fbcdn.net","tags":"soft","title":"修改host访问google的所有服务"},{"url":"http://x-wei.github.io/pelican_github_blog.html","text":"2016更新 新博客用了pelican3, 参考 这里 折腾了许久, 终于把 我的博客 搞得差不多了, 在此写一个总结, 以免自己以后忘了, 并且给和我一样菜的人提供一点参考.... 先扯点别的 其实啊, 很早就想要建立自己的博客, 把值得分享的东西拿出来放到网上, 但是又不屑于使用网易, 百度等提供的现成服务, 技术又很菜... 于是一直拖着. zim的出现让我很欣喜--zim可以写类似于博客的东西(不过是给自己看的~), 记录有价值的内容. 但是怎么把我的一些总结放到网上?? 我先后考虑了这些东西: googlesite-->wordpress-->jekyll+github-->pelican+github googlesite是个很好的工具, 很容易上手 (google好赞...), 我曾经用它做过一个个人页面. 但是这种傻瓜工具的缺点就是: 没法自己定制... 当我发现googlesite的bolg页面不支持标签云的时候, 就决定不用它了... 况且googlesite在国内需要修改一下host才能访问... 然后是wordpress, 这个似乎目前也是最流行的网页制作工具, 我看到了很多很多大牛小牛使用WP搭建的自己的网站, 而且都是自己的顶级域名, 看上去就灰常霸气~ 当我终于有空折腾, 兴冲冲地研究WP时, 却发现 顶级域名注册都是要交钱的 , 还要弄什么vpn...这... 大概不适合我... 在我纠结的时候, 请教了 dofine 同学, 他推荐我使用 github pages (后来证明这是非常正确的~). 早就听说git大名, 只是我太菜了... 不知能不能搞定啊... 还好可以随时询问dofine同学(有时我的问题很弱智, 他还是很耐心的回答, 真好~). github page弄好后, 要按装jekyll作为 静态页面 的生成工具. 在使用zim的时候, 我就对轻量级标记语言非常喜欢, jekyll可以使用Markdown格式(比zim的wiki语法还要简洁), 所以说应该是非常好的选择. 但是我在jekyll的安装这一步卡住了 ... 由于我用的是ubuntu10.04, 安装各种报错... 更新了rubygem之后以为成功了, 但是运行jekyll还是不行... 折腾了两天后, 我决定暂时放弃了... 就在纠结之时, 报着试一试的态度, 在 yssy 的gnu/linux版上问了一下, 得到了 farseerfc 学长非常热情耐心的回答. 他建议我 采用pelican 来生成静态页面, 这是一个法国人用python写的程序. 我很容易就安装好了, 然后又折腾了许久, 现在终于基本搞定...... 感觉pelican还是相当不错的选择, 配置好了之后就可以安心写文章了... 第一步: 生成github page 第一步要做的就是注册github, 生成一个自己的二级域名(比如我的x-wei.github.com). 注册和配置SSH密钥过程 help page 写得很清楚, 虽然我连SSH是什么都搞不清, 按它说的一步一步做, 很容易就搞定了. 然后要新建一个repo(中文翻译成\"依赖\"?), 注意 这个repo需要命名成: your_id.github.com , 所以一个id只能生成一个啦... 生成这个repo后会有提示, 运行一下mkdir, git init什么的就OK了. 这样, 建立好了your_id.github.com的repo之后, 只要把一个index.html文件上传到master分支, 就可以访问your_id.github.com看到那个index.html文件了~ 第二步: 安装和使用pelican pelican的安装需要用到pip: $sudo apt-get install python-pip 然后再用pip安装pelican: $sudo pip install pelican 这样, 安装就完成了~ pelican的使用很简单, 这是帮助信息: $ pelican -h usage: pelican [ -h ] [ -t THEME ] [ -o OUTPUT ] [ -m MARKUP ] [ -s SETTINGS ] [ -d ] [ -v ] [ -q ] [ -D ] [ --version ] [ -r ] [ path ] A tool to generate a static blog, with restructured text input files. positional arguments: path Path where to find the content files optional arguments: -h, --help show this help message and exit -t THEME, --theme-path THEME Path where to find the theme templates. If not specified, itwill use the default one included with pelican. -o OUTPUT, --output OUTPUT Where to output the generated files. If not specified, a directory will be created, named \"output\" in the current path. -m MARKUP, --markup MARKUP the list of markup language to use ( rst or md ) . Please indicate them separated by commas -s SETTINGS, --settings SETTINGS the settings of the application. Default to False. -d, --delete-output-directory Delete the output directory. -v, --verbose Show all messages -q, --quiet Show only critical errors -D, --debug Show all message, including debug messages --version Print the pelican version and exit -r, --autoreload Relaunch pelican each time a modification occurs on the contentfiles 额, 其实那些参数可以先无视(直接用默认参数)... 那么用法就很简单了: $pelican [path] , 其中, path是放置markdown或rst文件的目录. 如果手头有几篇.md文件或.rst文件, 那么只要: $pelican [.md/.rst文件所在目录] 就会看到效果了... 大概会在一个'output'目录里, 打开index.html就可以看到生成的页面, 只要把这些生成的文件push到github的master分支, 你的博客就建好了~~ 另外, 把.md文件分别放在几个子目录, 那么生成的页面显示属于不同分类的文章了~ 关于pelican的配置, 待会再说, 先说说git的上传... 第三步: 编辑.md/.rst文件 markdown和rst都是非常优秀的轻量级标记语言, 可以很方便的写出整洁漂亮的笔记, 编写博客文章只要写成一个一个的.md或.rst文件然后交给pelican就OK了. 关于这两种格式的语法, 其实我自己还不太熟悉呢... 网上有不少教程, 比如这个 markdown的教程 和 这个ReST教程 ... 需要注意的是, 在文章的开头要指定一下博客的信息: 博客标题, 时间, 标签... pelican的帮助页面各提供了一个示例(我稍微修改了一下): .rst示例 My super title ############## :date: 2010-10-03 10:20 :tags: tag1, tag2 :category: yeah //如果把这个rst文件放在posts/下的子目录的话, 那么这一行可以省略, 默认把子文件夹名作为分类 :author: Alexis Metaireau //由于settings文件已经指定了作者. 这一行可以省略 :slug: test-blog //这个是指定生成页面的名称, 比如这个是指定生成的页面名字是\"test-blog.html\" 这里写博客内容... .md示例 Date : 2010 - 12 - 03 Title : My super title Slug : test - blog Tags : tag1 , tag2 这里写博客内容 ... 另: 关于编辑器 编辑这类文件时最好能够预览效果, linux下用 ReText 即可~ 第四步: 把生成的文件上传到github 以前没用过git, 所以这个让我困惑了很长时间... 首先, 应该在your_id.github.com页面下有一个.git文件夹(大概是git init生成的吧), 然后, 把生成好了的那些文件(比如上一步的output文件夹里的东西)放在这个目录下, 依次运行以下三个命令: $ git add . $ git commit -am \"your commit message\" $ git push 额 是的, 需要三条命令才能完成上传... 另外, 貌似这样会覆盖掉原先的那些文件, 不必担心, github有history功能(我的理解 可能跟快照有点类似吧), 原先的东西应该可以找回来... push完成后, 你的注册邮箱会收到邮件\"page built successful\", 如果是第一次生成的话, 最多等10分钟, 你就可以访问your_id.github.com看到效果啦~~ 第五步: pelican的进一步配置 如果按照默认的参数, 直接$pelican path的话, 估计不会得到让你满意的页面--至少网站名字要改一下吧!! 还有, 默认的主题没有标签云, 反正我比较想要这个功能... farseerfc给了一个 settings.py 配置文件, 各个变量的名字含义应该比较清楚, 或者看pelican的 帮助页面 , 这个页面也提供了一个示例配置文件. 可以在这俩配置文件基础上进行修改... 修改完成了之后, 运行pelican时加上-s参数指定settings.py作为配置文件: $pelican -s settings.py [.md/.rst文件所在目录] 我是从farseerfc的配置文件改的, 大概是这个样子: # -*- coding: utf-8 -*- import sys TIMEZONE = 'Asia/Shanghai' DEFAULT_LANG = 'zhs' SITENAME = \"X. Wei's Blog\" AUTHOR = 'X.Wei' DISQUS_SITENAME = 'xweisblog' GITHUB_URL = '<https://github.com/X-Wei>' #github链接 SITEURL = '<http://x-wei.github.com>' GOOGLE_ANALYTICS = 'UA-30756331-1' #谷歌站点分析 TAG_FEED = 'feeds/ %s .atom.xml' DEFAULT_PAGINATION = 4 #默认每一页有多少篇文章 DEFAULT_CATEGORY = 'misc' OUTPUT_PATH = '.' #需要把输出路径从默认的'output'改成根目录(your_id.github.com目录), 因为githubpage需要把index.html上传到repo的master分支的根目录才可以! PATH = 'posts' #这个是指定放置.md/.rst文件的目录 LINKS = (( 'dofine' , '<http://www.dofine.me>' ), ( 'farseerfc' , \"<http://farseerfc.github.com/>\" ), ) #友情链接~ SOCIAL = ( ( 'github' , '<https://github.com/x-wei>' ), ) #社交网络链接 #~ ('twitter', '<http://twitter.com/farseerfc>'), #~ ('facebook', '<http://www.facebook.com/farseerfc>'), #~ ('weibo', '<http://weibo.com/farseerfc>'), #~ ('renren', '<http://www.renren.com/farseer>'), #这个是farseerfc同学自己加的, 可以显示他的新浪微博内容, 有微博的话可以把这些加上~ #~ TWITTER_USERNAME = 'farseerfc' #~ SIDEBAR_CUSTOM = r\"\"\" #~ <li class=\"nav-header\"><h4><i class=\"icon-list-alt\"></i>Weibo</h4></li> #~ <iframe width=\"100%\" height=\"550\" class=\"share_self\" frameborder=\"0\" scrolling=\"no\" #~ src=\"<http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=1&ptype=1&speed=0&skin=2&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=1862842353&verifier=b193b9de&dpc=1>\"> #~ </iframe> #~ \"\"\" #google自定义搜索(大概是站内搜索吧) #~ GOOGLE_CUSTOM_SEARCH_SIDEBAR = \"001578481551708017171:axpo6yvtdyg\" #~ GOOGLE_CUSTOM_SEARCH_NAVBAR = \"001578481551708017171:hxkva69brmg\" 由于配置文件里已经包含了PATH和OUTPUT_PATH什么的, 所以运行只要: $pelican -s settings.py 即可~ 然后, 关于主题模板, 可以使用-t参数指定主题. pelican目前主题在 github 上, 可以用$git clone https://github.com/farseerfc/pelican-themes 下载, 然后使用主题的话就是: $pelican -s settings.py -t [主题所在目录] [.md/.rst文件所在目录] 如果想生成后就预览一下, 那就写: $pelican -s settings.py -t [主题所在目录] [.md/.rst文件所在目录] | xdg-open index.html 个人感觉还是bootstrap2主题比较好... 目前由于pelican使用的人不多, 所以主题也就那十几个... 再罗嗦一下: 关于一些问题的解决方法 这几天折腾pelican很久很久, 发现以下几个问题: 不支持多层分类? --这个目前没法解决, 不过既然有标签功能, 分类不能多层也无所谓啦... 安装后不支持Markdown语法? 这个是因为Markdown没有与pelican包一起安装(pelican默认的还是支持rst格式), 安装上Markdown包就行了: $sudo pip install Markdown 如何插图? 这里不是讨论markdown语法如何插图, 而是怎么让生成的网页有图片. 其实很简单, 把.md文件里引用的图片复制一份到static目录即可~ (其实剪切也是可以的, 只不过那样的话编辑预览时就看不见图片了...) 中文tag不支持?(事后证明其实是支持的, 是我搞得不支持了...) pelican2.8(当前版本)是支持中文tag的! 我一开始直接git clone了3.0的源码, 然后不支持了... 需要重装pelican: $ sudo pip uninstall pelican $ sudo pip install pelican 不支持中文文件名( 2012-04-25: XXXXX!! 可以啊, 谁说不可以... ) 这个我没解决, 无所谓, 起一个英文文件名(.md文件以及Slug最好都用英文吧)好了, 毕竟博客标题(Title)是可以用中文的~ OK, 就写这些吧... (累死了...) (我发现几乎所有文章都是从zim里搬来的, so, 以后就不在文章最后标注\"imported from zim\"了...)","tags":"soft","title":"用pelican在github上创建自己的博客!"},{"url":"http://x-wei.github.io/hello-world.html","text":"建立起了我的github博客~! 作为一个很菜的低端桌面(ubuntu, gnome)用户, 这个还是相当不容易啊~~ 本来考虑使用的是jekyll, 但是安装过程老是报错... 最终选择了pelican生成静态页面, 这个工具很好用, 只是目前关注度不足啊~ 关于pelican的介绍可以参考 这里 , jekyll我折腾了N久没有搞定, 而pelican很快就装好了~ 非常感谢 farseerfc 同学, 非常耐心地回答我这个菜鸟的问题... 不过还是自力更生比较好, 好好看它的 文档 吧~ 另外, 我之前用 zim 写了一些笔记, 准备把值得分享的部分放到blog上来. zim0.56刚刚开始支持把笔记导出成markdown格式, 这实在是太棒啦!!","tags":"misc","title":"Hello World!"},{"url":"http://x-wei.github.io/google_doc_form.html","text":"在线办公我以前试过zoho, 但是一个xls文件有多个人同时编辑时会有问题... 昨天收到学长关于春游的 调查表 , 居然是用google doc做的, 很给力. 我早就想做一个分布式通讯录, 所以研究了一下. 越来越感觉google提供的服务(google doc, google site, google project)太方便了!!! 建立 大概的流程是这样的: 用google帐号进入google doc, 然后点击creat->form, 会看到这样的页面: 嗯, 功能是比较简单但是够用! 点击add item 或者右边俩方框的按钮, 就可以添加问题了, 很easy, 我做的同学录是这个样子的: 然后还可以选择主题... 发布 编辑完成之后, 就可以发布了, 点击右上角的\"Share\"(有个g+的图标, 我一开始以为是像人人那样的分享呢...), 之后页面底部就会有一个链接~! 把这个链接发给别人, 别人就可以提交了~ btw, 如果不想发布, 只想发给几个人看的话, 点击share右边的那个\"email this form\"按钮, 输入联系人的邮箱地址即可~ 统计 那么别人提交之后, 我怎么看统计结果呢? 进入google doc, 会发现刚才建立的form的结果在一个表单里: 点进去看, 就可以看到大家的回复统计 看! 多方便!! 然后, 可以把这个文件共享给别人: 右键->share: 发布后的修改 那么, form发布后, 还可不可以再修改呢? --可以! 打开google doc里的表单, form->edit form, 就会弹出编辑form的窗口啦!! 注意那个more action里面的confirmation, 不要勾选\"publish response summery\"...否则任何人提交后就可以看到目前所有的信息了... 一点不足 就是一旦发布了, 任何有这个链接的人都可以填这个表格... 没有密码之类的... 我的方法是设置一个必填的\"验证问题\"项, 只有回答对了验证问题的人, 我才发给他同学录... imported from zim","tags":"soft","title":"使用google doc建立在线调查表!~"},{"url":"http://x-wei.github.io/nautilus-menu.html","text":"Nautilus是gnome的默认文件浏览器, 那次更新安装了elementory主题的Nautilus之后, 发现菜单栏少了些很常用的功能: 比如上一层,主页, 刷新等... 其实可以自定义的, 编辑->customize toolbar: 顺便提一下, 发现了些方便的快捷键: alt+up: 父目录 alt+left: 返回 ~~alt+home: 到~/~~(不好使?) F2: 重命名 F5/ctrl+R: 刷新 ctrl+L: 输入位置地址 ctrl+H:显示隐藏文件 --imported from zim","tags":"soft","title":"自定义nautilus的菜单栏"},{"url":"http://x-wei.github.io/recursion-scr-shot.html","text":"就放一张图片, 知道怎么做的吗?~ btw, 真心感觉zim可以作为一个写给自己的博客了, 不仅仅是记笔记... BON!~ --imported from zim","tags":"misc","title":"无聊中的发现--递归截图~"},{"url":"http://x-wei.github.io/tex_insert_code.html","text":"这个问题... 我本来想用python解决的... 但是显然应该先搜一下吧... 果然, 早就有人解决了(其实是tex的常用命令里就有的), 比如 这里 ... \\ usepackage { listings } \\ lstset { language = C ++ } % 这条命令可以让 LaTeX 排版时将 C ++ 键字突出显示 \\ lstset { breaklines } % 这条命令可以让 LaTeX 自动将长的代码行换行排版 \\ lstset { extendedchars = false } % 这一条命令可以解决代码跨页时，章节标题，页眉等汉字不显示的问题 \\ begin { lstlisting } %paste your C ++ code here \\ end { lstlisting } 很简单的... 不过比较长的代码换行显示不是很爽(貌似不换行也不是办法啊)... 另外没有颜色高亮哎... 嗯, 貌似 这里 的介绍更详细... 还有这个人的 博客 ... 这篇文章 是针对python的高亮... 总结一下, 这样比较好: \\documentclass { article } \\usepackage { listings } \\usepackage { xcolor } \\usepackage { xeCJK } %必须加xeCJK包 \\setCJKmainfont { WenQuanYi Micro Hei } \\begin { document } \\lstset { numbers=left, numberstyle= \\tiny , keywordstyle= \\color { blue!70 } , commentstyle= \\color { red!50!green!50!blue!50 } , frame=shadowbox, rulesepcolor= \\color { red!20!green!20!blue!20 } , breaklines=true, extendedchars=true } \\begin { lstlisting } [language= { Python } ] %这里插入代码~ \\end { lstlisting } \\end { document } 附件: 我做的一个简单实例 ./insertcode2.tex imported from zim","tags":"soft","title":"tex插入程序代码--so easy~"},{"url":"http://x-wei.github.io/xelatex_zh.html","text":"前几天校内上看见了这个 latex中文指南 , 想试一试... 可是貌似不给力(or我没做对)安装了texlive之后还是不能编译它的测试源文件... 搜索发现了 这样一篇 非常强大的文章... 这货直接把他的源文件放上去了... 不过这样的话他的文章可读性就不好了... 要进行的操作: sudo apt-get install texlive-xetex latex-cjk-xcjk texlive-latex-recommended 其实好像这样之后就可以用中文了(按博客里的意思), 可惜我用的geany开始使用latex编译的, 老是报错... 其实应该用命令行 xelatex xx.tex就应该好使了... 然后我又按照博客里的提示安装了Gummi...... 一个可以在右面看到效果的texIDE... 还是不行, 因为默认的编译器都是latex不是xelatex... 当然我最终发现了要用xelatex, 那么老是用命令行也不大方便... gummi里面找到了设置: 这样就好了... 但是我现在已经非常喜欢geany了, 不想因为tex再单独用一个编辑环境... 还好找到了geany里面的设置: 生成-->设置生成命令, 在里面添加一个xelatex就行啦!~ 嗯 还是geany亲切~! 最后把源文件生成好的pdf也放进来吧... ./xelatex_test.tex ./xelatex 及中文 Gummi 在 ubuntu 上的配置.pdf imported from zim","tags":"soft","title":"xelatex--linux下tex中文的完全解决!"},{"url":"http://x-wei.github.io/chrome-background.html","text":"直接把 这里 的贴上吧... (以下为copy) 首先，下载安装chrome的 stylist插件 然后，打开\"扩展设置\"，点击chrome stylist的选项，点击demo进行修改。 把网页背景修改为豆沙绿的参数设置: 输入框1：demo 选项框2：regexp 输入框3：(ftp|http|https)://\\D 输入框4： * { background: #C7EDCC !important; } 修改后保存即可 (url和style text可根据自己喜好配置) 附：豆沙绿的参数 RGB颜色 199；237；204 十六位颜色代码 #C7EDCC 色调：85；饱和度：123；亮度：205 不会的话直接用 这个扩展 。 04/24/2012续 关于那个正则表达式, 如果写成 *{ background: #C7EDCC !important;} , 虽然一片绿豆色很护眼, 但是不少网页显示会有问题. 比如校内上新鲜事显示不了照片预览, gmail的加星标签看不见等... 最坑爹的是有的网页文字是浅色的, 那样的话几乎看不清楚了... 我现在使用的这样的规则: body { background : #C7EDCC !important ;} body { color : black } 第一行, 指定只是网页的body部分为绿豆沙色(大部分网页的背景都是body); 第二行, 指定body里的文字都使用黑色. 这样弄下来比原先要好不少(关于上面提到的显示校内网和gmail的问题都解决了), 虽然网页不是全部绿豆沙了... --imported from zim","tags":"soft","title":"chrome护眼设置--把背景设置为绿豆沙"},{"url":"http://x-wei.github.io/chrome输出网页为pdf!.html","text":"不用装插件, 直接右键-->打印-->打印到文件 即可! 太方便了! 关键是pdf分页很合理, 效果超好! 03/05/2012续 打印前需要把网页缩放足够小, 否则打印的文件只是一部分网页.","tags":"soft","title":"chrome输出网页为pdf!"},{"url":"http://x-wei.github.io/gnome-background.html","text":"我就说win能做到ubuntu也能~ 主题-->自定义-->颜色 修改窗口和输入框两项的颜色: 126-12-91 现在就很舒服了 然后貌似nautils要再设置一下: 编辑-->背景和徽标-->颜色-->把那个绿豆沙颜色加进去 再加上[chrome的设置], 嗯 就完美了! --imported from zim","tags":"soft","title":"gnome护眼设置--窗口背景设置为绿豆沙"},{"url":"http://x-wei.github.io/use_du_shell.html","text":"du用来计算目录的磁盘用量. 具体的参数可以用man或者--help, 这里不贴了(其实我也没仔细看...). 今天看见水源上有人说 用 -sh这个参数比较好: -s: 只计算各个目录的总用量(就是说不要递归操作) 后面跟着 或者. 就可以查看各个目录的大小了. -h: 易于查看的方式 比如~/目录经常不知道为什么空间在减少, 那就运行: du -sh ~/* 以及 du -sh /.* 即可 有时候文件夹比较多的情况, 还是不容易发现那个文件夹占用了大部分空间, 这时用sort命令对du的结果进行排序就行了! 参考了 这里 , 顺便学会了: 两条一起执行是用\"|\"进行分割的. 所以命令为: du -sm ~/* | sort -nr 注意这时du不能用-h参数, 因为这样的话文件可能是以M为单位也可能是以k为单位, 而sort的时候只看前面的数字值, 不看单位. 用-m参数, 表示让所有结果以M为单位显示. imported from zim","tags":"soft","title":"用du命令查看各目录大小"},{"url":"http://x-wei.github.io/cmus-很棒的终端音乐播放器.html","text":"最近发现一个终端下超好用的音乐播放器: CMUS . 界面简洁, vi的按键绑定, 由于最近越来越感觉键盘和快捷键的方便, 对这个迷你的播放器爱不释手. 关于它的用法可以参考 这里 , 另外 这里 还介绍了怎么设置replay gain(大概是不同音乐播放的音量相同), 但我没有设置. 网站上的文档不多, 我觉得最好的教程还是man: man cmus man cmus-tutorial 把tutorial看完就基本上会用了~ 这里列一下我觉得常用的几点吧: 1~7共7种view, 用数字键就可以切换; 使用命令要想vi一样加':', 常见命令有: :cd xx_dir :add xx_dir c--暂停, hl/<>--快进快退, x--播放, v--停止 右下角显示播放模式: R表示重复, S随机, C连续(播完一曲后不停)","tags":"soft","title":"cmus--很棒的终端音乐播放器"},{"url":"http://x-wei.github.io/zip乱码解决.html","text":"这个问题困扰了很久, 以前的方法参考了 这里 , 使用一条命令: unzip -O CP936 xxx.zip 但是谁tm记得住? 所以每次都要上网现查... 今天看到了ubuntu论坛上的 帖子 , 六楼给出了终极的解决方案. 见附件: zip乱码解决.zip 这个压缩包中的5个 7z* 文件拷贝覆盖到/usr/lib/p7zip/ 代码: sudo cp 7z* /usr/lib/p7zip/ 注意以后不要升级p7zip 一切就正常了! 而且打开时也没有乱码! so good!","tags":"soft","title":"zip乱码解决"},{"url":"http://x-wei.github.io/scrshot-shortcut.html","text":"终端输入gnome-screenshot --help, 发现选项\"-a\"表示的是抓取一部分屏幕. 于是打开编辑键盘快捷键窗口, 新建一个快捷键如下图: 这样, 按下Ctrl+Alt+s快捷键后即可启动抓图, 且是抓取一个区域. 不过不知为什么, 反应比较慢, 需要按下一段时间 (一秒钟?)才会有反应. 02/23/2012续: 安装了lucid之后不好使了! 症状就是, 参数-a不起作用, 按下快捷键后直接出来桌面截图, 如果一直按着的话会出来选取区域的, 但是此时已经把桌面截了N次了!.... 无语啊! 上网搜了好久,终于在 奶牛的博客 里看见了方法: 命令参数变成 -ai, 这样每次按下快捷键后会先弹出来交互界面... 也罢... 我非常无语... 嗯 就这样吧~... --imported from zim","tags":"soft","title":"把抓图工具关联到ubuntu快捷键"}]}