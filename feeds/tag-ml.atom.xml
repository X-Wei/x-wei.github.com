<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's blog - ml</title><link href="http://x-wei.github.io/" rel="alternate"></link><link href="http://x-wei.github.io/feeds/tag-ml.atom.xml" rel="self"></link><id>http://x-wei.github.io/</id><updated>2016-05-21T00:00:00+02:00</updated><entry><title>在java程序里使用weka进行机器学习</title><link href="http://x-wei.github.io/java-use-weka.html" rel="alternate"></link><published>2016-05-21T00:00:00+02:00</published><updated>2016-05-21T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-05-21:/java-use-weka.html</id><summary type="html">&lt;p&gt;[TOC] &lt;/p&gt;
&lt;p&gt;之前一直用weka的GUI界面做机器学习的任务, 感觉这个软件虽然界面丑, 不过确实是快速开展机器学期的利器. 关于GUI的weka使用以后有时间再写. 今天这篇记录一下最近使用的java版本的weka.   &lt;/p&gt;
&lt;h1&gt;1. Include jars into project&lt;/h1&gt;
&lt;p&gt;weka官网的下载链接里选择linux版本的weka压缩包即可, 下载以后找到weka.jar文件, 在工程里将其include一下就可以使用了(btw, 现在开始放弃eclipse, 进入IDEA的怀抱了...).   &lt;/p&gt;
&lt;p&gt;weka的文档在解压缩的文件里有, 另外在线文档在: &lt;a href="http://weka.sourceforge.net/doc.stable-3-8/"&gt;http://weka.sourceforge.net/doc.stable-3-8/&lt;/a&gt; &lt;/p&gt;
&lt;h3&gt;about libsvm...&lt;/h3&gt;
&lt;p&gt;关于libsvm需要有一点特别指出. weka自带的算法里是不包含libsvm的 (有个类似的SMO, 不过还是libsvm久经考验啊...), 需要使用weka的package manager安装. 打开package manager是在weka主界面的菜单里: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/java-use-weka/pasted_image002.png"&gt;&lt;br&gt;
在package manager里搜索到libsvm安装即可. 然后(linux下)在主目录可以看到有个wekafiles文件夹, &lt;code&gt;wekafiles/packages/LibSVM/&lt;/code&gt;目录下就是libsvm的内容.   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;需要指出的一点是, 要使用libsvm的话, &lt;a href="http://stackoverflow.com/questions/30821926/solved-weka-api-libsvm-classpath-not-found"&gt;需要同时引用两个jar文件&lt;/a&gt;, 而且都叫libsvm.jar …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC] &lt;/p&gt;
&lt;p&gt;之前一直用weka的GUI界面做机器学习的任务, 感觉这个软件虽然界面丑, 不过确实是快速开展机器学期的利器. 关于GUI的weka使用以后有时间再写. 今天这篇记录一下最近使用的java版本的weka.   &lt;/p&gt;
&lt;h1&gt;1. Include jars into project&lt;/h1&gt;
&lt;p&gt;weka官网的下载链接里选择linux版本的weka压缩包即可, 下载以后找到weka.jar文件, 在工程里将其include一下就可以使用了(btw, 现在开始放弃eclipse, 进入IDEA的怀抱了...).   &lt;/p&gt;
&lt;p&gt;weka的文档在解压缩的文件里有, 另外在线文档在: &lt;a href="http://weka.sourceforge.net/doc.stable-3-8/"&gt;http://weka.sourceforge.net/doc.stable-3-8/&lt;/a&gt; &lt;/p&gt;
&lt;h3&gt;about libsvm...&lt;/h3&gt;
&lt;p&gt;关于libsvm需要有一点特别指出. weka自带的算法里是不包含libsvm的 (有个类似的SMO, 不过还是libsvm久经考验啊...), 需要使用weka的package manager安装. 打开package manager是在weka主界面的菜单里: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/java-use-weka/pasted_image002.png"&gt;&lt;br&gt;
在package manager里搜索到libsvm安装即可. 然后(linux下)在主目录可以看到有个wekafiles文件夹, &lt;code&gt;wekafiles/packages/LibSVM/&lt;/code&gt;目录下就是libsvm的内容.   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;需要指出的一点是, 要使用libsvm的话, &lt;a href="http://stackoverflow.com/questions/30821926/solved-weka-api-libsvm-classpath-not-found"&gt;需要同时引用两个jar文件&lt;/a&gt;, 而且都叫libsvm.jar!!   &lt;/p&gt;
&lt;p&gt;这两个jar, 一个叫&lt;code&gt;LibSVM.jar&lt;/code&gt;, 在&lt;code&gt;wekafiles/packages/LibSVM/&lt;/code&gt;下, 另一个叫&lt;code&gt;libsvm.jar&lt;/code&gt;, 在&lt;code&gt;wekafiles/packages/LibSVM/lib/&lt;/code&gt;下...orz  &lt;/p&gt;
&lt;p&gt;如果只include第一个jar的话, 就会报错: "java.lang.Exception: libsvm classes not in CLASSPATH! ".   &lt;/p&gt;
&lt;h1&gt;2. terminology&lt;/h1&gt;
&lt;p&gt;首先统一一下各种东西的叫法...  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Instances&lt;/code&gt;: 就是dataset, 比如training set或者test set, Instances实际上就是一个Instance的集合  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Instance&lt;/code&gt;: 就是一个数据点了  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Attribute&lt;/code&gt;: 一个数据点有一些attribute (别处一般叫做feature), 其中有一个attribute其实是label(可以为missing)  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Classifier&lt;/code&gt;: weka里的Classifer其实是也包含了regressor或者cluster... 后面都称之为model  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Evaluation&lt;/code&gt;: 给定一个model和一个dataset, 给出evaluation的数据, 类似GUI界面给出的那些内容  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;3. 在程序里构建数据&lt;/h1&gt;
&lt;p&gt;这也是为什么要在java里用weka的原因: 如果数据可以直接以csv或者arff文件的方式得到, 那么直接在GUI界面下就可以搞了...  &lt;/p&gt;
&lt;h3&gt;新建Attribute&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://weka.sourceforge.net/doc.stable-3-8/weka/core/Attribute.html"&gt;http://weka.sourceforge.net/doc.stable-3-8/weka/core/Attribute.html&lt;/a&gt; &lt;br&gt;
新建numeric的attribue只要简单的在构造函数里传入一个attribute的名字即可:   &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;// Create numeric attributes "length" and "weight"   &lt;/span&gt;
&lt;span class="code-line"&gt;Attribute length = new Attribute("length");   &lt;/span&gt;
&lt;span class="code-line"&gt;Attribute weight = new Attribute("weight");&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;新建离散(normial)的attribue则需要一个list乘放所有可能的数值:   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;// Create list to hold nominal values "first", "second", "third"   &lt;/span&gt;
&lt;span class="code-line"&gt;List my_nominal_values = new ArrayList(3);   &lt;/span&gt;
&lt;span class="code-line"&gt;my_nominal_values.add("first");   &lt;/span&gt;
&lt;span class="code-line"&gt;my_nominal_values.add("second");   &lt;/span&gt;
&lt;span class="code-line"&gt;my_nominal_values.add("third");   &lt;/span&gt;
&lt;span class="code-line"&gt;// Create nominal attribute "position"   &lt;/span&gt;
&lt;span class="code-line"&gt;Attribute position = new Attribute("position", my_nominal_values);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;新建Instances(dataset)&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instances.html"&gt;http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instances.html&lt;/a&gt;&lt;br&gt;
&lt;code&gt;Instances&lt;/code&gt;实际上就是一个&lt;code&gt;Instance&lt;/code&gt;的集合, Instances可以类比为pandas里面的DataFrame, 然后每个instance相当于一行. 另外&lt;code&gt;Instances&lt;/code&gt;比&lt;code&gt;Instance&lt;/code&gt;多的就是Attribute信息(类比为pandas里DataFrame的表头head).   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Instances&lt;/code&gt;的构造函数有两种, 一种是直接在arff文件里读取, 这个后面再说. 另一种构造函数是在java函数里构建Instance时用的, 它构造一个空的Instance集合, 构造函数提供dataset的名字, attribute的集合(arraylist)以及初始的capacity:   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Instances(String name, ArrayList&amp;lt;Attribute&amp;gt; attInfo, int capacity)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;其中的第二个参数attInfo其实就相当于是表头信息了, 它是一个Attribute的ArrayList.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;ArrayList&amp;lt;Attribute&amp;gt; atts = new ArrayList&amp;lt;Attribute&amp;gt;();  &lt;/span&gt;
&lt;span class="code-line"&gt;atts.add(length);  &lt;/span&gt;
&lt;span class="code-line"&gt;atts.add(weight);  &lt;/span&gt;
&lt;span class="code-line"&gt;atts.add(position);  &lt;/span&gt;
&lt;span class="code-line"&gt;Instances adataset = new Instances("aDataSet", atts, 10);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(上面代码里的&lt;code&gt;length&lt;/code&gt;, &lt;code&gt;weight&lt;/code&gt; 和&lt;code&gt;position&lt;/code&gt;都是前面声明的Attribute对象)  &lt;/p&gt;
&lt;p&gt;Instances还可以指定哪一列对应的是class label (单个Instance则不能 — 因为单个Instance并没有表头信息attInfo):   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;void setClassIndex(int classIndex)&lt;/code&gt; &lt;/p&gt;
&lt;h3&gt;新建Instance&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instance.html"&gt;http://weka.sourceforge.net/doc.stable-3-8/weka/core/Instance.html&lt;/a&gt;&lt;br&gt;
&lt;code&gt;Instance&lt;/code&gt;是一个接口而不是一个类, 一半常用的是&lt;code&gt;DenseInstance&lt;/code&gt;类(它又继承自&lt;code&gt;AbstractInstance&lt;/code&gt;抽象类)   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;这里有一个坑: 一定要指定Instance所属的DataSet(既它属于哪一个Instances对象)再使用setValue函数, 否则在调用setValue的时候可能会有问题!!!   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;void setDataset(Instances instances)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;构造函数里只需要提供这个instance的attribute数量即可. 然后使用&lt;code&gt;setValue&lt;/code&gt;函数可以给每个attribue指定数值. setValue函数的第一个参数接收一个Attribue对象, 第二个参数就是这个attribue的数值(double或者string).   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;// Create empty instance with three attribute values   &lt;/span&gt;
&lt;span class="code-line"&gt;Instance inst = new DenseInstance(3);   &lt;/span&gt;
&lt;span class="code-line"&gt;instance.setDataset(  adataset); // before calling setValue, should first set the Dataset!``  &lt;/span&gt;
&lt;span class="code-line"&gt;// Set instance's values for the attributes "length", "weight", and "position"  &lt;/span&gt;
&lt;span class="code-line"&gt;inst.setValue(length, 5.3);   &lt;/span&gt;
&lt;span class="code-line"&gt;inst.setValue(weight, 300);   &lt;/span&gt;
&lt;span class="code-line"&gt;inst.setValue(position, "first");&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;4. 在程序里训练model&lt;/h1&gt;
&lt;p&gt;这里只做classification的例子好了. &lt;br&gt;
&lt;a href="http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/Classifier.html"&gt;http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/Classifier.html&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Classifier是一个Interface, 可以在文档里看到有很多类都实现了这个interface, 主要是三个常用的函数:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;void buildClassifier(Instances data)&lt;/code&gt; : 用data数据进行训练  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;double   classifyInstance(Instance instance)&lt;/code&gt; : 预测一个Instance的label  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;double[] distributionForInstance(Instance instance)&lt;/code&gt; :对于每一个可能的类, 给一个probability, 返回一个double数组  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以程序里训练model只需要调用&lt;code&gt;buildClassifier()&lt;/code&gt;函数即可.   &lt;/p&gt;
&lt;h1&gt;5. 从文件读入数据和model&lt;/h1&gt;
&lt;p&gt;之前讲的是在程序里得到数据, 在程序里储存结果的方法, 而如果可以保存数据到文件的话, 在GUI界面下调试模型应该更加放方便.   &lt;/p&gt;
&lt;p&gt;预先已经得到了数据的话, 可以先把数据保存问arff格式, 然后用GUI的weka训练和调试参数. 当得到满意的结果以后可以在GUI界面里选择保存训练好的模型(一个.model文件): &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/java-use-weka/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;然后, weka提供了非常方便的方法, 直接从arff文件里得到&lt;code&gt;Instances&lt;/code&gt;对象, 从model文件里得到&lt;code&gt;Classifier&lt;/code&gt;对象: &lt;br&gt;
(参考链接: &lt;a href="https://weka.wikispaces.com/Serialization"&gt;https://weka.wikispaces.com/Serialization&lt;/a&gt; 以及 &lt;a href="https://weka.wikispaces.com/Use+Weka+in+your+Java+code"&gt;https://weka.wikispaces.com/Use+Weka+in+your+Java+code&lt;/a&gt;)  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;Classifier clf = (Classifier) weka.core.SerializationHelper.read("/some/where/j48.model");  &lt;/span&gt;
&lt;span class="code-line"&gt;Instances testset = new Instances(new BufferedReader(new FileReader("/some/where/test.arff")));``&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;6. 输出Evaluation统计&lt;/h1&gt;
&lt;p&gt;当得到了训练好的模型&lt;code&gt;clf&lt;/code&gt;以及要使用的测试数据&lt;code&gt;testset&lt;/code&gt;以后, 可以在testset上测试模型, 并输出结果的统计数据. 这些是靠&lt;code&gt;Evaluation&lt;/code&gt;类完成的.&lt;br&gt;
&lt;a href="http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/evaluation/Evaluation.html"&gt;http://weka.sourceforge.net/doc.stable-3-8/weka/classifiers/evaluation/Evaluation.html&lt;/a&gt;&lt;br&gt;
Evaluation的构造函数里提供的Instances应该为training set, 这个训练集的作用是"to get some header information and prior class distribution information", 如果构造时给的是testing set的话, 应该调用&lt;code&gt;useNoPriors()&lt;/code&gt;函数一下.   &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;构造了evalation对象以后, 只要使用 &lt;code&gt;evaluateModel(Classifier classifier, Instances data)&lt;/code&gt; 函数即可, 第一个参数为训练好了的模型(&lt;code&gt;clf&lt;/code&gt;), 第二个参数为要用来测试的数据(&lt;code&gt;testset&lt;/code&gt;).   &lt;/p&gt;
&lt;p&gt;然后可以输出统计信息, 就像在wekaGUI界面一样, 主要靠&lt;code&gt;toSummaryString()&lt;/code&gt;和&lt;code&gt;toMatrixString()&lt;/code&gt;两个函数.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;Instances trainInstances = ... instances got from somewhere  &lt;/span&gt;
&lt;span class="code-line"&gt;Instances testInstances = ... instances got from somewhere  &lt;/span&gt;
&lt;span class="code-line"&gt;Classifier scheme = ... scheme got from somewhere&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;Evaluation evaluation = new Evaluation(trainInstances);  &lt;/span&gt;
&lt;span class="code-line"&gt;evaluation.evaluateModel(scheme, testInstances);  &lt;/span&gt;
&lt;span class="code-line"&gt;System.out.println(evaluation.toSummaryString());  &lt;/span&gt;
&lt;span class="code-line"&gt;System.out.println(evaluation.toMatrixString());&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;7. 输出预测结果&lt;/h1&gt;
&lt;p&gt;对于每一个Instance, 只需要调用Classifier的&lt;code&gt;classifyInstance(Instance instance)&lt;/code&gt;或者&lt;code&gt;distributionForInstance(Instance instance)&lt;/code&gt;函数, 即可得到预测结果...   &lt;/p&gt;
&lt;p&gt;Voila, 大概就是这样, weka这个工具还是蛮好用的(只要能忍受它界面的丑), 而且也算没有太多的坑...   &lt;/p&gt;</content><category term="ml"></category><category term="weka"></category><category term="java"></category></entry><entry><title>Approximate Retrieval(2): simHash</title><link href="http://x-wei.github.io/simhash.html" rel="alternate"></link><published>2015-10-08T17:30:00+02:00</published><updated>2015-10-08T17:30:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2015-10-08:/simhash.html</id><summary type="html">&lt;p&gt;除了上次介绍的&lt;a href="http://x-wei.github.io/minhash.html"&gt;minhash&lt;/a&gt;方法以外, 还有一种常见的hash方法, 叫做simHash. 这里做简要介绍.  &lt;br&gt;
这个hash函数的背景和上次一样, 还是考虑把文本抽象为ngram的集合:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/simhash/pasted_image.png"&gt; &lt;br&gt;
然后相似度依旧是Jaccard similarity:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/simhash/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;simHash&lt;/h2&gt;
&lt;p&gt;simHash的方法听上去比minHash还要简单:    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对一个文档&lt;em&gt;d&lt;/em&gt;中的每一个term(ngram, shingle) &lt;em&gt;t&lt;/em&gt;, 计算其hashcode(比如用java内建的&lt;code&gt;Object.hashCode()&lt;/code&gt;函数) &lt;em&gt;hash(t)&lt;/em&gt;.    &lt;/li&gt;
&lt;li&gt;把d中所有term的&lt;em&gt;hash(t)&lt;/em&gt;合成为一个hashcode作为d的hashcode &lt;em&gt;simHash(d)&lt;/em&gt;: &lt;em&gt;simHash(d)&lt;/em&gt;的长度与&lt;em&gt;hash(t)&lt;/em&gt;相同, &lt;em&gt;simHash(d)&lt;/em&gt;的第k个bit的取值为所有&lt;em&gt;hash(t)&lt;/em&gt;第k个bit的&lt;strong&gt;众数&lt;/strong&gt;.    &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;写成数学表达式很吓人, 其实只不过不断在{0,1}和{-1 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;除了上次介绍的&lt;a href="http://x-wei.github.io/minhash.html"&gt;minhash&lt;/a&gt;方法以外, 还有一种常见的hash方法, 叫做simHash. 这里做简要介绍.  &lt;br&gt;
这个hash函数的背景和上次一样, 还是考虑把文本抽象为ngram的集合:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/simhash/pasted_image.png"&gt; &lt;br&gt;
然后相似度依旧是Jaccard similarity:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/simhash/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;simHash&lt;/h2&gt;
&lt;p&gt;simHash的方法听上去比minHash还要简单:    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对一个文档&lt;em&gt;d&lt;/em&gt;中的每一个term(ngram, shingle) &lt;em&gt;t&lt;/em&gt;, 计算其hashcode(比如用java内建的&lt;code&gt;Object.hashCode()&lt;/code&gt;函数) &lt;em&gt;hash(t)&lt;/em&gt;.    &lt;/li&gt;
&lt;li&gt;把d中所有term的&lt;em&gt;hash(t)&lt;/em&gt;合成为一个hashcode作为d的hashcode &lt;em&gt;simHash(d)&lt;/em&gt;: &lt;em&gt;simHash(d)&lt;/em&gt;的长度与&lt;em&gt;hash(t)&lt;/em&gt;相同, &lt;em&gt;simHash(d)&lt;/em&gt;的第k个bit的取值为所有&lt;em&gt;hash(t)&lt;/em&gt;第k个bit的&lt;strong&gt;众数&lt;/strong&gt;.    &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;写成数学表达式很吓人, 其实只不过不断在{0,1}和{-1,+1}之间变而已, 总之就是对所有hash(t)的每一位进行统计, 如果1多就放1, 否则就放0...  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/simhash/pasted_image002.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;关于为什么simHash可以满足近邻hash的条件(即两个文档jacccard sim越大, 其simhash相等的可能性越大), 不知道... 不过可以参考这个链接: &lt;a href="http://matpalm.com/resemblance/simhash/"&gt;http://matpalm.com/resemblance/simhash/&lt;/a&gt; &lt;/p&gt;
&lt;h2&gt;simHash VS minHash&lt;/h2&gt;
&lt;p&gt;下面来比较一下二者的差别.  &lt;br&gt;
首先是表示方式:    &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simHash只需要直接拿term的集合即可使用   &lt;/li&gt;
&lt;li&gt;minHash需要首先建立字典, 然后用一个binary的向量(长度为字典长度)表示一个文档   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其次是取值范围:    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simHash得到的hash范围取决于应用到每个term上的hash函数的范围, simHash与所有term的hash位数相同.   &lt;/li&gt;
&lt;li&gt;minHash的范围等于字典的长度, 如果字典里有M个term那么minHash取值在1到M之间.    &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是minHash也有优点:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要生成不同的simHash比较困难, 取决于应用在每个term上的hash函数有多少种.    &lt;/li&gt;
&lt;li&gt;生成不同的minHash非常容易: 每次shuffle就可以对一篇文章生成不同的minHash.    &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以如果我们想要用多个hash来索引一个文章的时候, minHash可以很容易实现.    &lt;/p&gt;</content><category term="ml"></category></entry><entry><title>minHash: 一种快速approximate retrieval方法</title><link href="http://x-wei.github.io/minhash.html" rel="alternate"></link><published>2015-09-27T11:00:00+02:00</published><updated>2015-09-27T11:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2015-09-27:/minhash.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;approximate retrieval&lt;/strong&gt;(相似搜索)这个问题之前实习的时候就经常遇到: 如何快速在大量数据中如何找出相近的数据.    &lt;/p&gt;
&lt;p&gt;问题描述: 假设有N个数据, 并且对于他们有一个相似度(或距离)的度量函数&lt;code&gt;sim(i,j)&lt;/code&gt;, 我们的问题就是如何快速找出所有N个点中相似度较大的i和j组合.    &lt;/p&gt;
&lt;p&gt;乍一看这个问题必须要对所有的(i,j)计算相似度, 但是N^2的复杂度在N太大的情况下是不能够忍受的.    &lt;/p&gt;
&lt;h2&gt;kdtree&lt;/h2&gt;
&lt;p&gt;之前在algo-note里面遇到过&lt;a href="http://x-wei.github.io/algoI_week5_2.html"&gt;kdtree&lt;/a&gt;, 用它可以使得寻找nearest neighbor的复杂度减少到logN. 但是这种情况对于维度低一点(比如二三维)的情况合适, 维度到了成千上万的时候并不是很好的选择, 所以这里不多讨论.    &lt;/p&gt;
&lt;h2&gt;simhash&lt;/h2&gt;
&lt;p&gt;另一个思路是, 使用某个hash函数, 对于每一个数据计算一个哈希值. 这个hash函数要满足: &lt;strong&gt;当i和j的相似度很高的时候, hash(i)和hash(j)的值(很可能)相同.&lt;/strong&gt; 这次介绍的minHash就是这样的一种方法.    &lt;/p&gt;
&lt;h2&gt;Jaccard similarity&lt;/h2&gt;
&lt;p&gt;明确问题含义, 首先需要定义相似度. 这里主要考虑文本相似度的问题, 假设字典D有M个term …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;approximate retrieval&lt;/strong&gt;(相似搜索)这个问题之前实习的时候就经常遇到: 如何快速在大量数据中如何找出相近的数据.    &lt;/p&gt;
&lt;p&gt;问题描述: 假设有N个数据, 并且对于他们有一个相似度(或距离)的度量函数&lt;code&gt;sim(i,j)&lt;/code&gt;, 我们的问题就是如何快速找出所有N个点中相似度较大的i和j组合.    &lt;/p&gt;
&lt;p&gt;乍一看这个问题必须要对所有的(i,j)计算相似度, 但是N^2的复杂度在N太大的情况下是不能够忍受的.    &lt;/p&gt;
&lt;h2&gt;kdtree&lt;/h2&gt;
&lt;p&gt;之前在algo-note里面遇到过&lt;a href="http://x-wei.github.io/algoI_week5_2.html"&gt;kdtree&lt;/a&gt;, 用它可以使得寻找nearest neighbor的复杂度减少到logN. 但是这种情况对于维度低一点(比如二三维)的情况合适, 维度到了成千上万的时候并不是很好的选择, 所以这里不多讨论.    &lt;/p&gt;
&lt;h2&gt;simhash&lt;/h2&gt;
&lt;p&gt;另一个思路是, 使用某个hash函数, 对于每一个数据计算一个哈希值. 这个hash函数要满足: &lt;strong&gt;当i和j的相似度很高的时候, hash(i)和hash(j)的值(很可能)相同.&lt;/strong&gt; 这次介绍的minHash就是这样的一种方法.    &lt;/p&gt;
&lt;h2&gt;Jaccard similarity&lt;/h2&gt;
&lt;p&gt;明确问题含义, 首先需要定义相似度. 这里主要考虑文本相似度的问题, 假设字典D有M个term(term可以是单词, 也可以是n-gram或叫shingle): &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;一段文本(document i)可以用binary vectorization变为一个binary的向量:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image001.png"&gt; &lt;br&gt;
(这里没有用TF或者TFIDF, 只用一个简单的binary向量化, 因为只有binary的时候才适合我们接下来的推导...)   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;每个document可以看作一些term的&lt;em&gt;集合&lt;/em&gt;, 集合之间的相似度有一个经典的度量: jaccard similarity. &lt;br&gt;
对集合S1和S2, 他们的相似度定义为:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image003.png"&gt; &lt;br&gt;
也很好理解, 重合部分比例越高相似度就越高, 另外jaccard-sim取值在0到1之间.    &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;对于document i和j, 他们的向量形式分别是di和dj. 现在我们希望计算hash(di)和hash(dj), 使得:    &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image004.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2&gt;minHash&lt;/h2&gt;
&lt;p&gt;min hash的思路是这样的, 首先生成一个随机的(1...M)的排序(permutation)π:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;然后, 对于每个document d, 都按照这个permutation, 把d的分量从新排列:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image006.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;然后定义&lt;code&gt;minHash(di)&lt;/code&gt;为permutation以后的di里的第一个不为0的位置:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image008.png"&gt; &lt;br&gt;
(上面个公式里d的下标只是代表第i个文本, 并不代表分量... 我应该写上标的..) &lt;br&gt;
所以&lt;code&gt;minHash()&lt;/code&gt;返回1到M之间的一个数.    &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;proof&lt;/h2&gt;
&lt;p&gt;现在证明一下为什么这样选择minHash函数可以保证两个文本的哈希值相等的概率为他们的jaccard similarity.    &lt;/p&gt;
&lt;p&gt;对于d2和d2, 我们分别查看π(d1)和π(d2)的每个分量, 这两个数有(11), (10), (01), (00)这四种可能, 分别记每种可能性的出现次数为a,b,c,d:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image009.png"&gt; &lt;br&gt;
那么jaccard similarity可以表示为:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image010.png"&gt; &lt;br&gt;
再看&lt;code&gt;minHash()&lt;/code&gt;是如何计算的, 当π(d1)_k和π(d2)_k都为0的时候会继续增加k, 一直到π(d1)_k和π(d2)_k中某一个为1.  &lt;br&gt;
那么&lt;code&gt;minHash(d1)==minHash(d2)&lt;/code&gt;的情况就是二者都为1的情况, 这种情况的可能性为:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/minhash/pasted_image011.png"&gt; &lt;br&gt;
这个概率恰好就是jaccard similarity.    &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="ml"></category></entry><entry><title>lin-reg = max-likelihood: 贝叶斯视角看线性回归</title><link href="http://x-wei.github.io/linreg-bayes.html" rel="alternate"></link><published>2015-09-26T00:00:00+02:00</published><updated>2015-09-26T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2015-09-26:/linreg-bayes.html</id><summary type="html">&lt;p&gt;几乎所有的ml课都是从线性回归讲起, ETH的课也不例外. 不过这次老师用了贝叶斯的视角讲这个问题, 自从高中接触丁老师讲的线性回归以来 第一次听到一个不同于最小二乘的解读, 感觉很有意思. 又想起来刘未鹏那篇非常棒的&lt;a href="http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/"&gt;博客&lt;/a&gt;, 于是想记录一下.    &lt;/p&gt;
&lt;h2&gt;notation&lt;/h2&gt;
&lt;p&gt;首先有n个数据点:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image001.png"&gt; &lt;br&gt;
其中y是实数, 每个x有d个维度, 为了方便表示截距, 再给x加入一个始终等于1的维度:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image002.png"&gt; &lt;br&gt;
例子: y代表房价, x代表了房子的面积, 使用时间, 距离市中心的距离等因素.   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;least square viewpoint&lt;/h2&gt;
&lt;p&gt;在最小二乘的视角里, 线性回归是用一个x的线性函数拟合y:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image003.png"&gt; &lt;br&gt;
使得拟合结果和观测结果的误差尽量小.  &lt;br&gt;
不过这次不说最小二乘, 所以接下来不讨论这个思路...   &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;assumptions in Bayes viewpoint&lt;/h2&gt;
&lt;p&gt;在贝叶斯视角里, 我们假设: &lt;br&gt;
&lt;strong&gt;假设1. y = 某个x的线性函数 + 观测噪音&lt;/strong&gt; &lt;br&gt;
即:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image006.png"&gt; &lt;br&gt;
其中εi是一个&lt;em&gt;随机变量&lt;/em&gt;, 所以y也是一个随机变量.  &lt;br&gt;
另外再有一个比较强的假设: &lt;br&gt;
&lt;strong&gt;假设2.  ε服从centered高斯分布, iid.&lt;/strong&gt; &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image007.png"&gt; &lt;br&gt;
(btw, 对一个随机变量建模, 一般来说, 连续随机变量就用高斯 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;几乎所有的ml课都是从线性回归讲起, ETH的课也不例外. 不过这次老师用了贝叶斯的视角讲这个问题, 自从高中接触丁老师讲的线性回归以来 第一次听到一个不同于最小二乘的解读, 感觉很有意思. 又想起来刘未鹏那篇非常棒的&lt;a href="http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/"&gt;博客&lt;/a&gt;, 于是想记录一下.    &lt;/p&gt;
&lt;h2&gt;notation&lt;/h2&gt;
&lt;p&gt;首先有n个数据点:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image001.png"&gt; &lt;br&gt;
其中y是实数, 每个x有d个维度, 为了方便表示截距, 再给x加入一个始终等于1的维度:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image002.png"&gt; &lt;br&gt;
例子: y代表房价, x代表了房子的面积, 使用时间, 距离市中心的距离等因素.   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;least square viewpoint&lt;/h2&gt;
&lt;p&gt;在最小二乘的视角里, 线性回归是用一个x的线性函数拟合y:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image003.png"&gt; &lt;br&gt;
使得拟合结果和观测结果的误差尽量小.  &lt;br&gt;
不过这次不说最小二乘, 所以接下来不讨论这个思路...   &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;assumptions in Bayes viewpoint&lt;/h2&gt;
&lt;p&gt;在贝叶斯视角里, 我们假设: &lt;br&gt;
&lt;strong&gt;假设1. y = 某个x的线性函数 + 观测噪音&lt;/strong&gt; &lt;br&gt;
即:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image006.png"&gt; &lt;br&gt;
其中εi是一个&lt;em&gt;随机变量&lt;/em&gt;, 所以y也是一个随机变量.  &lt;br&gt;
另外再有一个比较强的假设: &lt;br&gt;
&lt;strong&gt;假设2.  ε服从centered高斯分布, iid.&lt;/strong&gt; &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image007.png"&gt; &lt;br&gt;
(btw, 对一个随机变量建模, 一般来说, 连续随机变量就用高斯, 离散随机变量用泊松)   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;Bayes formula&lt;/h2&gt;
&lt;p&gt;贝叶斯公式长这个样子:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image004.png"&gt; &lt;br&gt;
只看最左边和最右边的内容, 表达为:    &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;posterior = likelihood * prior &lt;br&gt;
后验概率 = 可能性 * 先验概率   &lt;/br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(上面其实应该是"正比于"而不是等号, 由于P(Y)我们并不关心, 所以可以直接忽略之)   &lt;/p&gt;
&lt;p&gt;公式里Y代表可以观察到结果, X代表结果背后不能直接观察的量(&lt;em&gt;不要和数据里的XY混淆...&lt;/em&gt;).  &lt;br&gt;
贝叶斯公式的意义在于, 让我们从可观测的Y反推不可观测的X的概率. 既然我们已经得到了观测结果Y, 那么找到使得后验概率最大的X就说明我们在观测基础上得到了最可信的X的估计.    &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;那么在我们这个问题里, X代表模型, 即某一个β的取值; Y代表观测结果, 即我们看到的n个数据点.  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image005.png"&gt; &lt;br&gt;
所以我们的问题就是: 在已经有了这些观测点的基础之上, 应该选那个β的取值, 使得后验概率最大?   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;lin-reg = max-likelihood&lt;/h2&gt;
&lt;p&gt;线性回归认为, 对于任意的β的取值, 其先验概率都是一样的, 所以在贝叶斯公式里可以忽略ℙ(X), 只需要考虑最大化likelihood ℙ(Y|​X)即可 — 再一次, 不要把贝叶斯的XY和数据的X和Y混淆...  &lt;br&gt;
即选择β: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image015.png"&gt; &lt;br&gt;
由于随机变量y只是随机变量ε的一个函数(且给定β, ε和y一一对应):  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image008.png"&gt; &lt;br&gt;
所以可以最大化ε的likelihood:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image009.png"&gt; &lt;br&gt;
由于之前对ε有假设:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image010.png"&gt; &lt;br&gt;
那么:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image011.png"&gt; &lt;br&gt;
(不要以为ε独立于β: ε=y - βt x, 所以上面表达式里其实还是有β的. ) &lt;br&gt;
⇒ 两边取log并加上负号:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image014.png"&gt; &lt;br&gt;
Voila, 所以极大似然=最小二乘!  &lt;br&gt;
对, 饶了一圈还是最小二乘, 但是这样的意义变了, 明白了为什么要最小化平方误差这个值.   &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;precise solution to linreg&lt;/h2&gt;
&lt;p&gt;(notation有点混乱了, 之前贝叶斯里面应该用AB而不是用XY的orz...) &lt;br&gt;
每个x是一个列向量, 这里, 把所有数据用矩阵形式表示:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image016.png"&gt; &lt;br&gt;
矩阵表示的好处是平方误差可以用矩阵表示:  &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image017.png"&gt; &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image018.png"&gt; &lt;br&gt;
对矩阵运算求偏导, 偏导为0的时候即可得到最优的β: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/linreg-bayes/pasted_image019.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="ml"></category></entry></feed>