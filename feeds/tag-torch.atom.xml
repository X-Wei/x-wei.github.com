<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's blog - torch</title><link href="http://x-wei.github.io/" rel="alternate"></link><link href="http://x-wei.github.io/feeds/tag-torch.atom.xml" rel="self"></link><id>http://x-wei.github.io/</id><updated>2016-10-10T00:00:00+02:00</updated><entry><title>[learning torch] 6. optim (optimization tools)</title><link href="http://x-wei.github.io/learn-torch-6-optim.html" rel="alternate"></link><published>2016-10-10T00:00:00+02:00</published><updated>2016-10-10T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-10:/learn-torch-6-optim.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/26/deep_learning_with_torch_step_7_optim/"&gt;http://rnduja.github.io/2015/10/26/deep_learning_with_torch_step_7_optim/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/optim/blob/master/doc/intro.md"&gt;https://github.com/torch/optim/blob/master/doc/intro.md&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Before we implement the gd update step by defining a &lt;code&gt;gradientUpdate&lt;/code&gt; function and calling it in a loop.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;gradientUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;learningRate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;forward â€¦&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/26/deep_learning_with_torch_step_7_optim/"&gt;http://rnduja.github.io/2015/10/26/deep_learning_with_torch_step_7_optim/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/optim/blob/master/doc/intro.md"&gt;https://github.com/torch/optim/blob/master/doc/intro.md&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Before we implement the gd update step by defining a &lt;code&gt;gradientUpdate&lt;/code&gt; function and calling it in a loop.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;gradientUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;learningRate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="nx"&gt;assumes&lt;/span&gt; &lt;span class="nx"&gt;pred&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;what&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt; &lt;span class="nx"&gt;expects&lt;/span&gt; &lt;span class="kr"&gt;as&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;:&lt;span class="kt"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;zeroGradParameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;grad_cri&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;:&lt;span class="kt"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;grad_cri&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;updateParameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;learningRate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But this is functionality is implemented in the &lt;code&gt;optim&lt;/code&gt; module. In addition to just grad-descent, it has more complicated optimization algorithms implemented.  &lt;/p&gt;
&lt;h2 id="interface"&gt;Interface&lt;/h2&gt;
&lt;p&gt;The interface for all optimization algos are: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;params_new, fs, ... = optim._method_(feval, params[, config][, state])&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;explination:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;params&lt;/code&gt;: current parameters vector (&lt;strong&gt;1D tensor&lt;/strong&gt;), this will be updated during optimization &lt;/li&gt;
&lt;li&gt;&lt;code&gt;feval&lt;/code&gt;: a user-defined closure that respects this API: &lt;code&gt;f, df/dx = feval(x)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;config&lt;/code&gt;: a table of parameters for the algorithm (e.g. learning rate) &lt;/li&gt;
&lt;li&gt;&lt;code&gt;state&lt;/code&gt;: a table of state variables &lt;/li&gt;
&lt;li&gt;&lt;code&gt;params_new&lt;/code&gt;: the resulting new parameter (in a 1D tensor), which minimizes the function f &lt;/li&gt;
&lt;li&gt;&lt;code&gt;fs&lt;/code&gt;: a table of f values evaluated during the optimization, &lt;code&gt;fs[#fs]&lt;/code&gt; is the optimized function value &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt;&lt;br&gt;
As optim expects the input to be 1D tensors, we need to &lt;strong&gt;flatten&lt;/strong&gt; the parameters in our model, this can be achieved via:  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;params, gradParams = model:getParameters()&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;the reuslting &lt;code&gt;params&lt;/code&gt; and &lt;code&gt;gradParams&lt;/code&gt; are all flattened into 1D tensor.  &lt;/p&gt;
&lt;h2 id="example-sgd-to-train-mlp-the-xor-function"&gt;Example: sgd to train mlp the XOR function&lt;/h2&gt;
&lt;p&gt;Here is an example for learning an XOR using a mlp with one hidden layer.  &lt;/p&gt;
&lt;h3 id="model-criterion"&gt;model, criterion&lt;/h3&gt;
&lt;p&gt;First, define the model and criterion (use MSE here, see it as a regression problem):  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;require 'nn' &lt;/span&gt;
&lt;span class="code-line"&gt;inputs = 2; outputs = 1; HUs = 20 -- parameters&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;model = nn.Sequential()  -- make a multi-layer perceptron &lt;/span&gt;
&lt;span class="code-line"&gt;model:add(nn.Linear(inputs, HUs)) &lt;/span&gt;
&lt;span class="code-line"&gt;model:add(nn.Tanh()) &lt;/span&gt;
&lt;span class="code-line"&gt;model:add(nn.Linear(HUs, outputs))&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;criterion = nn.MSECriterion()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="data"&gt;data&lt;/h3&gt;
&lt;p&gt;Then generate dataset of XORs: sample 2d inputs, and lables are -1 if the samples are of the sign, otherwise +1. Generate 128 training samples: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;batchSize = 128 &lt;/span&gt;
&lt;span class="code-line"&gt;batchInputs = torch.DoubleTensor(batchSize, inputs)  &lt;/span&gt;
&lt;span class="code-line"&gt;batchLabels = torch.DoubleTensor(batchSize)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;for i = 1, batchSize do &lt;/span&gt;
&lt;span class="code-line"&gt;   local input = torch.randn(2)   &lt;/span&gt;
&lt;span class="code-line"&gt;   local label &lt;/span&gt;
&lt;span class="code-line"&gt;   if input[1] * input[2] &amp;gt; 0 then  -- calculate label for XOR function &lt;/span&gt;
&lt;span class="code-line"&gt;      label = -1 &lt;/span&gt;
&lt;span class="code-line"&gt;   else &lt;/span&gt;
&lt;span class="code-line"&gt;      label = 1 &lt;/span&gt;
&lt;span class="code-line"&gt;   end &lt;/span&gt;
&lt;span class="code-line"&gt;   batchInputs[i]:copy(input) &lt;/span&gt;
&lt;span class="code-line"&gt;   batchLabels[i] = label &lt;/span&gt;
&lt;span class="code-line"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="feval-closure"&gt;feval() closure&lt;/h3&gt;
&lt;p&gt;Then define the feval function that returns the loss and the gradient wrt the loss:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;feval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;gradParams&lt;/span&gt;:&lt;span class="kt"&gt;zero&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchInputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;:&lt;span class="kt"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;batchLabels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;dloss_doutputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;criterion&lt;/span&gt;:&lt;span class="kt"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;batchLabels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="nx"&gt;model&lt;/span&gt;:&lt;span class="kt"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchInputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;dloss_doutputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gradParams&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;finally, apply &lt;code&gt;optim.sgd&lt;/code&gt; to the batch for 500 epochs:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;require 'optim' &lt;/span&gt;
&lt;span class="code-line"&gt;local sgdcfg = {learningRate=0.01}&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;for epoch=1,500 do &lt;/span&gt;
&lt;span class="code-line"&gt;    optim.sgd(feval, params, sgdcfg) &lt;/span&gt;
&lt;span class="code-line"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;can take some examples to test:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;x = torch.Tensor(2) &lt;/span&gt;
&lt;span class="code-line"&gt;x[1] =  0.5; x[2] =  0.5; print(model:forward(x)[1]) &lt;/span&gt;
&lt;span class="code-line"&gt;x[1] =  0.5; x[2] = -0.5; print(model:forward(x)[1]) &lt;/span&gt;
&lt;span class="code-line"&gt;x[1] = -0.5; x[2] =  0.5; print(model:forward(x)[1]) &lt;/span&gt;
&lt;span class="code-line"&gt;x[1] = -0.5; x[2] = -0.5; print(model:forward(x)[1])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;-0.0073583598776157  &lt;/span&gt;
&lt;span class="code-line"&gt;0.24137506111789     &lt;/span&gt;
&lt;span class="code-line"&gt;0.31254747107449     &lt;/span&gt;
&lt;span class="code-line"&gt;-0.14114052583337&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And the signs are correct for XOR function.  &lt;/p&gt;</content><category term="torch"></category></entry><entry><title>[learning torch] 5. nngraph (another way to construct nn)</title><link href="http://x-wei.github.io/learn-torch-5-nngraph.html" rel="alternate"></link><published>2016-10-08T15:00:00+02:00</published><updated>2016-10-08T15:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-08:/learn-torch-5-nngraph.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/07/deep_learning_with_torch_step_4_nngraph/"&gt;http://rnduja.github.io/2015/10/07/deep_learning_with_torch_step_4_nngraph/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/nngraph/"&gt;https://github.com/torch/nngraph/&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;The aim of this library is to provide users of nn package with tools to easily create complicated architectures.  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;luarocks install nngraph&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;optionally can install &lt;code&gt;graphvis&lt;/code&gt; for graph visualization.  &lt;/p&gt;
&lt;p&gt;From previous posts, to build â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/07/deep_learning_with_torch_step_4_nngraph/"&gt;http://rnduja.github.io/2015/10/07/deep_learning_with_torch_step_4_nngraph/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/nngraph/"&gt;https://github.com/torch/nngraph/&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;The aim of this library is to provide users of nn package with tools to easily create complicated architectures.  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;luarocks install nngraph&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;optionally can install &lt;code&gt;graphvis&lt;/code&gt; for graph visualization.  &lt;/p&gt;
&lt;p&gt;From previous posts, to build networks, there are 2 important classes:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Module&lt;/code&gt; as a nn layer, can do forward and backward prop &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Container&lt;/code&gt; to combine several &lt;code&gt;Module&lt;/code&gt;s &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The nngraph library provides a way to build any complex network focusing on the network graph and avoiding the use of Containers.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="nngraphnode"&gt;nngraph.Node&lt;/h2&gt;
&lt;p&gt;Any &lt;code&gt;Module&lt;/code&gt; can be wrapped into a &lt;code&gt;nngraph.Node&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;For all &lt;code&gt;nn.Module&lt;/code&gt;, nngraph overrides the &lt;code&gt;__call__&lt;/code&gt; meta method (the function call operator &lt;code&gt;()&lt;/code&gt;), by &lt;em&gt;calling&lt;/em&gt; a module, a &lt;code&gt;Node&lt;/code&gt; is returned:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt; require 'nngraph'; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; module = nn.Identity() &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; module &lt;/span&gt;
&lt;span class="code-line"&gt;nn.Identity &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; module() &lt;/span&gt;
&lt;span class="code-line"&gt;nngraph.Node &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The arguments of  &lt;code&gt;__call__&lt;/code&gt; are other (parent) nodes that come into this node, in this way we specify which module(s) will feed into the current node/module: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt; h1 = nn.Linear(20, 10)() &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; h2 = nn.Linear(10, 1)(h1) -- h1 is input of h2  &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="simple-sequential-example"&gt;Simple sequential example&lt;/h2&gt;
&lt;p&gt;We make a network by calling &lt;code&gt;nn.gModulet()&lt;/code&gt;, this takes 2 arguments: a table of &lt;code&gt;inputs&lt;/code&gt;, and a table of &lt;code&gt;outputs&lt;/code&gt;:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt; mlp = nn.gModule({h1}, {h2}) &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0002s]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we can plot the model using graphviz:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;-- draw graph (the forward graph, '.fg'), use it with itorch notebook &lt;/span&gt;
&lt;span class="code-line"&gt;graph.dot(model.fg, 'MLP') &lt;/span&gt;
&lt;span class="code-line"&gt;-- or save graph to file MLP.svg and MLP.dot &lt;/span&gt;
&lt;span class="code-line"&gt;graph.dot(model.fg, 'MLP', 'MLP')&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/learn-torch-5-nngraph/pasted_image.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;(The first and last nodes are dummy nodes and regroup all inputs and outputs of the graph.) &lt;/p&gt;
&lt;h2 id="dag-example"&gt;DAG example&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nngraph#a-network-with-2-inputs-and-2-outputs"&gt;https://github.com/torch/nngraph#a-network-with-2-inputs-and-2-outputs&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Here is an example to build a model with 2 inputs and 2 outputs:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;h1 = nn.Linear(20, 20)() &lt;/span&gt;
&lt;span class="code-line"&gt;h2 = nn.Linear(10, 10)() &lt;/span&gt;
&lt;span class="code-line"&gt;hh1 = nn.Linear(20, 1)(nn.Tanh()(h1)) &lt;/span&gt;
&lt;span class="code-line"&gt;hh2 = nn.Linear(10, 1)(nn.Tanh()(h2)) &lt;/span&gt;
&lt;span class="code-line"&gt;madd = nn.CAddTable()({hh1, hh2}) &lt;/span&gt;
&lt;span class="code-line"&gt;oA = nn.Sigmoid()(madd) &lt;/span&gt;
&lt;span class="code-line"&gt;oB = nn.Tanh()(madd) &lt;/span&gt;
&lt;span class="code-line"&gt;gmod = nn.gModule({h1, h2}, {oA, oB})&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;x1 = torch.rand(20) &lt;/span&gt;
&lt;span class="code-line"&gt;x2 = torch.rand(10)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;gmod:updateOutput({x1, x2}) &lt;/span&gt;
&lt;span class="code-line"&gt;gmod:updateGradInput({x1, x2}, {torch.rand(1), torch.rand(1)}) &lt;/span&gt;
&lt;span class="code-line"&gt;graph.dot(gmod.fg, 'Big MLP')&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;alternatively, &lt;strong&gt;use &lt;/strong&gt;&lt;code&gt;-&lt;/code&gt;&lt;strong&gt; to make your code looks like the data flow&lt;/strong&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;h1 = - nn.Linear(20,20) &lt;/span&gt;
&lt;span class="code-line"&gt;h2 = - nn.Linear(10,10) &lt;/span&gt;
&lt;span class="code-line"&gt;hh1 = h1 - nn.Tanh() - nn.Linear(20,1) &lt;/span&gt;
&lt;span class="code-line"&gt;hh2 = h2 - nn.Tanh() - nn.Linear(10,1) &lt;/span&gt;
&lt;span class="code-line"&gt;madd = {hh1,hh2} - nn.CAddTable() &lt;/span&gt;
&lt;span class="code-line"&gt;oA = madd - nn.Sigmoid() &lt;/span&gt;
&lt;span class="code-line"&gt;oB = madd - nn.Tanh() &lt;/span&gt;
&lt;span class="code-line"&gt;gmod = nn.gModule( {h1,h2}, {oA,oB} )&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="annotations-of-nodes"&gt;Annotations of nodes&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nngraph#annotations"&gt;https://github.com/torch/nngraph#annotations&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;node::annotate()&lt;/code&gt; to annotated nodes, can add name/description, or change node color. &lt;/p&gt;
&lt;p&gt;WIth annotation, can enable debugging too:  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nngraph#debugging"&gt;https://github.com/torch/nngraph#debugging&lt;/a&gt; &lt;/p&gt;</content><category term="torch"></category></entry><entry><title>[learning torch] 4. Criterion (loss function)</title><link href="http://x-wei.github.io/learn-torch-4-criterion.html" rel="alternate"></link><published>2016-10-08T14:00:00+02:00</published><updated>2016-10-08T14:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-08:/learn-torch-4-criterion.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/05/deep_learning_with_torch_step_3_nn_criterions/"&gt;http://rnduja.github.io/2015/10/05/deep_learning_with_torch_step_3_nn_criterions/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Criterion&lt;/code&gt;: abstract class, given input and target(true label), a &lt;code&gt;Criterion&lt;/code&gt; can compute the gradient according to a certain loss function.   &lt;/p&gt;
&lt;h2 id="criterion-class"&gt;Criterion class&lt;/h2&gt;
&lt;p&gt;important methods:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;forward(input, target)&lt;/code&gt;: compute â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;ref: &lt;a href="http://rnduja.github.io/2015/10/05/deep_learning_with_torch_step_3_nn_criterions/"&gt;http://rnduja.github.io/2015/10/05/deep_learning_with_torch_step_3_nn_criterions/&lt;/a&gt; &lt;br&gt;
doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Criterion&lt;/code&gt;: abstract class, given input and target(true label), a &lt;code&gt;Criterion&lt;/code&gt; can compute the gradient according to a certain loss function.   &lt;/p&gt;
&lt;h2 id="criterion-class"&gt;Criterion class&lt;/h2&gt;
&lt;p&gt;important methods:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;forward(input, target)&lt;/code&gt;: compute the loss function, the &lt;code&gt;input&lt;/code&gt; is usually the prediction/log-probability prediction of the network, &lt;code&gt;target&lt;/code&gt; is the truth label of training data.   &lt;/li&gt;
&lt;li&gt;&lt;code&gt;backward(input, target)&lt;/code&gt;: compute gradient of the loss function.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;subclasses of &lt;code&gt;Criterion&lt;/code&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classification critierions: cross-entropy, neg loglikelihood, ...  &lt;/li&gt;
&lt;li&gt;regression criterions: MSE, Abs, KL divergence, ...  &lt;/li&gt;
&lt;li&gt;embedding criterions  &lt;/li&gt;
&lt;li&gt;misc criterions  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="classification-criterion-examples"&gt;Classification criterion examples&lt;/h2&gt;
&lt;h3 id="classnllcriterion"&gt;ClassNLLCriterion&lt;/h3&gt;
&lt;p&gt;negative log likelihood criterion  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md#nn.ClassNLLCriterion"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md#nn.ClassNLLCriterion&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;crt = nn.ClassNLLCriterion([weights])&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;optional argument &lt;code&gt;weights&lt;/code&gt; is to assign class weights (1D tensor), which is useful for unbalanced dataset.   &lt;/p&gt;
&lt;p&gt;For NLL criterion, the &lt;code&gt;input&lt;/code&gt; given through a &lt;code&gt;forward(input, target)&lt;/code&gt; is expected to be the &lt;em&gt;log-probabilities&lt;/em&gt; of each class. The &lt;code&gt;target&lt;/code&gt; is expected to be a class index (1 to n).   &lt;/p&gt;
&lt;p&gt;The &lt;em&gt;probabilities&lt;/em&gt; of each class can be computed by applying softmax on &lt;em&gt;logits&lt;/em&gt;,  the log-proba is just to take the log of the probabilities. Can use directly &lt;a href="https://github.com/torch/nn/blob/master/doc/transfer.md#logsoftmax"&gt;logsoftmax&lt;/a&gt; layer to achieve this (ex. add &lt;code&gt;nn.LogSoftMax&lt;/code&gt; as last layer of a sequential container).   &lt;/p&gt;
&lt;p&gt;If the input &lt;code&gt;x&lt;/code&gt; is log-proba of each class, the loss is just:   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;loss = forward(x, target) = -x[target_class]&lt;/code&gt; &lt;/p&gt;
&lt;h3 id="crossentropycriterion"&gt;CrossEntropyCriterion&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md#nn.CrossEntropyCriterion"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md#nn.CrossEntropyCriterion&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;This combines a logsoftmax and a NLLcriterion, so the &lt;code&gt;input&lt;/code&gt; is expected to be &lt;em&gt;logits&lt;/em&gt; (scores)  &lt;/p&gt;
&lt;h3 id="margincriterion"&gt;MarginCriterion&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md#margincriterion"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md#margincriterion&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;computes hinge loss of binary classification problem.   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;input&lt;/code&gt; x is expected to be svm scores, &lt;code&gt;target&lt;/code&gt; y is expected to be Â±1 labels.   &lt;/p&gt;
&lt;h2 id="regression-criterion-examples_1"&gt;Regression criterion examples&lt;/h2&gt;
&lt;h3 id="msecriterion"&gt;MSECriterion&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/criterion.md#nn.MSECriterion"&gt;https://github.com/torch/nn/blob/master/doc/criterion.md#nn.MSECriterion&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;criterion = nn.MSECriterion()&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;the loss is just MSE, input and target both have n elements:   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;loss = forward(x,y) = sum[ (xi-yi)^2 ] / n&lt;/code&gt; &lt;/p&gt;
&lt;h3 id="abscriterion"&gt;AbsCriterion&lt;/h3&gt;
&lt;p&gt;L1 distance between x and y.   &lt;/p&gt;
&lt;h3 id="distkldivcriterion"&gt;DistKLDivCriterion&lt;/h3&gt;
&lt;p&gt;KL divergence for class probabilities   &lt;/p&gt;
&lt;h2 id="a-complete-example_1"&gt;A Complete Example&lt;/h2&gt;
&lt;h3 id="updating-function"&gt;updating function&lt;/h3&gt;
&lt;p&gt;First write a function for  grad-desc updating for a &lt;code&gt;model&lt;/code&gt;, input &lt;em&gt;to the model&lt;/em&gt; is &lt;code&gt;x&lt;/code&gt;, truth label is &lt;code&gt;y&lt;/code&gt;.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="kr"&gt;function&lt;/span&gt; &lt;span class="nf"&gt;gradientUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learningRate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="kd"&gt;local&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;-- assumes pred is what criterion expects as input  &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="kd"&gt;local&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;zeroGradParameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="kd"&gt;local&lt;/span&gt; &lt;span class="n"&gt;grad_cri&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grad_cri&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;updateParameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learningRate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="kr"&gt;end&lt;/span&gt;  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function implements an update step, given a training sample (&lt;code&gt;x&lt;/code&gt;,&lt;code&gt;y&lt;/code&gt;):  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the model computes its output by &lt;code&gt;model:forward(x)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;criterion takes model's output, and computes loss by&lt;code&gt;criterion:forward(pred, y)&lt;/code&gt;, &lt;em&gt;note&lt;/em&gt;: the output of model shall be what criterion expects, e.g. pred=log-class-proba for NLL criterion.   &lt;/li&gt;
&lt;li&gt;criterion gives the gradient of loss function wrt the model output by &lt;code&gt;cri:backward(pred, y)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;model computes the gradient of its parameters using the gradient from criterion by &lt;code&gt;model:backward(x, grad_cri)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;the model do a gradient descent step to modify its parameters by &lt;code&gt;model:updateParameters(learningRate)&lt;/code&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;This is the function that we should pass to an optimizer.&lt;/em&gt; &lt;/p&gt;
&lt;h3 id="model-criterion-and-data"&gt;model, criterion and data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the model is just a linear layer (5 inputs, 1 output ), output = Ax+b  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;lua
        model = nn.Sequential()  
        model:add(nn.Linear(5,1))&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the criterion is just hinge loss:   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;criterion = nn.MarginCriterion(1)&lt;/code&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the data, just use 2 datapoints:   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;lua
    x1 = torch.rand(5)  
    y1 = torch.Tensor({1})  
    x2 = torch.rand(5)  
    y2 = torch.Tensor({-1})&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="training"&gt;training&lt;/h3&gt;
&lt;p&gt;To train the model, we run the update funcion on the data points 1000 times (&lt;em&gt;epochs&lt;/em&gt;):   &lt;/p&gt;
&lt;p&gt;&lt;code&gt;lua
    for i = 1,1000 do  
        gradientUpdate(model, x1, y1, criterion, 0.01)  
        gradientUpdate(model, x1, y1, criterion, 0.01)  
    end&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="evaluating"&gt;evaluating&lt;/h3&gt;
&lt;p&gt;to see the prediciton, just use &lt;code&gt;model:forward(x)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;lua
    print('prediction for x1='..model:forward(x1)[1]..' expected value='..y1[1])  
    print('prediction for x2='..model:forward(x2)[1]..' expected value='..y2[1])&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;to see loss, use &lt;code&gt;criterion:forward(model_out, y)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;lua
    print('loss after training for x1 = ' .. criterion:forward(model:forward(x1), y1))  
    print('loss after training for x2 = ' .. criterion:forward(model:forward(x2), y2))&lt;/code&gt;&lt;/p&gt;</content><category term="torch"></category></entry><entry><title>[learning torch] 3. Container (models)</title><link href="http://x-wei.github.io/learn-torch-3-container.html" rel="alternate"></link><published>2016-10-07T20:20:00+02:00</published><updated>2016-10-07T20:20:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-07:/learn-torch-3-container.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/containers.md"&gt;https://github.com/torch/nn/blob/master/doc/containers.md&lt;/a&gt; &lt;br&gt;
ref: &lt;a href="http://rnduja.github.io/2015/10/04/deep_learning_with_torch_step_2_nn_containers/"&gt;http://rnduja.github.io/2015/10/04/deep_learning_with_torch_step_2_nn_containers/&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Container, similarly to Module, is the abstract class defining the base methods inherited from concrete containers. &lt;em&gt;Container contains modules (layers)&lt;/em&gt;.  &lt;/p&gt;
&lt;h2 id="container-class"&gt;Container class&lt;/h2&gt;
&lt;p&gt;important methods:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;add(module)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;get(index)&lt;/code&gt;: get â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/containers.md"&gt;https://github.com/torch/nn/blob/master/doc/containers.md&lt;/a&gt; &lt;br&gt;
ref: &lt;a href="http://rnduja.github.io/2015/10/04/deep_learning_with_torch_step_2_nn_containers/"&gt;http://rnduja.github.io/2015/10/04/deep_learning_with_torch_step_2_nn_containers/&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Container, similarly to Module, is the abstract class defining the base methods inherited from concrete containers. &lt;em&gt;Container contains modules (layers)&lt;/em&gt;.  &lt;/p&gt;
&lt;h2 id="container-class"&gt;Container class&lt;/h2&gt;
&lt;p&gt;important methods:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;add(module)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;get(index)&lt;/code&gt;: get module at the index &lt;/li&gt;
&lt;li&gt;&lt;code&gt;size()&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;important subclasses: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Sequential&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Parallel&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Concat&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sequential"&gt;Sequential&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Sequential&lt;/code&gt;&lt;strong&gt; is just a stack of layers&lt;/strong&gt;, add layer by &lt;code&gt;model:add()&lt;/code&gt;. Here is a simple 2-layer MLP example:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt;  mlp = nn.Sequential() &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:add( nn.Linear(25, 1) ) -- 1 output &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:forward(torch.range(1,10)) &lt;/span&gt;
&lt;span class="code-line"&gt; 1.2697 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 1]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="parallel"&gt;Parallel&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;module = Parallel(inputDimension,outputDimension)&lt;/code&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Creates a container module that applies its ith child module to the ith slice of the input Tensor&lt;/em&gt; by using &lt;code&gt;select&lt;/code&gt; on dimension &lt;code&gt;inputDimension&lt;/code&gt;. It concatenates the results of its contained modules together along dimension &lt;code&gt;outputDimension&lt;/code&gt;.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So if the input for parallel model is &lt;code&gt;x&lt;/code&gt;,  the input for its ith child module should be: &lt;code&gt;x.select(inputDimension, i)&lt;/code&gt;,&lt;br&gt;
and the parallel model should be: &lt;code&gt;torch.cat( out1, out2, ouputDimension)&lt;/code&gt; (concat along this dimension).  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt; mlp = nn.Parallel(2,1) -- select(split) on dim2 for input, concat along dim1 for output &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:add(nn.Linear(10,3)) -- input=1st slice of x (x:select()), output1: size=3 &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:add(nn.Linear(10,2)) -- output2: size=2 &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; x = torch.randn(10,2)  &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; x &lt;/span&gt;
&lt;span class="code-line"&gt; 0.3242 -1.3911 &lt;/span&gt;
&lt;span class="code-line"&gt; 0.7433 -0.2725 &lt;/span&gt;
&lt;span class="code-line"&gt; 0.3947  0.3332 &lt;/span&gt;
&lt;span class="code-line"&gt; 1.1618  0.6743 &lt;/span&gt;
&lt;span class="code-line"&gt; 0.6655 -1.0901 &lt;/span&gt;
&lt;span class="code-line"&gt; 0.0419 -0.7845 &lt;/span&gt;
&lt;span class="code-line"&gt;-0.8508 -1.4670 &lt;/span&gt;
&lt;span class="code-line"&gt;-0.3842 -0.4107 &lt;/span&gt;
&lt;span class="code-line"&gt; 0.5238  2.3616 &lt;/span&gt;
&lt;span class="code-line"&gt; 1.4136 -0.1327 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 10x2]&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0002s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; mlp:forward(x) &lt;/span&gt;
&lt;span class="code-line"&gt;-0.0456 &lt;/span&gt;
&lt;span class="code-line"&gt;-0.5682 &lt;/span&gt;
&lt;span class="code-line"&gt;-0.3488 &lt;/span&gt;
&lt;span class="code-line"&gt;-1.3786 &lt;/span&gt;
&lt;span class="code-line"&gt;-0.6320 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 5]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="concat"&gt;Concat&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;module = nn.Concat(dim)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;Concat concatenates the output of its "parallel" children modules along &lt;code&gt;dim&lt;/code&gt;: these child modules &lt;em&gt;take the same inputs&lt;/em&gt;, and their output is concatenated. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;mlp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;Concat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--&lt;/span&gt; &lt;span class="nt"&gt;ouput&lt;/span&gt; &lt;span class="nt"&gt;concat&lt;/span&gt; &lt;span class="nt"&gt;along&lt;/span&gt; &lt;span class="nt"&gt;dim&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;mlp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;add&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt; &lt;span class="nt"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;Linear&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;10&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;);&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;mlp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;add&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt; &lt;span class="nt"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;Linear&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;10&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;3&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;);&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;randn&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;mlp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;7497&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;-0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;1909&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3280&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;-0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3981&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0207&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;torch.DoubleTensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="torch"></category></entry><entry><title>[learning torch] 2. Module (layers)</title><link href="http://x-wei.github.io/learn-torch-2-module.html" rel="alternate"></link><published>2016-10-07T19:40:00+02:00</published><updated>2016-10-07T19:40:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-07:/learn-torch-2-module.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Module&lt;/code&gt; is an abstract class which defines fundamental methods necessary for a &lt;em&gt;Layer&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/module.md"&gt;https://github.com/torch/nn/blob/master/doc/module.md&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="module-class"&gt;Module class&lt;/h2&gt;
&lt;p&gt;variables in &lt;code&gt;Module&lt;/code&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Tensor, the ouput computed from last call of &lt;code&gt;forward(input)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;gradInput&lt;/code&gt;: Tensor, gradient wrt input of module, computed from â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Module&lt;/code&gt; is an abstract class which defines fundamental methods necessary for a &lt;em&gt;Layer&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;doc: &lt;a href="https://github.com/torch/nn/blob/master/doc/module.md"&gt;https://github.com/torch/nn/blob/master/doc/module.md&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="module-class"&gt;Module class&lt;/h2&gt;
&lt;p&gt;variables in &lt;code&gt;Module&lt;/code&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Tensor, the ouput computed from last call of &lt;code&gt;forward(input)&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;gradInput&lt;/code&gt;: Tensor, gradient wrt input of module, computed from last call of &lt;code&gt;updateGradInput(input, gradOutput)&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;important methods in &lt;code&gt;Module&lt;/code&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;forward(input)&lt;/code&gt;: return corresponding output of layer &lt;/li&gt;
&lt;li&gt;&lt;code&gt;backward(input, gradOutput)&lt;/code&gt;: return gradInput wrt the given input &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="linear"&gt;Linear&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear"&gt;https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Linear&lt;/code&gt; extends &lt;code&gt;Module&lt;/code&gt;, it's just linear transformation of input: &lt;code&gt;y=Ax+b&lt;/code&gt; (parameters/variables: &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;bias&lt;/code&gt;) &lt;/p&gt;
&lt;p&gt;&lt;code&gt;gradWeight&lt;/code&gt;, &lt;code&gt;gradBias&lt;/code&gt; are respectively the gradient of each parameter.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;Linear&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;3&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--&lt;/span&gt; &lt;span class="nt"&gt;3&lt;/span&gt; &lt;span class="nt"&gt;input&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt; &lt;span class="nt"&gt;output&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;fill&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;zero&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;Tensor&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;1,2,3&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;gradinput&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;backward&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;y&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;gradinput&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;torch.DoubleTensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;gradInput&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;torch.DoubleTensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;gradWeight&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;1132e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;171&lt;/span&gt;  &lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;2000e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;01&lt;/span&gt; &lt;span class="nt"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3587e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;223&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;7112e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;243&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3276e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;251&lt;/span&gt; &lt;span class="nt"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0404e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;180&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;torch.DoubleTensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="nx"&gt;x3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="nt"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;ln&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;gradBias&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;6&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt; &lt;span class="nt"&gt;6&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;torch.DoubleTensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="identity"&gt;Identity&lt;/h2&gt;
&lt;p&gt;output reproduces input, this layer can be used to model the input layer of a neural network.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;th&amp;gt; id = nn.Identity() &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; y = id:forward(x) &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0000s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; y &lt;/span&gt;
&lt;span class="code-line"&gt; 1 &lt;/span&gt;
&lt;span class="code-line"&gt; 2 &lt;/span&gt;
&lt;span class="code-line"&gt; 3 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 3]&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; id:backward(x,y) &lt;/span&gt;
&lt;span class="code-line"&gt; 1 &lt;/span&gt;
&lt;span class="code-line"&gt; 2 &lt;/span&gt;
&lt;span class="code-line"&gt; 3 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 3]&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                      [0.0001s]  &lt;/span&gt;
&lt;span class="code-line"&gt;th&amp;gt; id.gradInput &lt;/span&gt;
&lt;span class="code-line"&gt; 1 &lt;/span&gt;
&lt;span class="code-line"&gt; 2 &lt;/span&gt;
&lt;span class="code-line"&gt; 3 &lt;/span&gt;
&lt;span class="code-line"&gt;[torch.DoubleTensor of size 3]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="other-modules"&gt;Other modules&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/nn/blob/master/doc/simple.md"&gt;https://github.com/torch/nn/blob/master/doc/simple.md&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;some examples: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Add&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Mul&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;CMul&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reshape&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;</content><category term="torch"></category></entry><entry><title>[learning torch] 1. Tensor</title><link href="http://x-wei.github.io/learn-torch-1-tensor.html" rel="alternate"></link><published>2016-10-07T15:30:00+02:00</published><updated>2016-10-07T15:30:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-10-07:/learn-torch-1-tensor.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;Tensor&lt;/code&gt; is the fondamental data type in torch, (similar to numpy for tensorflow), it's a potentially multi-dimensional matrix. &lt;/p&gt;
&lt;p&gt;See doc: &lt;a href="https://github.com/torch/torch7/blob/master/doc/tensor.md"&gt;https://github.com/torch/torch7/blob/master/doc/tensor.md&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="basic-ops"&gt;basic ops&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Indicate shape in constructor: &lt;/p&gt;
&lt;p&gt;th&amp;gt; x = torch.Tensor(3,4) 
                                                                      [0.0000s] 
th&amp;gt; x 
 3.7366e â€¦&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;Tensor&lt;/code&gt; is the fondamental data type in torch, (similar to numpy for tensorflow), it's a potentially multi-dimensional matrix. &lt;/p&gt;
&lt;p&gt;See doc: &lt;a href="https://github.com/torch/torch7/blob/master/doc/tensor.md"&gt;https://github.com/torch/torch7/blob/master/doc/tensor.md&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="basic-ops"&gt;basic ops&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Indicate shape in constructor: &lt;/p&gt;
&lt;p&gt;th&amp;gt; x = torch.Tensor(3,4) 
                                                                      [0.0000s] 
th&amp;gt; x 
 3.7366e+193  9.4775e+170  3.3018e+180   4.8950e-85 
 1.3808e+267  7.6859e+261   3.7512e-81  1.4692e+195 
 9.7016e+189  6.9641e+252  9.1815e+170  4.5239e+217 
[torch.DoubleTensor of size 3x4] &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By default the elements of a newly allocated memory are &lt;strong&gt;not initialized&lt;/strong&gt;, might contain arbitrary numbers !  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x:dim()&lt;/code&gt;: return nb of dimensions &lt;/li&gt;
&lt;li&gt;&lt;code&gt;x:nElement()&lt;/code&gt;: return nb of elements ("size") &lt;/li&gt;
&lt;li&gt;&lt;code&gt;x:size()&lt;/code&gt;: return "shape", shortcut is: &lt;code&gt;#x&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;x:resize(sz1, sz2, ...)&lt;/code&gt;: will not throw exception when total size is inconsistent! &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;3&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongStorage&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;3&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongStorage&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt;  &lt;span class="mf"&gt;7.0981e+194&lt;/span&gt;  &lt;span class="mf"&gt;7.4861e-114&lt;/span&gt;  &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt;  &lt;span class="mf"&gt;8.2791e-114&lt;/span&gt;  &lt;span class="mf"&gt;3.6822e+180&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mf"&gt;4.8548e-27&lt;/span&gt;   &lt;span class="mf"&gt;6.9204e-72&lt;/span&gt;  &lt;span class="mf"&gt;8.8289e+199&lt;/span&gt;  &lt;span class="mf"&gt;1.1567e+247&lt;/span&gt;   &lt;span class="mf"&gt;4.8548e-27&lt;/span&gt;  &lt;span class="mf"&gt;7.7700e-109&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;x6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0002&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;Columns&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt;  &lt;span class="mf"&gt;7.0981e+194&lt;/span&gt;  &lt;span class="mf"&gt;7.4861e-114&lt;/span&gt;  &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt;  &lt;span class="mf"&gt;8.2791e-114&lt;/span&gt;  &lt;span class="mf"&gt;3.6822e+180&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mf"&gt;6.9204e-72&lt;/span&gt;  &lt;span class="mf"&gt;8.8289e+199&lt;/span&gt;  &lt;span class="mf"&gt;1.1567e+247&lt;/span&gt;   &lt;span class="mf"&gt;4.8548e-27&lt;/span&gt;  &lt;span class="mf"&gt;7.7700e-109&lt;/span&gt;   &lt;span class="mf"&gt;6.9006e-72&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;Columns&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mf"&gt;4.8548e-27&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;1.0240e-259&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;x7&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt;  &lt;span class="mf"&gt;7.0981e+194&lt;/span&gt;  &lt;span class="mf"&gt;7.4861e-114&lt;/span&gt;  &lt;span class="mf"&gt;1.7479e+270&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;8.2791e-114&lt;/span&gt;  &lt;span class="mf"&gt;3.6822e+180&lt;/span&gt;   &lt;span class="mf"&gt;4.8548e-27&lt;/span&gt;   &lt;span class="mf"&gt;6.9204e-72&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fill with constant value: &lt;/p&gt;
&lt;p&gt;th&amp;gt; x:fill(1) 
 1  1  1  1 
 1  1  1  1 
 1  1  1  1 
[torch.DoubleTensor of size 3x4] &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;other constructors: &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;-- note: y is just a reference of x!!! &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;x3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="storage"&gt;Storage&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;One could say that a &lt;code&gt;Tensor&lt;/code&gt; is a particular way of viewing a &lt;code&gt;Storage&lt;/code&gt;: a &lt;code&gt;Storage&lt;/code&gt; only represents a chunk of memory, while the &lt;code&gt;Tensor&lt;/code&gt; interprets this chunk of memory as having dimensions.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;-- 'flatten' version of x &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kr"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="kr"&gt;do&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="kr"&gt;end&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;short for range: torch.range(1,5) &lt;/p&gt;
&lt;h2 id="slicing"&gt;Slicing&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/torch7/blob/master/doc/tensor.md#extracting-sub-tensors"&gt;https://github.com/torch/torch7/blob/master/doc/tensor.md#extracting-sub-tensors&lt;/a&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slicing using &lt;code&gt;sub()&lt;/code&gt; and &lt;code&gt;select()&lt;/code&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;-- x[2:3, 2:4] slicing &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;x3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;-- select(dim, index), dim=1 for rows, =2 for cols &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;2&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;6&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;10&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Or use slicing/indexing &lt;strong&gt;shortcut&lt;/strong&gt;: &lt;code&gt;[{ {dim1start, dim1end},...}] [dim1, dim2,...]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;13&lt;/span&gt;  &lt;span class="mi"&gt;14&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;  &lt;span class="mi"&gt;16&lt;/span&gt;  &lt;span class="mi"&gt;17&lt;/span&gt;  &lt;span class="mi"&gt;18&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;19&lt;/span&gt;  &lt;span class="mi"&gt;20&lt;/span&gt;  &lt;span class="mi"&gt;21&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt;  &lt;span class="mi"&gt;23&lt;/span&gt;  &lt;span class="mi"&gt;24&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;25&lt;/span&gt;  &lt;span class="mi"&gt;26&lt;/span&gt;  &lt;span class="mi"&gt;27&lt;/span&gt;  &lt;span class="mi"&gt;28&lt;/span&gt;  &lt;span class="mi"&gt;29&lt;/span&gt;  &lt;span class="mi"&gt;30&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="n"&gt;x6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0002&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;}}]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;10&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;11&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[{&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;3&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;15&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;21&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mi"&gt;27&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="all-are-references"&gt;All are references&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;All tensor operations in this class do not make any memory copy. &lt;strong&gt;All these methods transform the existing tensor, or return a new tensor referencing the same storage&lt;/strong&gt;.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;&lt;span class="n"&gt;zero&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;-- x will be effected &lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If don't want to modify original tensor, use &lt;code&gt;clone()&lt;/code&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;clone&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;      &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="matrix-ops"&gt;Matrix ops&lt;/h2&gt;
&lt;p&gt;Some matrix operations &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;random matrix: &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manualSeed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.4170&lt;/span&gt;  &lt;span class="mf"&gt;0.9972&lt;/span&gt;  &lt;span class="mf"&gt;0.7203&lt;/span&gt;  &lt;span class="mf"&gt;0.9326&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.0001&lt;/span&gt;  &lt;span class="mf"&gt;0.1281&lt;/span&gt;  &lt;span class="mf"&gt;0.3023&lt;/span&gt;  &lt;span class="mf"&gt;0.9990&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.1468&lt;/span&gt;  &lt;span class="mf"&gt;0.2361&lt;/span&gt;  &lt;span class="mf"&gt;0.0923&lt;/span&gt;  &lt;span class="mf"&gt;0.3966&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;transpose  &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;At&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                                                                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0000&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;At&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.4170&lt;/span&gt;  &lt;span class="mf"&gt;0.0001&lt;/span&gt;  &lt;span class="mf"&gt;0.1468&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.9972&lt;/span&gt;  &lt;span class="mf"&gt;0.1281&lt;/span&gt;  &lt;span class="mf"&gt;0.2361&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.7203&lt;/span&gt;  &lt;span class="mf"&gt;0.3023&lt;/span&gt;  &lt;span class="mf"&gt;0.0923&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.9326&lt;/span&gt;  &lt;span class="mf"&gt;0.9990&lt;/span&gt;  &lt;span class="mf"&gt;0.3966&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="n"&gt;x3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;matrix mul is just &lt;code&gt;*&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;At&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;2.5568&lt;/span&gt;  &lt;span class="mf"&gt;1.2773&lt;/span&gt;  &lt;span class="mf"&gt;0.7330&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;1.2773&lt;/span&gt;  &lt;span class="mf"&gt;1.1059&lt;/span&gt;  &lt;span class="mf"&gt;0.4544&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;     &lt;span class="mf"&gt;0.7330&lt;/span&gt;  &lt;span class="mf"&gt;0.4544&lt;/span&gt;  &lt;span class="mf"&gt;0.2431&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;x3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;inner product: &lt;code&gt;dot()&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;At&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="mf"&gt;2.5568154905493&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;inverse: &lt;code&gt;torch.inverse(sq_mat)&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="more-operations"&gt;More operations&lt;/h2&gt;
&lt;p&gt;can be found at:  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/torch/torch7/blob/master/doc/maths.md"&gt;https://github.com/torch/torch7/blob/master/doc/maths.md&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;example:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.ones()/eye()/zeros()&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;torch.cat()&lt;/code&gt;: concat tensors    &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;         &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;         &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;         &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;         &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;         &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DoubleTensor&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.conv2()&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;</content><category term="torch"></category></entry></feed>