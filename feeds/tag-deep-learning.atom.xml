<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's blog - deep learning</title><link href="http://x-wei.github.io/" rel="alternate"></link><link href="http://x-wei.github.io/feeds/tag-deep-learning.atom.xml" rel="self"></link><id>http://x-wei.github.io/</id><updated>2017-11-30T00:00:00+01:00</updated><entry><title>[Convolutional Neural Networks] week4. Special applications: Face recognition &amp; Neural style transfer</title><link href="http://x-wei.github.io/Ng_DLMooc_c4wk4.html" rel="alternate"></link><published>2017-11-30T00:00:00+01:00</published><updated>2017-11-30T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-30:/Ng_DLMooc_c4wk4.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;p&gt;This week: two special application of ConvNet.  &lt;/p&gt;
&lt;h2 id="i-face-recognition"&gt;I-Face Recognition&lt;/h2&gt;
&lt;h3 id="what-is-face-recognition"&gt;What is face recognition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Face verification &amp;amp; face recognition&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;verification: input = image and ID → output whether the image and ID are the same.  &lt;/li&gt;
&lt;li&gt;recognition: database = K persons, input = image → output = ID of the image among the K person or "not recognized …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;p&gt;This week: two special application of ConvNet.  &lt;/p&gt;
&lt;h2 id="i-face-recognition"&gt;I-Face Recognition&lt;/h2&gt;
&lt;h3 id="what-is-face-recognition"&gt;What is face recognition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Face verification &amp;amp; face recognition&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;verification: input = image and ID → output whether the image and ID are the same.  &lt;/li&gt;
&lt;li&gt;recognition: database = K persons, input = image → output = ID of the image among the K person or "not recognized".  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ the verification system's precision needs to be very high in order to be used in face recognition.  &lt;/p&gt;
&lt;h3 id="one-shot-learning"&gt;One Shot Learning&lt;/h3&gt;
&lt;p&gt;"one shot": learn from just &lt;em&gt;one&lt;/em&gt; example, and able to recognize the person again.&lt;br&gt;
A CNN+softmax is not practical, e.g. when new images are added to database, output_dim will change...  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;⇒ instead, learn a &lt;em&gt;similarity&lt;/em&gt; function.&lt;br&gt;
&lt;code&gt;d(img1, img2)&lt;/code&gt; = degree of difference between images. + threshold &lt;code&gt;tau&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="siamese-network"&gt;Siamese Network&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image.png"&gt;&lt;br&gt;
To learn a disimilarity function: &lt;em&gt;Siamese Network&lt;/em&gt;.  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Use CNN+FC to &lt;strong&gt;encode&lt;/strong&gt;&lt;em&gt; a pic &lt;/em&gt;&lt;code&gt;x&lt;/code&gt;&lt;em&gt; into vector &lt;/em&gt;&lt;code&gt;f(x)&lt;/code&gt;&lt;em&gt;.&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image001.png"&gt;&lt;br&gt;
⇒ define disimilarity function d(x1, x2) as distance between encoded vectors.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image002.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;More formally:&lt;br&gt;
want to learn NN params for the encoding f(x) such that:&lt;br&gt;
when x1 and x2 are same person, dist(f(x1), f(x2)) is small, otherwise large.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="triplet-loss"&gt;Triplet Loss&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image005.png"&gt;&lt;br&gt;
triplet: (&lt;em&gt;anchor, positive, negative)&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image004.png"&gt;&lt;br&gt;
want f(A) similar to f(P) and different from f(N):&lt;br&gt;
i.e. want &lt;code&gt;d(A, P) - d(A, N) &amp;lt;= 0&lt;/code&gt;&lt;br&gt;
⇒ to avoid NN from learning a trival output (i.e. all encodings are identical, d(A, P)=d(A, N)=0), &lt;em&gt;add margin &lt;/em&gt;&lt;code&gt;alpha&lt;/code&gt;.&lt;br&gt;
want &lt;code&gt;d(A, P) - d(A, N) + alpha&amp;lt;= 0&lt;/code&gt; &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image006.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss function definition&lt;/strong&gt;:&lt;br&gt;
similar to hinge loss: &lt;code&gt;L = max(0, d(A,P)-d(A,N) + alpha)&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image007.png"&gt;&lt;br&gt;
→ Generate triplets from dataset, and feed to the NN.&lt;br&gt;
note: here we do need &amp;gt;1 pics of the same person (Anchor and Positive).  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choosing triplets A, P, N&lt;/strong&gt;&lt;br&gt;
If A,P,N are &lt;em&gt;randomly&lt;/em&gt; chosen, the constraint is easily satisfied → NN won't learn much.&lt;br&gt;
→ &lt;em&gt;choose A,P,N that are "hard" to train on. &lt;/em&gt;Computation efficiency of learning is improved.&lt;br&gt;
(&lt;em&gt;details presented in the FaceNet paper&lt;/em&gt;)  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;in practice: better download pretrained model.  &lt;/p&gt;
&lt;h3 id="face-verification-and-binary-classification"&gt;Face Verification and Binary Classification&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image.png"&gt;&lt;br&gt;
Triplet loss is one way of doing face verification.&lt;br&gt;
another way that works as well: treat as a binary classification problem.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Given input image x1 and x2 → feed f(x1) and f(x2) to a logistic regression unit.&lt;br&gt;
→ feed the &lt;em&gt;difference in encodings&lt;/em&gt; and feed to logistic regression.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image009.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;computation trick: precompute encodings of imgs in database at inference time.  &lt;/p&gt;
&lt;h2 id="ii-neural-style-transfer_1"&gt;II-Neural Style Transfer&lt;/h2&gt;
&lt;h3 id="what-is-neural-style-transfer"&gt;What is neural style transfer?&lt;/h3&gt;
&lt;p&gt;content image &lt;code&gt;C&lt;/code&gt;+ style image &lt;code&gt;S&lt;/code&gt; → generated image &lt;code&gt;G&lt;/code&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image010.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="what-are-deep-convnets-learning"&gt;What are deep ConvNets learning?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image011.png"&gt;&lt;br&gt;
&lt;strong&gt;visualize hidden units of different layers&lt;/strong&gt;&lt;br&gt;
Pick &lt;em&gt;one unit&lt;/em&gt; in 1st layer → find the &lt;em&gt;nine image patches that maximize the unit's activation&lt;/em&gt;.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;In deeper layers: units can see larger image patches → gather nine argmax image patchs as before.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image012.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;deeper layers can detect higher level shapes:&lt;br&gt;
layer 3:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image013.png"&gt;&lt;br&gt;
layer 4:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image014.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="cost-function"&gt;Cost Function&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image015.png"&gt;&lt;br&gt;
Define a cost function &lt;em&gt;for the generated image&lt;/em&gt;.&lt;br&gt;
&lt;code&gt;J(G)&lt;/code&gt; measure how good is an image, contains two parts:  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;J_content(C, G)&lt;/code&gt;: how similar is G to C  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;J_style(S, G)&lt;/code&gt;: how similar is G to S.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image016.png"&gt;&lt;br&gt;
&lt;strong&gt;Find generated image G&lt;/strong&gt;&lt;br&gt;
(similar to embedding?)  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize G randomly  &lt;/li&gt;
&lt;li&gt;Use gradient-descent to minimized J(G)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image017.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="content-cost-function"&gt;Content Cost Function&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;J_content(C, G)&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Given a pre-trained CNN&lt;/em&gt;, use &lt;em&gt;hidden layer l&lt;/em&gt; to compute J_content. The depth of &lt;em&gt;l&lt;/em&gt; controls the level of details focuses on.  &lt;/p&gt;
&lt;p&gt;Define J_content(C, G) = difference (e.g. L2-norm) between the activation of layer l of image C and image G.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image019.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="style-cost-function"&gt;Style Cost Function&lt;/h3&gt;
&lt;p&gt;Use layer &lt;code&gt;l&lt;/code&gt; to measure "style".&lt;br&gt;
→ &lt;strong&gt;style&lt;/strong&gt; defined as &lt;em&gt;correlation between activations across channels&lt;/em&gt;.&lt;br&gt;
e.g. &lt;code&gt;n_C=5&lt;/code&gt; channels of slices &lt;code&gt;n_W * n_H&lt;/code&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image020.png"&gt;&lt;br&gt;
correlation between 2 channel ~= &lt;em&gt;which high-level features tend to occur / not occur together in an image.&lt;/em&gt; &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;style matrix&lt;/strong&gt;&lt;br&gt;
notation:  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a_ijk&lt;/code&gt; = (scalar)activation at hight=&lt;code&gt;i&lt;/code&gt;, width=&lt;code&gt;j&lt;/code&gt;, channel=&lt;code&gt;k&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;"Style matrix) &lt;code&gt;G[l]&lt;/code&gt;, (G for "Gram matrix") shape = &lt;code&gt;n_C * n_C&lt;/code&gt;, measures &lt;em&gt;how correlated any two channels are. &lt;/em&gt;(i.e. G[l] measures the degree to which the activations of different feature detectors in layer l vary (or correlate) together with each other.)  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;G_kk&lt;/code&gt;' = correlation between kth and k'th channel.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;:= sum_over_i_j( a_ijk * a_ijk')  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image021.png"&gt;&lt;br&gt;
mathematically, this "correlation" is unnormalized cross-covariance (without substracting the mean).  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Compute &lt;code&gt;G&lt;/code&gt; for both the style image and generated image.&lt;br&gt;
→ J_style(Gen_img, Sty_img) = difference (Frobenius norm) between G(gen) and G(sty)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image022.png"&gt;&lt;br&gt;
In practice, take J_style for multiple layers:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image023.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="1d-and-3d-generalizations"&gt;1D and 3D Generalizations&lt;/h3&gt;
&lt;p&gt;images: 2D data.&lt;br&gt;
→ &lt;em&gt;generalize convolution to 1D and 3D data.&lt;/em&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1D data&lt;/strong&gt;&lt;br&gt;
e.g. EKG data (heart voltage).&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;(note: most 1D data use RNN...)  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3D data&lt;/strong&gt;&lt;br&gt;
have height width and depth,&lt;br&gt;
e.g. C.T. data; movie date (frame by frame)&lt;br&gt;
→ generalized from 2D&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk4/pasted_image025.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Convolutional Neural Networks] week3. Object detection</title><link href="http://x-wei.github.io/Ng_DLMooc_c4wk3.html" rel="alternate"></link><published>2017-11-27T00:00:00+01:00</published><updated>2017-11-27T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-27:/Ng_DLMooc_c4wk3.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h3 id="object-localization"&gt;Object Localization&lt;/h3&gt;
&lt;p&gt;Classification VS. Localization VS. Detection&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;classification with localization&lt;/strong&gt;&lt;br&gt;
Apart from softmax output (for classification), &lt;em&gt;add 4 more outputs of bounding box&lt;/em&gt;: &lt;code&gt;b_x, b_y, b_h, b_w&lt;/code&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image002.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining target label y in localization&lt;/strong&gt;&lt;br&gt;
label format:&lt;br&gt;
&lt;code&gt;P_c&lt;/code&gt; indicating if there's any object&lt;br&gt;
bounding box: &lt;code&gt;b_x, b_y, b_h, b_w&lt;/code&gt;&lt;br&gt;
class proba …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h3 id="object-localization"&gt;Object Localization&lt;/h3&gt;
&lt;p&gt;Classification VS. Localization VS. Detection&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;classification with localization&lt;/strong&gt;&lt;br&gt;
Apart from softmax output (for classification), &lt;em&gt;add 4 more outputs of bounding box&lt;/em&gt;: &lt;code&gt;b_x, b_y, b_h, b_w&lt;/code&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image002.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining target label y in localization&lt;/strong&gt;&lt;br&gt;
label format:&lt;br&gt;
&lt;code&gt;P_c&lt;/code&gt; indicating if there's any object&lt;br&gt;
bounding box: &lt;code&gt;b_x, b_y, b_h, b_w&lt;/code&gt;&lt;br&gt;
class proba: &lt;code&gt;c_1, c_2, c_3&lt;/code&gt; &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Loss function: squared error&lt;br&gt;
if y_1=P_c=1: loss = square error (y, y_hat)&lt;br&gt;
if y_1=P_c=0: loss = (y_1 - y_1_hat)^2&lt;br&gt;
can use different loss function for different components, but sq-loss works in practice.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image003.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="landmark-detection"&gt;Landmark Detection&lt;/h3&gt;
&lt;p&gt;"landmark": important points in image. → let NN output their coords.  &lt;/p&gt;
&lt;p&gt;e.g. recognize coord of eye's corner or points along the eye/nose/mouth&lt;br&gt;
→ specify a number of landmarks&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="object-detection"&gt;Object Detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;sliding windows detection&lt;/strong&gt;&lt;br&gt;
example: car detection.&lt;br&gt;
training image: &lt;em&gt;closely-croped&lt;/em&gt; image&lt;br&gt;
in prediciton: use sliding window and pass to ConvNet; use window of different size.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image007.png"&gt;&lt;br&gt;
Sliding window is OK with pre-DL algos.&lt;br&gt;
disadvantage: computation cost too high — each window's crop ran &lt;em&gt;independently&lt;/em&gt; through ConvNet.&lt;br&gt;
→ sliding window also can be implemented "convolutionally" — some computation can be cached.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="convolutional-implementation-of-sliding-windows"&gt;Convolutional Implementation of Sliding Windows&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Turning FC layer into conv layers&lt;/strong&gt;&lt;br&gt;
example: last conv/maxpool layer: size=5&lt;em&gt;5&lt;br&gt;
→ &lt;/br&gt;&lt;/em&gt;replace FC(output_dim=400) by 400 5&lt;em&gt;5 filters&lt;/em&gt;&lt;br&gt;
→ replace next FC layer by 1&lt;em&gt;1 filters&lt;br&gt;
→ replace softmax layer by 1&lt;/br&gt;&lt;/em&gt;1 filters and activation.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;conv implementation of sliding window&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image009.png"&gt;&lt;br&gt;
example: training image 14&lt;em&gt;14&lt;/em&gt;3, testing image 16&lt;em&gt;16&lt;/em&gt;3&lt;br&gt;
instead of corping image to 14&lt;em&gt;14 and feed to ConvNet, &lt;/em&gt;feed the larger picture directly to ConvNet*.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image010.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;→ &lt;em&gt;output contains results of all patches&lt;/em&gt;!&lt;br&gt;
⇒ instead of computing each sliding window sequentially, can get all results with a single pass of the full image!!&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image012.png"&gt;&lt;br&gt;
problem: bounding box position is not accurate.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="bounding-box-predictions"&gt;Bounding Box Predictions&lt;/h3&gt;
&lt;p&gt;To output more accurate bounding boxes: aspect-ration no longer 1:1.&lt;br&gt;
&lt;strong&gt;YOLO algorithm&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image013.png"&gt;&lt;br&gt;
"You Only Look Once"&lt;br&gt;
For each grid cell: apply image classification with bouding boxes (described in 1st section, 8 outputs).&lt;br&gt;
needs labelled data: assign each obj to the grid where its center is in.&lt;br&gt;
&lt;em&gt;output volume: 3&lt;/em&gt;3&lt;em&gt;8&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image014.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image015.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Also: a lot of computation shared, efficient ⇒ possible to do real-time.  &lt;/p&gt;
&lt;p&gt;note: bounding box annotation in YOLO &lt;em&gt;can be out of [0,1] range.&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image016.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="intersection-over-union"&gt;Intersection Over Union&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Evaluating&lt;/strong&gt; object localization:&lt;br&gt;
→  intersection over union (IoU) function = size(intesection) / size(union) = &lt;em&gt;measure of overlap of two bounding boxes.&lt;/em&gt;&lt;br&gt;
"correct" if IoU &amp;gt;= 0.5&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image017.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="non-max-suppression"&gt;Non-max Suppression&lt;/h3&gt;
&lt;p&gt;Problem: algo might detect the same obj multiple times.&lt;br&gt;
example:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image018.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image019.png"&gt;&lt;br&gt;
each bouding box has a confidence score — keep the max bouding box, suppress the overlapping ones.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image020.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image021.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="anchor-boxes"&gt;Anchor Boxes&lt;/h3&gt;
&lt;p&gt;Problem: each grid detects only one obj → can a grid &lt;em&gt;detect multiple obj&lt;/em&gt; ? → use anchor boxes.  &lt;/p&gt;
&lt;p&gt;In data labeling: predefine 2 shapes (anchor boxes); use 8 sets of 8 outputs for each anchor box.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image022.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Compare with previous:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;previous: each obj assigned to the grid which contains its mid point  &lt;/li&gt;
&lt;li&gt;now each obj assigned to (cell, anchorbox): cell=the grid which contains its mid point; anchor_box=&lt;em&gt; the anchorbox that has highest IoU with the labelled bounding box&lt;/em&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image023.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;In practice: choose 5~10 anchor boxes by hand; or use Kmeans on object's shapes.  &lt;/p&gt;
&lt;h3 id="yolo-algorithm"&gt;YOLO Algorithm&lt;/h3&gt;
&lt;p&gt;Put all components together.&lt;br&gt;
example:  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;detecting pedestrian/car/motercycle. (4 classes)  &lt;/li&gt;
&lt;li&gt;grid: 3*3  &lt;/li&gt;
&lt;li&gt;2 anchor boxes  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ &lt;strong&gt;Preparing training set&lt;/strong&gt;&lt;br&gt;
y shape = 3*3*2*8&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;train a ConvNet on this with output_dim = 3&lt;em&gt;3&lt;/em&gt;16&lt;br&gt;
→ &lt;strong&gt;making predictions&lt;/strong&gt;&lt;br&gt;
2*8 outputs for each of the 9 grids&lt;br&gt;
→ &lt;strong&gt;nonmax supression for each class&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image025.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="optional-region-proposals"&gt;(Optional) Region Proposals&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image026.png"&gt;&lt;br&gt;
Region proposal algo (R-CNN): less often than YOLO.&lt;br&gt;
Sliding window disadvantage: many regions are not interesting.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image027.png"&gt;&lt;br&gt;
⇒ select just a few windows&lt;br&gt;
first run segmentation algo, then run CNN on bounding box of blobs.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image028.png"&gt;&lt;br&gt;
→ still quite slow&lt;br&gt;
faster variants:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk3/pasted_image029.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Convolutional Neural Networks] week2. Deep convolutional models: case studies</title><link href="http://x-wei.github.io/Ng_DLMooc_c4wk2.html" rel="alternate"></link><published>2017-11-22T00:00:00+01:00</published><updated>2017-11-22T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-22:/Ng_DLMooc_c4wk2.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-case-studies"&gt;I-Case studies&lt;/h2&gt;
&lt;h3 id="why-look-at-case-studies"&gt;Why look at case studies?&lt;/h3&gt;
&lt;p&gt;Good way to get intuition of different component of CNN: case study &amp;amp; reading paper.&lt;br&gt;
&lt;strong&gt;Outline&lt;/strong&gt; &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classic networks:  &lt;ul&gt;
&lt;li&gt;LeNet-5  &lt;/li&gt;
&lt;li&gt;AlexNet  &lt;/li&gt;
&lt;li&gt;VGG  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ResNet (152-layer NN)  &lt;/li&gt;
&lt;li&gt;Inception  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="classic-networks"&gt;Classic Networks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LeNet-5&lt;/strong&gt;(1998)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image002.png"&gt;&lt;br&gt;
Goal: recognize hand-written digits.&lt;br&gt;
image → 2 CONV-MEANPOOL layers, all CONV are valid (without padding …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-case-studies"&gt;I-Case studies&lt;/h2&gt;
&lt;h3 id="why-look-at-case-studies"&gt;Why look at case studies?&lt;/h3&gt;
&lt;p&gt;Good way to get intuition of different component of CNN: case study &amp;amp; reading paper.&lt;br&gt;
&lt;strong&gt;Outline&lt;/strong&gt; &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classic networks:  &lt;ul&gt;
&lt;li&gt;LeNet-5  &lt;/li&gt;
&lt;li&gt;AlexNet  &lt;/li&gt;
&lt;li&gt;VGG  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ResNet (152-layer NN)  &lt;/li&gt;
&lt;li&gt;Inception  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="classic-networks"&gt;Classic Networks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LeNet-5&lt;/strong&gt;(1998)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image002.png"&gt;&lt;br&gt;
Goal: recognize hand-written digits.&lt;br&gt;
image → 2 CONV-MEANPOOL layers, all CONV are valid (without padding) → 2 FC → softmax&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image.png"&gt;&lt;br&gt;
takeaway (patterns still used today):  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;as go deeper, n_H, n_W goes down, n_C goes up  &lt;/li&gt;
&lt;li&gt;conv-pool repeated some times, then FC-FC-output  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;sidenote:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;used sigmoid/tanh as activation, instead of ReLU.  &lt;/li&gt;
&lt;li&gt;has non-linearity after pooling  &lt;/li&gt;
&lt;li&gt;orignial paper hard to read  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image006.png"&gt;&lt;br&gt;
Same pattern: conv-maxpool layers → FC layers → softmax&lt;br&gt;
but much more params.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image004.png"&gt;&lt;br&gt;
sidenote:  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use ReLU as activation  &lt;/li&gt;
&lt;li&gt;multi-GPU training  &lt;/li&gt;
&lt;li&gt;"local response normalization" (LRN): normalize across all channels (not widely used today).  &lt;/li&gt;
&lt;li&gt;a lot hparams to pick  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;VGG-16&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image007.png"&gt;&lt;br&gt;
Much less hparams:&lt;br&gt;
All &lt;strong&gt;CONV: 3&lt;em&gt;3,s=1,padding=same, MAXPOOL: 2&lt;/em&gt;2,s=2&lt;/strong&gt;&lt;br&gt;
→ e.g. "(CONV 64) * 2" meaning 2 conv layers (3*3,s=1,padding=same) of 64 channels.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;note:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pretty large even by modern standard: 138M params  &lt;/li&gt;
&lt;li&gt;simplicity in architecture: POOL reduce n_H/n_W by 2 each time; CONV n_C=64-&amp;gt;128-&amp;gt;256-&amp;gt;512 (increase by 2), very systematic.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="resnets"&gt;ResNets&lt;/h3&gt;
&lt;p&gt;Very deep NN are hard to train → ResNet: &lt;em&gt;skip connections&lt;/em&gt;, to be able to train ~100 layers NN.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Residual block&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image012.png"&gt;&lt;br&gt;
Normal NN: from a[l] to a[l+2], two linear &amp;amp; ReLU operations. &lt;em&gt;"main path"&lt;/em&gt;.&lt;br&gt;
ResNet: a[l] taks shortcut and &lt;em&gt;goes directly to a[l+2]'s non-linearity&lt;/em&gt;. "&lt;em&gt;shortcut&lt;/em&gt;" / "&lt;em&gt;skip connection&lt;/em&gt;".&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image010.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image009.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image039.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Using residual block allows training &lt;em&gt;very deep&lt;/em&gt; NN:&lt;br&gt;
stack them to get ResNet (i.e. add shortcuts to "plain" NN).  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image013.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Problem of training  plain NN: &lt;em&gt;training error goes up (in practice) when having deeper NN&lt;/em&gt;.&lt;br&gt;
Because deeper NN are harder to train (vanishing/exploding gradients, etc.)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image016.png"&gt;&lt;br&gt;
With ResNet: training error goes down even with deeper layers.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image015.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="why-resnets-work"&gt;Why ResNets Work&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;a[l+2] = g(z[l+2] + a[l])  &lt;/span&gt;
&lt;span class="code-line"&gt;  = g(w[l+1] * a[l+1] + b[l+1] + a[l])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;→ note: when applying weight decay, w can be small (w~=0, b~=0)&lt;br&gt;
⇒ a[l+2] ~= g(a[l]) = a[l] (assume g=ReLU)&lt;br&gt;
⇒ it's easy to get a[l+2]=a[l], i.e. &lt;em&gt;identity function from a[l] to a[l+2] is easily learned&lt;/em&gt;&lt;br&gt;
→ whereas in plain NN, it's difficult to learn an identity function between layers, thus more layers make result &lt;em&gt;worse&lt;/em&gt;&lt;br&gt;
→ adding 2 layers doesn't hurt the network to learn a shallower NN's function, i.e. performance is not hurt when increasing #layers.&lt;br&gt;
→ when necessary can do even better than learning identity function&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image017.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Side note:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;z[l+2]&lt;/code&gt; and &lt;code&gt;a[l]&lt;/code&gt; have the same dimension (so that they can be added in g) → i.e. many "same" padding are used to preserve dimension.  &lt;/li&gt;
&lt;li&gt;If  their dimensions are not matched (e.g. for pooling layers) → add extra &lt;code&gt;w_s&lt;/code&gt; to be applied on &lt;code&gt;a[l]&lt;/code&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image018.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image019.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="networks-in-networks-and-1x1-convolutions"&gt;Networks in Networks and 1x1 Convolutions&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image021.png"&gt;&lt;br&gt;
Using 1*1 conv: for one single channel, just multiply the input image(slice) by a constant...&lt;br&gt;
But for &amp;gt;1 channels: each output number is inner prod of input channel "slice" and conv filter.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image020.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;1&lt;em&gt;1 conv: ~= fully-connected layer applied to each of n_H&lt;/em&gt;n_W slices, adds non-linearity to NN.&lt;br&gt;
→ 1&lt;em&gt;1 conv also called "&lt;/em&gt;network in network*"  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example:&lt;br&gt;
To &lt;em&gt;shrink&lt;/em&gt; #channels via 1*1 conv.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image022.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="inception-network-motivation"&gt;Inception Network Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image023.png"&gt;&lt;br&gt;
Instead of choosing filter size, &lt;em&gt;do them all in parallel.&lt;/em&gt;&lt;br&gt;
note: use SAME padding &amp;amp; stride=1 to have the same n_H, n_W&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Problem: computation cost.&lt;br&gt;
example: input shape = 28&lt;em&gt;28&lt;/em&gt;192, filter 5&lt;em&gt;5&lt;/em&gt;192, 32 filters, output shape = 28&lt;em&gt;28&lt;/em&gt;32&lt;br&gt;
totoal #multiplication = 28 * 28 * 32 * 5 * 5 * 192 = 120M&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image025.png"&gt;&lt;br&gt;
→ &lt;strong&gt;reduce #computation with 1*1 conv&lt;/strong&gt;&lt;br&gt;
Reduce n_C of input by 1&lt;em&gt;1 conv ("bottleneck-layer") before doing the 5&lt;/em&gt;5 conv.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h1 id="computation-11192-282816-5516-282832-124m_2"&gt;computation = 1&lt;em&gt;1&lt;/em&gt;192 * 28&lt;em&gt;28&lt;/em&gt;16 + 5&lt;em&gt;5&lt;/em&gt;16 * 28&lt;em&gt;28&lt;/em&gt;32 = 12.4M&lt;/h1&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image027.png"&gt;&lt;br&gt;
Does bottleneck layer hurt model performance ? → no.  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;h3 id="inception-network"&gt;Inception Network&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Inception module&lt;/strong&gt;:&lt;br&gt;
For max pooling layer, out n_C equals input n_C → &lt;em&gt;use a 1&lt;/em&gt;1 conv to shrink n_C*.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image028.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inception network:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Putting inception modules together.  &lt;/li&gt;
&lt;li&gt;Have &lt;em&gt;side branches&lt;/em&gt;: taking hidden layer and feed to FC for output.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;— ensure features from hidden units at intermediate layers are not too bad for prediction — kind of regularization&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image030.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;The name "inception" come from: a meme...&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image031.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="ii-practical-advice-for-using-convnets_1"&gt;II-Practical advice for using ConvNets&lt;/h2&gt;
&lt;p&gt;Advices on how to use these classical CNN models.  &lt;/p&gt;
&lt;h3 id="using-open-source-implementation"&gt;Using Open-Source Implementation&lt;/h3&gt;
&lt;p&gt;Difficult to replicate the work just from paper: a lot of details&amp;amp;hparams&lt;br&gt;
→ use open-sourced version.  &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="transfer-learning"&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;Download weights of other's NN as pretrained params.&lt;br&gt;
→ pretrained params are trained on huge datasets, and takes weeks to train on multiple GPUs.&lt;br&gt;
example: cat detector  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 class: tigger/misty/neither  &lt;/li&gt;
&lt;li&gt;training set at hand is small  &lt;/li&gt;
&lt;li&gt;→ download both code and weights online  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g. ImageNet NN&lt;br&gt;
→ change last layer's softmax&lt;br&gt;
→ all Conv/Pool layers set &lt;em&gt;frozen&lt;/em&gt; (not trainable)&lt;br&gt;
→ only training softmax layer's weight with training set.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image032.png"&gt;&lt;br&gt;
OR:&lt;br&gt;
&lt;em&gt;Precompute&lt;/em&gt; the hidden layer (fixed function mapping from x to feature vector) and save to disk.&lt;br&gt;
→ train a shallow model on top. → save computation.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;If have a large training set at hand ⇒ freeze a few layers and train the rest.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image033.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;If have a &lt;em&gt;huge&lt;/em&gt;  dataset: train the whole NN.  &lt;/p&gt;
&lt;h3 id="data-augmentation"&gt;Data Augmentation&lt;/h3&gt;
&lt;p&gt;More data are alway welcome.&lt;br&gt;
&lt;strong&gt;Common augmentation method&lt;/strong&gt;:  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mirroring  &lt;/li&gt;
&lt;li&gt;Randome cropping  &lt;/li&gt;
&lt;li&gt;Rotation/Shearing/Local warping: used a bit less in practice  &lt;/li&gt;
&lt;li&gt;Color shifting  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image035.png"&gt;&lt;br&gt;
In practice: shifts drawn from some random distribution.&lt;br&gt;
e.g. PCA-color-augmentation (details in AlexNet paper): ~keep overall color the same.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Implementaing distortions during training&lt;/strong&gt;&lt;br&gt;
If data is huge → CPU thread to get &lt;em&gt;stream&lt;/em&gt; of images → add distortion for each image → form minibatch of data → pass to training.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image036.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="state-of-computer-vision"&gt;State of Computer Vision&lt;/h3&gt;
&lt;p&gt;Observations for DL for CV.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data VS. hand-engineering&lt;/strong&gt;&lt;br&gt;
As more data are available → simpler algo, less hand-engineering.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image037.png"&gt;&lt;br&gt;
Learing algo has 2 sources of knowledge:  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;labeled data  &lt;/li&gt;
&lt;li&gt;hand engineered features / network architecture / specialized components  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transfer learning can help when dataset is small.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips for doing well on benchmarks/winning competitions&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensembling:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Train several(3~15) NN independently, then &lt;em&gt;average their outputs&lt;/em&gt;.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Multi-crop at test time&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Predict on multiple versions of test images and average results.&lt;br&gt;
e.g. 10-crop at test time&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk2/pasted_image038.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Convolutional Neural Networks] week1. Foundations of Convolutional Neural Networks</title><link href="http://x-wei.github.io/Ng_DLMooc_c4wk1.html" rel="alternate"></link><published>2017-11-19T00:00:00+01:00</published><updated>2017-11-19T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-19:/Ng_DLMooc_c4wk1.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h3 id="computer-vision"&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;CV with DL: rapid progress in past two years.  &lt;/p&gt;
&lt;p&gt;CV problems:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image classification  &lt;/li&gt;
&lt;li&gt;object detection: bounding box of objects  &lt;/li&gt;
&lt;li&gt;neural style transfer  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;input features could be very high dimension: e.g. 1000x1000 image → 3 million dim input ⇒ if 1st layer has 1000 hidden units → &lt;em&gt;3 billion params …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h3 id="computer-vision"&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;CV with DL: rapid progress in past two years.  &lt;/p&gt;
&lt;p&gt;CV problems:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image classification  &lt;/li&gt;
&lt;li&gt;object detection: bounding box of objects  &lt;/li&gt;
&lt;li&gt;neural style transfer  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;input features could be very high dimension: e.g. 1000x1000 image → 3 million dim input ⇒ if 1st layer has 1000 hidden units → &lt;em&gt;3 billion params for first layer...&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;foundamental operation: convolution.  &lt;/p&gt;
&lt;h3 id="edge-detection-example"&gt;Edge Detection Example&lt;/h3&gt;
&lt;p&gt;Motivating example for convolution operation: detecting vertical edges.  &lt;/p&gt;
&lt;p&gt;Convolve image with a &lt;strong&gt;filter(kernel) &lt;/strong&gt;matrix:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image001.png"&gt;&lt;br&gt;
Each element in resulting matrix: sum(element-wise multiplication of filter and input image).  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Why the filter can detect vertical edge?&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="more-edge-detection"&gt;More Edge Detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Positive V.S. negative edges&lt;/strong&gt;:&lt;br&gt;
dark to light V.S. light to dark&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image004.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Instead of picking filter by hand, the actual parameters can be &lt;em&gt;learned&lt;/em&gt; by ML.&lt;br&gt;
Next: discuss some building blocks of CNN, padding/striding/pooling...  &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="padding"&gt;Padding&lt;/h3&gt;
&lt;p&gt;Earlier: image &lt;em&gt;shrinks&lt;/em&gt; after convolution.&lt;br&gt;
Input &lt;code&gt;n*n&lt;/code&gt; image, convolve with &lt;code&gt;f*f&lt;/code&gt; filter ⇒ output shape = &lt;code&gt;(n-f+1) * (n-f+1)&lt;/code&gt;&lt;br&gt;
downside:  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image shrinks on every step (if 100 layer → shrinks to very small images)  &lt;/li&gt;
&lt;li&gt;pixels &lt;em&gt;at corner&lt;/em&gt; are less used in the output  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ &lt;em&gt;pad the image&lt;/em&gt; so that output shape is invariant.  &lt;/p&gt;
&lt;p&gt;if &lt;code&gt;p&lt;/code&gt; = padding amount (width of padded border)&lt;br&gt;
→ output shape = &lt;code&gt;(n+2p-f+1) * (n+2p-f+1)&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Terminology: &lt;em&gt;valid&lt;/em&gt; and &lt;em&gt;same&lt;/em&gt; convolutions:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;valid&lt;/strong&gt; convolution: no padding, output shape = (n-f+1) * (n-f+1)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;same&lt;/strong&gt; convolution: output size equals input size. i.e. filter width &lt;code&gt;p = (f-1) / 2&lt;/code&gt; (only works &lt;em&gt;when f is odd — &lt;/em&gt;this is also a convention in CV, partially because this way there'll be a central filter)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="strided-convolutions"&gt;Strided Convolutions&lt;/h3&gt;
&lt;p&gt;Example &lt;code&gt;stride = 2&lt;/code&gt; in convolution:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image005.png"&gt;&lt;br&gt;
(convention: stop moving if filter goes out of image border.)  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;if input image &lt;code&gt;n*n&lt;/code&gt;, filter size &lt;code&gt;f*f&lt;/code&gt;, padding = &lt;code&gt;f&lt;/code&gt;, stride = &lt;code&gt;s&lt;/code&gt;&lt;br&gt;
⇒ output shape = &lt;code&gt;(floor((n+2p-f)/s) + 1) * (floor((n+2p-f)/s) + 1)&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image006.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note on cross-correlation v.s. convolution&lt;/strong&gt;&lt;br&gt;
In math books, "convolution" involves &lt;em&gt;flip filter in both direction&lt;/em&gt; before doing "convolution" operation.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image007.png"&gt;&lt;br&gt;
The operation discribed before is called "cross-correlation".  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;(Why doing the flipping in math: to ensure assosative law for convolution — (A&lt;em&gt;B)&lt;/em&gt;C=A&lt;em&gt;(B&lt;/em&gt;C).)  &lt;/p&gt;
&lt;h3 id="convolutions-over-volume"&gt;Convolutions Over Volume&lt;/h3&gt;
&lt;p&gt;example: convolutions on RGB image&lt;br&gt;
image size = 6&lt;em&gt;6&lt;/em&gt;3 = height * width * #channels&lt;br&gt;
filter size = 3&lt;em&gt;3&lt;/em&gt;3, (convention: &lt;em&gt;filter's #channels matches the image&lt;/em&gt;)&lt;br&gt;
output size = 4&lt;em&gt;4 (&lt;/em&gt;1)  —  &lt;strong&gt;output is 2D for each filter&lt;/strong&gt;.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image008.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiple filters:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;take &amp;gt;1 filters  &lt;/li&gt;
&lt;li&gt;stack outputs together to form an &lt;em&gt;output volume&lt;/em&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image009.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary of dimensions&lt;/strong&gt;:&lt;br&gt;
input shape = &lt;code&gt;n*n*n_c&lt;/code&gt;&lt;br&gt;
filter shape = &lt;code&gt;f*f*n_c&lt;/code&gt; &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h1 id="filters-n_c_1"&gt;filters = &lt;code&gt;n_c&lt;/code&gt;'&lt;/h1&gt;
&lt;p&gt;⇒ output shape = &lt;code&gt;(n-f+1) * (n-f+1) * n_c&lt;/code&gt;'  &lt;/p&gt;
&lt;h3 id="one-layer-of-a-convolutional-network"&gt;One Layer of a Convolutional Network&lt;/h3&gt;
&lt;p&gt;For each filter's output: &lt;em&gt;add bias b, then apply nonlinear activation function.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;One layer of a CNN:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image011.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;with analogy to normall NN:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;linear operation (matrix mul V.S. convolution)  &lt;/li&gt;
&lt;li&gt;bias  &lt;/li&gt;
&lt;li&gt;nonlinear activation  &lt;/li&gt;
&lt;li&gt;difference: Number of parameters &lt;em&gt;doesn't depend on input dimension&lt;/em&gt;: even for very large images.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notation summary:&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image014.png"&gt;&lt;br&gt;
note: &lt;em&gt;ordering&lt;/em&gt; of dimensions: example index, height, width, #channel.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="simple-convolutional-network-example"&gt;Simple Convolutional Network Example&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image015.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;general trend: as going to later layers, &lt;em&gt;image size shrinks, #channels increases&lt;/em&gt;.  &lt;/p&gt;
&lt;h3 id="pooling-layers"&gt;Pooling Layers&lt;/h3&gt;
&lt;p&gt;Pooling layers makes CNN more robust.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Max pooling&lt;/strong&gt;&lt;br&gt;
divide input into regions, take max of each region.  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hyperparams:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(common choice) &lt;em&gt;filter size f=2 or 3, strid size s=2, padding p=0.&lt;/em&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;note: &lt;em&gt;no params to learn&lt;/em&gt; for max pooling layer, pooling layer not counted in #layers (conv-pool as a single layer)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image017.png"&gt;&lt;br&gt;
Intuition: a large number indicats a detected feature in that region → preseved after pooling.  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Formula of dimension &lt;code&gt;floor((n+2p-f+1)/s + 1)&lt;/code&gt; holds for POOL layer as well.  &lt;/p&gt;
&lt;p&gt;Output of max pooling: the same #channels as input (i.e. do maxpooling on each channel).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Average pooling&lt;/strong&gt;&lt;br&gt;
Less often used than max pooling.&lt;br&gt;
Typical usecase: collapse 7&lt;em&gt;7&lt;/em&gt;1000 activation into 1&lt;em&gt;1&lt;/em&gt;1000.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="cnn-example"&gt;CNN Example&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;LeNet-5&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image018.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c4wk1/pasted_image020.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="why-convolutions"&gt;Why Convolutions?&lt;/h3&gt;
&lt;p&gt;2 main advantages of CONV over FC: &lt;em&gt;param sharing; sparsity of connections.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Parameter sharing&lt;/strong&gt;:&lt;br&gt;
A feature detector useful in one part of img is probably useful in another part as well.&lt;br&gt;
→ no need to learn separate feature detectors in different parts.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparsity of connections&lt;/strong&gt;:&lt;br&gt;
For each output value depends only on a small number of inputs (the pixels near that position)  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invarance to translation...  &lt;/li&gt;
&lt;/ul&gt;</content><category term="deep learning"></category></entry><entry><title>[Structuring Machine Learning Projects] week2. ML Strategy (2)</title><link href="http://x-wei.github.io/Ng_DLMooc_c3wk2.html" rel="alternate"></link><published>2017-11-17T00:00:00+01:00</published><updated>2017-11-17T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-17:/Ng_DLMooc_c3wk2.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-error-analysis"&gt;I-Error Analysis&lt;/h2&gt;
&lt;h3 id="carrying-out-error-analysis"&gt;Carrying out error analysis&lt;/h3&gt;
&lt;p&gt;"&lt;strong&gt;Error analysis&lt;/strong&gt;": manually examine the mistakes → get insight of what's next.  &lt;/p&gt;
&lt;p&gt;"&lt;em&gt;ceiling on performance&lt;/em&gt;"  &lt;/p&gt;
&lt;p&gt;example:&lt;br&gt;
cat classification, found some false-positives of dog pictures. → should you try to make ML system better on dog or not ?&lt;br&gt;
→ error analysis:  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get ~100 false positive examples  &lt;/li&gt;
&lt;li&gt;count …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-error-analysis"&gt;I-Error Analysis&lt;/h2&gt;
&lt;h3 id="carrying-out-error-analysis"&gt;Carrying out error analysis&lt;/h3&gt;
&lt;p&gt;"&lt;strong&gt;Error analysis&lt;/strong&gt;": manually examine the mistakes → get insight of what's next.  &lt;/p&gt;
&lt;p&gt;"&lt;em&gt;ceiling on performance&lt;/em&gt;"  &lt;/p&gt;
&lt;p&gt;example:&lt;br&gt;
cat classification, found some false-positives of dog pictures. → should you try to make ML system better on dog or not ?&lt;br&gt;
→ error analysis:  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get ~100 false positive examples  &lt;/li&gt;
&lt;li&gt;count how many are dogs  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ if only 5% of errors are dogs → performance can improve by &amp;lt;=5% even if totaly solved dog problem.&lt;br&gt;
→ if 50% are dos → might need to improve on dogs.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example2 (evaluate multiple ideas in parallel):&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image.png"&gt;&lt;br&gt;
Pick one idea to iterate on: &lt;em&gt;use a spreadsheet&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="cleaning-up-incorrectly-labeled-data"&gt;Cleaning up incorrectly labeled data&lt;/h3&gt;
&lt;p&gt;What to do if there are &lt;em&gt;incorrect labels&lt;/em&gt; in data ?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In training set:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;DL algos are quite robust to random errors in training set.&lt;/em&gt;&lt;br&gt;
  → if incorrect labels is close to random errors (percentage not too high), it's OK to train.&lt;br&gt;
  caveat: Robust to &lt;em&gt;random&lt;/em&gt; errors, not &lt;em&gt;systematic&lt;/em&gt; errors. E.g. all white dogs are labeled as cats.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In dev/test set:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In error analysis, count cases of incorrect labels.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image002.png"&gt;&lt;br&gt;
  If #incorrect labels makes a significent different for evaluating, then fix it.&lt;br&gt;
  example:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image004.png"&gt;&lt;br&gt;
  Remember: goal of dev set is to help selecting between two models.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Correcting labels in dev/test sets:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;apply the same process to test set — dev/test sets have the same distribution.  &lt;/li&gt;
&lt;li&gt;consider both false positive and false negatives. → to make estimate of performance unbiased. (might take longer time)  &lt;/li&gt;
&lt;li&gt;less important to correct training set: training set can come from slight different distribution, but important dev/test come from the same distribution.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="build-your-first-system-quickly-then-iterate"&gt;Build your first system quickly, then iterate&lt;/h3&gt;
&lt;p&gt;example: speech recognition&lt;br&gt;
many directions to go → which direction to pick ?  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Build system quickly and iterate.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set up dev/test set, set metric  &lt;/li&gt;
&lt;li&gt;build intitial system &lt;em&gt;quickly&lt;/em&gt;: build something quick &amp;amp; dirty that works.  &lt;/li&gt;
&lt;li&gt;Bias/Variance analysis &amp;amp; Error analysis → prioritize next steps  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ii-mistmatched-training-and-devtest-set_1"&gt;II-Mistmatched training and dev/test set&lt;/h2&gt;
&lt;h3 id="training-and-testing-on-different-distributions"&gt;Training and testing on different distributions&lt;/h3&gt;
&lt;p&gt;When distribution of train and dev/test sets are different.  &lt;/p&gt;
&lt;p&gt;example: cat app&lt;br&gt;
Two sources:  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;webpages (high resolution, a lot of data)  &lt;/li&gt;
&lt;li&gt;user uploaded (blury, relatively small amount).  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image005.png"&gt;&lt;br&gt;
&lt;strong&gt;option1&lt;/strong&gt;. put both data together, randomly shuffle &amp;amp; split train/dev/test&lt;br&gt;
&lt;em&gt;advantage&lt;/em&gt;: train/dev/test come from distribution&lt;br&gt;
&lt;em&gt;disadvantage&lt;/em&gt;: many examples in dev/test set come from webpages — but cares more about performance on  user-uploaded examples → not recommended, target is not really what we care about  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;option2&lt;/strong&gt;. training set mostly from web, for dev/test all from user-uploaded.&lt;br&gt;
&lt;em&gt;advantage&lt;/em&gt;: Err_dev/Err_test really reflects what the target is.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example2: speech recognition (speech activated rearview mirror)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training data: many data coming from different sources of speech recognition.  &lt;/li&gt;
&lt;li&gt;dev/test: small amount, coming from speech activated rearview mirror.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;takeaway:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use large training set, even if distribution is different from dev/test set  &lt;/li&gt;
&lt;li&gt;dev/test data should reflect what to expect from the system.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bias-and-variance-with-mismatched-data-distributions"&gt;Bias and Variance with mismatched data distributions&lt;/h3&gt;
&lt;p&gt;B&amp;amp;V analysis changes when training set distribution is different from dev/test set.  &lt;/p&gt;
&lt;p&gt;When distr(train)!=distr(dev/test):&lt;br&gt;
&lt;em&gt;No longer can say system has large variance problem when seeing Err_train &amp;lt; Err_dev&lt;/em&gt;. (Poor performance on dev set may not come from overfitting, but may also from change of distrubtion in data).&lt;br&gt;
⇒ introduce &lt;strong&gt;training-dev set&lt;/strong&gt;: &lt;em&gt;same distrubution as training set, but not used in training&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image007.png"&gt;&lt;br&gt;
Now can look at Err_traindev and see if model has variance/bias problem or data-mismatch problem:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image010.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;General principles&lt;/strong&gt;:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image011.png"&gt;&lt;br&gt;
(also possible to have Err_dev/Err_test &amp;lt; Err_train/Err_traindev, because of data mismatch)  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More general formulation&lt;/strong&gt; (example: rearview mirror):&lt;br&gt;
include Err_human &lt;em&gt;on dev/test data&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image012.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="addressing-data-mismatch"&gt;Addressing data mismatch&lt;/h3&gt;
&lt;p&gt;How to address data-mismatch problem? → no systematic solution.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manual error analysis: understand difference between train and dev sets.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g. noise in car  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;make training data more similar / collect more data similar to dev/test set.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g. simulate noisy in-car data (&lt;em&gt;artificial data synthesis&lt;/em&gt;)  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artificial data synthesis&lt;/strong&gt;&lt;br&gt;
caution: avoid synthesise only a small part of all possible examples.&lt;br&gt;
car noise example:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image014.png"&gt;&lt;br&gt;
car recognition example:&lt;br&gt;
synthesis car pictures from a video game&lt;br&gt;
problem: if there're only 20 different cars in video cars → overfit  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="iii-learning-from-multiple-tasks_1"&gt;III-Learning from multiple tasks&lt;/h2&gt;
&lt;h3 id="transfer-learning"&gt;Transfer learning&lt;/h3&gt;
&lt;p&gt;Learned knowledge from one task applied to a second task.&lt;br&gt;
reason: some low-level features can be shared for different tasks.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example 1. cat classifier applied to X-ray scans diagnosis.&lt;br&gt;
&lt;em&gt;change last output layer of original model&lt;/em&gt;, initial w[L]/b[L] of last layer and retrain the params.&lt;br&gt;
if dataset small: only retrain last layer params (&lt;em&gt;pre-training&lt;/em&gt;)&lt;br&gt;
else: retrain all params (&lt;em&gt;fine-tuning&lt;/em&gt;)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image017.png"&gt;&lt;br&gt;
example 2. speech recognition transfer to trigger word detection&lt;br&gt;
also possilbe to create more layers to NN&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image018.png"&gt;&lt;br&gt;
&lt;strong&gt;When to use transfer learning:&lt;/strong&gt; &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;task A ans B have the same input  &lt;/li&gt;
&lt;li&gt;a lot of data for task A, relatively small amount of data for task B  &lt;/li&gt;
&lt;li&gt;low level feature of task A could be helpful for task B  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="multi-task-learning"&gt;Multi-task learning&lt;/h3&gt;
&lt;p&gt;transfer learning: task A and B are sequential&lt;br&gt;
multi-task learning: &lt;em&gt;in parallel&lt;/em&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example: self-driving car&lt;br&gt;
multiple kind of objects to detect&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image019.png"&gt;&lt;br&gt;
multi-label problem (&lt;em&gt;each example can have multiple labels&lt;/em&gt;):&lt;br&gt;
→ output layer should no longer be softmax&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image020.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Training on NN for 4 tasks instead of 4 separate NNs: early-layer features can be shared.  &lt;/p&gt;
&lt;p&gt;With missing entries are in labels:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image021.png"&gt;&lt;br&gt;
⇒ in loss function, sum only on labeled entries.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image022.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When to use multi-task learning&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lower-level features can be shared  &lt;/li&gt;
&lt;li&gt;similar amount of data for each task — data for other tasks could help learning of main task  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image023.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can train a &lt;em&gt;big enough&lt;/em&gt; NN to do well on all tasks.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in practice: multi-task learning is &lt;em&gt;much less common&lt;/em&gt; than transfer learning.  &lt;/p&gt;
&lt;h2 id="iv-end-to-end-deep-learning_1"&gt;IV-End-to-end deep learning&lt;/h2&gt;
&lt;h3 id="what-is-end-to-end-deep-learning"&gt;What is end-to-end deep learning?&lt;/h3&gt;
&lt;p&gt;E2E: omit multiple stages in pipeline by a single NN.  &lt;/p&gt;
&lt;p&gt;example: speech recognition.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: E2E can work well only when have &lt;em&gt;really large dataset&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;example2: face recognition from camera.&lt;br&gt;
2-stage works better than E2E:&lt;br&gt;
image → face detection → face recognition.&lt;br&gt;
reason: a lot of data for each of the 2 tasks, but much less data for E2E.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;exapmle3: machine translation.&lt;br&gt;
E2E works well because of large amount of training data.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example4: estimating child's age from X-ray img.&lt;br&gt;
separate stages works better.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image026.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="whether-to-use-end-to-end-deep-learning"&gt;Whether to use end-to-end deep learning&lt;/h3&gt;
&lt;p&gt;Pros and cons of E2E learning.&lt;br&gt;
&lt;strong&gt;Pros&lt;/strong&gt;:  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;let the data speak, avoid intermediate values (e.g. phonemes in speech recognition)  &lt;/li&gt;
&lt;li&gt;less hand-designing of components needed  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Need large amount of data (X, Y)  &lt;/li&gt;
&lt;li&gt;Excludes potentially useful hand-designed components  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Key question for applying E2E learning: &lt;em&gt;sufficient&lt;/em&gt; data available to learn a function of the &lt;em&gt;complexity&lt;/em&gt; needed to map from x to y?  &lt;/p&gt;
&lt;p&gt;example: self-driving cars&lt;br&gt;
in practice: multi-stage system&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk2/pasted_image027.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Structuring Machine Learning Projects] week1. ML Strategy (1)</title><link href="http://x-wei.github.io/Ng_DLMooc_c3wk1.html" rel="alternate"></link><published>2017-11-15T00:00:00+01:00</published><updated>2017-11-15T00:00:00+01:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-11-15:/Ng_DLMooc_c3wk1.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-introduction-to-ml-strategy"&gt;I-Introduction to ML Strategy&lt;/h2&gt;
&lt;h3 id="why-ml-strategy"&gt;Why ML Strategy&lt;/h3&gt;
&lt;p&gt;A lot of ideas of improving ML performance: &lt;em&gt;strategy on how to choose&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;→ how to figure out which ones to pursue and which ones to discard ?  &lt;/p&gt;
&lt;h3 id="orthogonalization"&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;How to tune hyperparams &amp;amp; what to expect.  &lt;/p&gt;
&lt;p&gt;TV tuning example: each knob does only one …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="i-introduction-to-ml-strategy"&gt;I-Introduction to ML Strategy&lt;/h2&gt;
&lt;h3 id="why-ml-strategy"&gt;Why ML Strategy&lt;/h3&gt;
&lt;p&gt;A lot of ideas of improving ML performance: &lt;em&gt;strategy on how to choose&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;→ how to figure out which ones to pursue and which ones to discard ?  &lt;/p&gt;
&lt;h3 id="orthogonalization"&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;How to tune hyperparams &amp;amp; what to expect.  &lt;/p&gt;
&lt;p&gt;TV tuning example: each knob does only one thing.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chain of assumptions in ML&lt;/strong&gt;:&lt;br&gt;
training set performance → dev set → test set → real world  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"one knob for each chain"  &lt;/li&gt;
&lt;li&gt;Will go through these "knobs" in this course.  &lt;/li&gt;
&lt;li&gt;Don't use early stopping: this is not orthogonalized enough  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image005.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="ii-setting-up-your-goal_1"&gt;II-Setting up your goal&lt;/h2&gt;
&lt;h3 id="single-number-evaluation-metric"&gt;Single number evaluation metric&lt;/h3&gt;
&lt;p&gt;Faster progress if only have &lt;em&gt;one single real number evaluation metric&lt;/em&gt;. → more efficient in making decisions.  &lt;/p&gt;
&lt;p&gt;example:&lt;br&gt;
Using both percision and recall as metric is not good → &lt;em&gt;difficule to pick the best model to keep on iterating from.&lt;/em&gt;&lt;br&gt;
→ Use F1 score instead.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="satisficing-and-optimizing-metric"&gt;Satisficing and Optimizing metric&lt;/h3&gt;
&lt;p&gt;When it's difficult to pick a single real number eval metric → &lt;em&gt;set up satisficing and optimizing metrics.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;example: accuracy &amp;amp; running time trade-off&lt;br&gt;
Instead of doing a linear combination of the two, use this:&lt;br&gt;
    maximize accuracy&lt;br&gt;
    subject to running time &amp;lt;= 100 ms&lt;br&gt;
In this case: accuracy is &lt;em&gt;optimizing metric&lt;/em&gt;, running time is &lt;em&gt;satisficing metric&lt;/em&gt;.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;In general:&lt;br&gt;
if having N metrics → &lt;em&gt;pick 1 as optimizing metric, the N-1 rest as satisficing metric&lt;/em&gt;.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example: assistant wake-up word accuracy VS false positive.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image006.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="train-dev-test-distributions"&gt;Train-dev-test distributions&lt;/h3&gt;
&lt;p&gt;How to setup dev/test sets.&lt;br&gt;
Idea: dev/test sets come from the same distribution.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;example: cat classification in different regions.&lt;br&gt;
This is bad dev/test setup:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image007.png"&gt;&lt;br&gt;
Good practice: random shuffle data and split into dev/test sets.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image008.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="size-of-the-dev-and-test-sets"&gt;Size of the dev and test sets&lt;/h3&gt;
&lt;p&gt;Pre-DL era, old way of splitting data: 70/30 or 60/30/10 split.&lt;br&gt;
→ resonable when datasets are small. (100~10k examples)  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;In DL era: much more training (~1M) examples.&lt;br&gt;
⇒  98/1/1 split is more resonable.  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Size of test/dev set&lt;/strong&gt;: &lt;em&gt;big enough to give high confidence in system's performance&lt;/em&gt;.  &lt;/li&gt;
&lt;li&gt;OK to not have a test set, but not recommended.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="when-to-change-devtest-sets-and-metrics"&gt;When to change dev/test sets and metrics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;example 1&lt;/strong&gt;&lt;br&gt;
cat classification: algo A has pornographic false positives.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image009.png"&gt;&lt;br&gt;
→ change metric to penalize heavily pornographic FPs.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image011.png"&gt;&lt;br&gt;
to implement this weighting, need to go through dev/test sets.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;example 2&lt;/strong&gt;&lt;br&gt;
Cat classification: user's upload is blury while trained on high quality images.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image012.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="iii-comparing-to-human-level-performance_1"&gt;III-Comparing to human-level performance&lt;/h2&gt;
&lt;h3 id="why-human-level-performance"&gt;Why human-level performance?&lt;/h3&gt;
&lt;p&gt;Workflow ML can be more efficient when trying to match human level performance.  &lt;/p&gt;
&lt;p&gt;Bayes optional error: best possible error, &lt;em&gt;theoritical optimal&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image013.png"&gt;&lt;br&gt;
ML progress usually slows down after surpassing human-level performance:  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually human-level is not far from Bayes optimal  &lt;/li&gt;
&lt;li&gt;as long as ML performace &amp;lt; human, there are tools to improve performance.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image014.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="avoidable-bias"&gt;Avoidable bias&lt;/h3&gt;
&lt;p&gt;Using human rating can prevent overfitting on training set.&lt;br&gt;
exapmle: compare training set error with human performance.  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;err_train &amp;gt; err_human ⇒ focus on reducing bias (e.g. bigger NN)  &lt;/li&gt;
&lt;li&gt;err_train ~= err_human ⇒ focus on reducing variance (e.g. regularize, more training data)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image015.png"&gt;&lt;br&gt;
→ Use human-level error as a proxy for Bayes error.  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Terminology:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Avoidable bias&lt;/strong&gt; is the &lt;em&gt;gap&lt;/em&gt; between training err and Bayes err.&lt;br&gt;
  (interpretataion: some errors are inavoidable because &lt;em&gt;Bayes err is not 0&lt;/em&gt;.)  &lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt;: gap between training err and dev err.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="understanding-human-level-performance"&gt;Understanding human-level performance&lt;/h3&gt;
&lt;p&gt;"&lt;em&gt;Human level error as proxy for Bayes error&lt;/em&gt;"  &lt;/p&gt;
&lt;p&gt;example: Medical image classification.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image016.png"&gt;&lt;br&gt;
⇒ Should pick lowest human error as an estimate (upper bound) of Bayes error.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Error analysis example (which human-err to pick to estimate avoidable bias):&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image018.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="surpassing-human-level-performance"&gt;Surpassing human-level performance&lt;/h3&gt;
&lt;p&gt;What's the avoidable bias when err_train and err_dev are smaller than err_human ?&lt;br&gt;
→ less clear in choosing directions.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;examples of tasks where ML &amp;gt;&amp;gt; human performance:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image019.png"&gt;&lt;br&gt;
⇒ all these tasks are:  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learned from structured data  &lt;/li&gt;
&lt;li&gt;are not natural perception tasks  &lt;/li&gt;
&lt;li&gt;have processed huge amount of data  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="improving-your-model-performance"&gt;Improving your model performance&lt;/h3&gt;
&lt;p&gt;Recall: two fundamental assumptions of supervised learning:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can fit training set well (achieve to ~= avoidable bias)  &lt;/li&gt;
&lt;li&gt;The performance on training set generalize well to dev/test sets. (achieve low variance)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The big roadmap:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c3wk1/pasted_image020.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Improving Deep Neural Networks] week2. Optimization algorithms</title><link href="http://x-wei.github.io/Ng_DLMooc_c2wk2.html" rel="alternate"></link><published>2017-10-23T00:00:00+02:00</published><updated>2017-10-23T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-10-23:/Ng_DLMooc_c2wk2.html</id><summary type="html">&lt;p&gt;[TOC]    &lt;/p&gt;
&lt;p&gt;This week: optimization algos to faster train NN, on large dataset.  &lt;/p&gt;
&lt;h2 id="mini-batch-gradient-descent"&gt;Mini-batch gradient descent&lt;/h2&gt;
&lt;h3 id="batch-vs-mini-batch-gd"&gt;batch v.s. mini-batch GD&lt;/h3&gt;
&lt;p&gt;Compute J on &lt;code&gt;m&lt;/code&gt; examples: vectorization, i.e. stacking x(i) y(i) horizontally.&lt;br&gt;
&lt;code&gt;X = [x(1), ..., x(m)]&lt;/code&gt;&lt;br&gt;
&lt;code&gt;Y = [y(1), ..., y(m)]&lt;/code&gt;&lt;br&gt;
→ still slow or impossible with large …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]    &lt;/p&gt;
&lt;p&gt;This week: optimization algos to faster train NN, on large dataset.  &lt;/p&gt;
&lt;h2 id="mini-batch-gradient-descent"&gt;Mini-batch gradient descent&lt;/h2&gt;
&lt;h3 id="batch-vs-mini-batch-gd"&gt;batch v.s. mini-batch GD&lt;/h3&gt;
&lt;p&gt;Compute J on &lt;code&gt;m&lt;/code&gt; examples: vectorization, i.e. stacking x(i) y(i) horizontally.&lt;br&gt;
&lt;code&gt;X = [x(1), ..., x(m)]&lt;/code&gt;&lt;br&gt;
&lt;code&gt;Y = [y(1), ..., y(m)]&lt;/code&gt;&lt;br&gt;
→ still slow or impossible with large &lt;code&gt;m&lt;/code&gt;.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;⇒ split all m examples into &lt;em&gt;mini-batches&lt;/em&gt;. X^t^, Y^t^&lt;br&gt;
e.g. mini batch size = 1000.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image001.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Minibatch GD:&lt;br&gt;
each step, run one iteration of GD using X{t}, Y{t} instead of doing with full X, Y.&lt;br&gt;
one "&lt;strong&gt;epoch&lt;/strong&gt;": one pass through all training set&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="understanding-mini-batch-gradient-descent_1"&gt;Understanding mini-batch gradient descent&lt;/h2&gt;
&lt;p&gt;with batch-GD: each iteration will decrease cost function.&lt;br&gt;
in mini-batch: cost J^t^ is computed on different dataset — noisy.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image004.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;How to choose minibatch size  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;extreme 1: minibatch size = m → batch GD  &lt;/p&gt;
&lt;p&gt;coverge fastest in each iter (&lt;em&gt;but too long time or impossible per iter&lt;/em&gt;)  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;extreme 2: minibatch size = 1 → &lt;em&gt;stochastic GD&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;noisy, never coverge, (&lt;em&gt;but loose all vectorization speedup)&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image006.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Guidelines on choosing batch size&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;small training set (m&amp;lt;2000) → just batch size  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;otherwise:   &lt;/p&gt;
&lt;p&gt;typical minibatch size = 64/128/256/512 (make sure minibatch size fits in CPU/GPU memory)  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="exponentially-weighted-moveing-averages"&gt;Exponentially weighted (moveing) averages&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In fact, exp-weighted-avg is an non-parametric estimator/smoother of a series of vlaues.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;example: Temperature over the year&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image007.png"&gt;&lt;br&gt;
→ use a &lt;em&gt;exp-weighted moving average&lt;/em&gt; to model this:&lt;br&gt;
&lt;code&gt;theta[t]&lt;/code&gt; = temperature at day t, t = 1,2,3,...&lt;br&gt;
&lt;code&gt;v[t]&lt;/code&gt; = averaged(smoothed) estimate of theta, t = &lt;strong&gt;0&lt;/strong&gt;,1,2,3,...&lt;br&gt;
&lt;strong&gt;exp-weighted average&lt;/strong&gt;: &lt;em&gt;recursivly compute v[t].&lt;/em&gt;&lt;br&gt;
&lt;code&gt;v[0] = 0, v[t] = 0.9 * v[t-1] + 0.1 * theta[t]&lt;/code&gt;&lt;br&gt;
(param: beta = 0.9)&lt;br&gt;
⇒ v[t] ~= &lt;em&gt;average of theta[t] over the last 1/(1-beta) days&lt;/em&gt;. (c.f. next section)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image009.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="understanding-exponentially-weighted-averages"&gt;Understanding exponentially weighted averages&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image010.png"&gt;&lt;br&gt;
Understanding the math of exp-weighted-average:&lt;br&gt;
→ unroll the recursive formula:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image011.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;⇔ v[100] = &lt;em&gt;a convolution of theta[t-100:t] and a exp-decaying function:&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image012.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;A small trick on &lt;em&gt;estimating exps&lt;/em&gt;: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image013.png"&gt;&lt;br&gt;
That's why in previous section, we say this formula ~= averaging over last 1/(1-beta) (=1/epslon) days' theta.&lt;br&gt;
e.g. theta[t-10]'s weight is 0.9 ^ 10 ~= 1/e ~= 0.35, i.e. after 10 days, the weight of theta[t-10] is decayed to ~&amp;lt; 1/3.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantage&lt;/strong&gt; to just computing moving window averages:  in implementation, only keep updating a single variable v_theta — very small memory usage.  &lt;/p&gt;
&lt;h2 id="bias-correction-in-exponentially-weighted-averages"&gt;Bias correction in exponentially weighted averages&lt;/h2&gt;
&lt;p&gt;To make exp-weighted averages more accurate by &lt;em&gt;bias correction.&lt;/em&gt;&lt;br&gt;
Problem with previous implementation in initial phase:   &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v[0] = 0, beta = 0.98  &lt;/li&gt;
&lt;li&gt;v[1] = 0.98&lt;em&gt;0 + 0.02 * theta[1] ⇒ &lt;/em&gt;v[t] starts lower than theta. *(purple curve VS green curve)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image014.png"&gt;&lt;br&gt;
⇒ &lt;strong&gt;Correction&lt;/strong&gt;:&lt;br&gt;
take &lt;code&gt;v[t] / (1 - beta^t)&lt;/code&gt; instead of v[t].&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image015.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;In practice: &lt;em&gt;most people don't bother to implement bias correction&lt;/em&gt; — just wait for the initial phase to warm up...  &lt;/p&gt;
&lt;h2 id="gradient-descent-with-momentum"&gt;Gradient descent with momentum&lt;/h2&gt;
&lt;p&gt;GD with momentum:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;almost always faster than normal GD  &lt;/li&gt;
&lt;li&gt;in short: &lt;em&gt;compute exp-weighted-avg of the gradients as gradient to use&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;example:  if contour of loss is an ellipse, can't use too large step in GD, and oscillate. → average of steps will be faster.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image018.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GD momentum&lt;/strong&gt;:&lt;br&gt;
Use exp-weighted-avg of dW (V_dW) and of db (V_db). — smooth out oscillation steps of normal GD.&lt;br&gt;
update params with the averaged value V_dW, V_db.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image017.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Why the name "momentum": &lt;em&gt;rolling down a ball in a bowl&lt;/em&gt;&lt;br&gt;
dW/db: ~accelation&lt;br&gt;
V_dW/B_db: ~velocity at current time (&lt;em&gt;this is why the smoothing average is called 'v'&lt;/em&gt;)&lt;br&gt;
beta: ~friction  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image019.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;in practice:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beta=0.9 works well for most cases  &lt;/li&gt;
&lt;li&gt;no bias correction implemented  &lt;/li&gt;
&lt;li&gt;can use beta = 0 →  ~V_dW is scaled by 1/(1-beta) → use a scaled alpha then.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ less intuitive, and couples the tuning of alpha and beta  &lt;/p&gt;
&lt;h2 id="rmsprop"&gt;RMSprop&lt;/h2&gt;
&lt;p&gt;"Root-Mean-Square-prop".  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;also for speedup GD  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in short:   &lt;/p&gt;
&lt;p&gt;&lt;em&gt;S =&lt;/em&gt; exp-weighted-avg of gradient &lt;em&gt;squared&lt;/em&gt; &lt;em&gt;(that's why call param beta2);&lt;/em&gt;&lt;br&gt;
when updating params(W,b)&lt;em&gt;, scale dW,db by &lt;strong&gt;&lt;em&gt;sqrt of S&lt;/em&gt;&lt;/strong&gt;. &lt;/em&gt;&lt;code&gt;dW / sqrt(S_dW)&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image022.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;example: ellipse contour, want slow rate in &lt;code&gt;b&lt;/code&gt; directrion and fast rate in &lt;code&gt;w&lt;/code&gt; direction.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image023.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;⇒ for each step (dW, db) = gradient of current minibatch:  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;db &lt;em&gt;large&lt;/em&gt;, dW &lt;em&gt;small&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;→ update w by dW/sqrt(S_dW), ⇔ &lt;em&gt;larger&lt;/em&gt; step in W direction&lt;br&gt;
→ db/sqrt(S_db) ⇔ &lt;em&gt;smaller&lt;/em&gt; step in b direction → damping out oscillations in b direction  &lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="adam-optimization-algorithm"&gt;Adam optimization algorithm&lt;/h2&gt;
&lt;p&gt;"Adaptive Moment-estimation"  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;combination&lt;/em&gt; of RMSprop and momentum.  &lt;/li&gt;
&lt;li&gt;Proved to work well for a varity of problems.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algo:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;maintain both V_dW, V_db (hyper-param=beta1) and S_dW, S_db (hyper-param=beta2)  &lt;/li&gt;
&lt;li&gt;implement bias correction: V_corrected, S_corrected — divid by (1-beta^t)  &lt;/li&gt;
&lt;li&gt;param update (W, b): &lt;code&gt;V / sqrt(S)&lt;/code&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;hyperparameters:   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;alpha: learning rate, &lt;em&gt;needs tuning&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;beta1: usually 0.9  &lt;/li&gt;
&lt;li&gt;beta2: usually 0.999  &lt;/li&gt;
&lt;li&gt;epsilon: 10e-8 (not important)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="learning-rate-decay"&gt;Learning rate decay&lt;/h2&gt;
&lt;p&gt;slowly reduce learning rate.  &lt;/p&gt;
&lt;p&gt;In minibatch with fixed learning rate: &lt;em&gt;will never converge&lt;/em&gt;.&lt;br&gt;
⇒ decay learning rate → oscillating around a smaller region of optima.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image026.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Implementation:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 &lt;em&gt;epoch&lt;/em&gt; = 1 pass through whole data.  &lt;/li&gt;
&lt;li&gt;decay learning rate alpha after each epoch (hyper-param &lt;em&gt;decay-rate&lt;/em&gt;):&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image028.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;other decay method:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exponentially decay alpha:   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image029.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sqrt of epoch_num:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image030.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discrete staircase:  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image031.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manual decay: when training takes &lt;em&gt;long time&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-problem-of-local-optima"&gt;The problem of local optima&lt;/h2&gt;
&lt;p&gt;low-dimention optimas: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image032.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;This gives &lt;em&gt;wrong&lt;/em&gt; intuition, in practice (high-dim), most 0-gradient points are &lt;em&gt;saddle points.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image034.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;plateau&lt;/strong&gt;: region where gradient close to 0 for long time.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk2/pasted_image035.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;take-away:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;unlikely&lt;/em&gt; to stuck in bad local optima: D dimentional → ~2^(-D) of chance.  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;plateaus&lt;/em&gt; can make learning slow → use momentum/RMSprop/Adam to speedup training.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="assignment"&gt;assignment&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;implementing mini-batch GD&lt;/strong&gt;:&lt;br&gt;
shuffle data (note: X and Y's columns are sync-ed!):  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;permutation = list(np.random.permutation(m))  &lt;/span&gt;
&lt;span class="code-line"&gt;        shuffled_X = X[:, permutation]  &lt;/span&gt;
&lt;span class="code-line"&gt;        shuffled_Y = Y[:, permutation].reshape((1,m))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;→ partition data:&lt;br&gt;
kth mini batch:  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]  &lt;/span&gt;
&lt;span class="code-line"&gt;     mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;detail: when m % minibatch_size != 0: handle last batch (smaller than a regular batch)  &lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Improving Deep Neural Networks] week3. Hyperparameter tuning, Batch Normalization and Programming Frameworks</title><link href="http://x-wei.github.io/Ng_DLMooc_c2wk3.html" rel="alternate"></link><published>2017-10-23T00:00:00+02:00</published><updated>2017-10-23T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-10-23:/Ng_DLMooc_c2wk3.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="hyperparameter-parameters"&gt;Hyperparameter parameters&lt;/h2&gt;
&lt;p&gt;Tips for hyperparam-tuning.  &lt;/p&gt;
&lt;h3 id="tuning-process"&gt;Tuning process&lt;/h3&gt;
&lt;p&gt;Many hyperparams to tune, mark importance by colors (red &amp;gt; yellow &amp;gt; purple):&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;How to select set of values to explore ?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do &lt;strong&gt;NOT&lt;/strong&gt; use grid search (grid of n * n)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;— this was OK in pre-DL era.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;try random values.&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;reason: difficule to know which …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h2 id="hyperparameter-parameters"&gt;Hyperparameter parameters&lt;/h2&gt;
&lt;p&gt;Tips for hyperparam-tuning.  &lt;/p&gt;
&lt;h3 id="tuning-process"&gt;Tuning process&lt;/h3&gt;
&lt;p&gt;Many hyperparams to tune, mark importance by colors (red &amp;gt; yellow &amp;gt; purple):&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;How to select set of values to explore ?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do &lt;strong&gt;NOT&lt;/strong&gt; use grid search (grid of n * n)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;— this was OK in pre-DL era.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;try random values.&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;reason: difficule to know which hyperparam is most important, by randomization, &lt;em&gt;can try out n&lt;/em&gt;n distinct values for each hyperparam.*&lt;br&gt;
In extreme case, one is &lt;code&gt;alpha&lt;/code&gt;, the other is &lt;code&gt;epislon&lt;/code&gt;.  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in grid search: only n distinct values of alpha are tried  &lt;/li&gt;
&lt;li&gt;in random choice: can have n*n distinct values of alpha  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image001.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coarse to fine&lt;/strong&gt; sample scheme: zoom in to smaller regions of hyperparam space and re-sample more densely.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image002.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="using-an-appropriate-scale-to-pick-hyperparameters"&gt;Using an appropriate scale to pick hyperparameters&lt;/h3&gt;
&lt;p&gt;"Sampling at random", but at &lt;em&gt;appropriate scale, not uniformly.&lt;/em&gt;&lt;br&gt;
example: choice of alpha in [0.001, 1]&lt;br&gt;
→ &lt;em&gt;sample uniformly at log scale&lt;/em&gt; is more resonable: equal resources are used to search at each scale.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image003.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;implementation:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;r = -4 * np.random.rand()  # -4 &amp;lt;= r &amp;lt;= 0, uniformly at randome  &lt;/span&gt;
&lt;span class="code-line"&gt;alpha = np.exp(10, r) # 10e-4 &amp;lt;= alpha &amp;lt;= 1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;sampling beta for exp-weighted-avg: &lt;/em&gt;sample in the range of [0.9, 0.999]&lt;br&gt;
→ convert to sampling 1-beta, which is in range [0.0001, 0.1]  &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="hyperparameters-tuning-in-practice-pandas-vs-caviar"&gt;Hyperparameters tuning in practice: Pandas vs. Caviar&lt;/h3&gt;
&lt;p&gt;Tricks on how to &lt;em&gt;organize&lt;/em&gt; hyper-param-tuning process.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;re-test hyperparams occasionally:&lt;/em&gt; intuitions get stale, re-evaluate hyperparams every several months.  &lt;/p&gt;
&lt;p&gt;Two major schools of training  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Panda approach&lt;/strong&gt;: &lt;em&gt;babysitting one model&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Huge dataset, limited computing resources, can only train one model → babysit the model as it's training. Watch learning curve, try changing hyparams once a day.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image004.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Caviar approach&lt;/strong&gt;: &lt;em&gt;train many models in parallel&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Have enough computation power.&lt;br&gt;
  Different model/hyperparams being trained at the same time in parallel, pick the best one.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="batch-normalization_1"&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Batch normalization&lt;/strong&gt;:&lt;br&gt;
(in some cases) &lt;em&gt;makes NN much more robust, and DNN much easier to train.&lt;/em&gt; &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="normalizing-activations-in-a-network"&gt;Normalizing activations in a network&lt;/h3&gt;
&lt;p&gt;In pre-DL: normalize inputs to speedup learning. "make contours round"&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image007.png"&gt;&lt;br&gt;
In NN: normalize the activation &lt;code&gt;a[l-1]&lt;/code&gt; from previous layer could help (in practice, usually normalize &lt;code&gt;z[l-1]&lt;/code&gt;.)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image009.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BatchNorm algo&lt;/strong&gt;:&lt;br&gt;
intermediate values at each layer: &lt;code&gt;z[l]&lt;/code&gt;&lt;br&gt;
→ compute mean &amp;amp; variance&lt;br&gt;
⇒ get normalized &lt;code&gt;z[l]_normed&lt;/code&gt;. (mean=0, std=1)&lt;br&gt;
→ &lt;strong&gt;trasform&lt;/strong&gt; &lt;code&gt;z[l]_normed&lt;/code&gt; to &lt;code&gt;z_tilde[l]&lt;/code&gt; (mean=&lt;code&gt;beta&lt;/code&gt;, std=&lt;code&gt;gamma&lt;/code&gt;, &lt;em&gt;beta and gamma are learnable params&lt;/em&gt;),&lt;br&gt;
reason: for hidden units, want to move/stretch the support of hidden inputs, so as to profit from non-linearity of activation function.  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image011.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="fitting-batch-norm-into-a-neural-network"&gt;Fitting Batch Norm into a neural network&lt;/h3&gt;
&lt;p&gt;Add batchnorm to NN: replace z[l] to z_tilde[l] at each layer before activation g[l].&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image012.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Extra params to learn: &lt;code&gt;gamma[l]&lt;/code&gt; and &lt;code&gt;beta[l]&lt;/code&gt; at each layer.  &lt;/p&gt;
&lt;p&gt;In practice: no need to implement all details of BN, use DL framework.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No bias term (b) in BN&lt;/strong&gt;:&lt;br&gt;
z[l] = W[l] * a[l-1] + b[l]&lt;br&gt;
but z[l] will be centered anyway → &lt;code&gt;b[l]&lt;/code&gt; is not useful.&lt;br&gt;
→ &lt;code&gt;b[l]&lt;/code&gt; is replaced by &lt;code&gt;beta[l]&lt;/code&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image014.png"&gt;&lt;br&gt;
Dimension of beta[l], gamma[l]: the same as b[l] ( = n[l] * 1).  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="why-does-batch-norm-work"&gt;Why does Batch Norm work?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;intuition 1&lt;/strong&gt;: similar to normalizing input ("make contours round")  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;intuition 2&lt;/strong&gt;: weights in deeper layers are more robust to changes in ealier layer weights.&lt;br&gt;
i.e. Robost to &lt;em&gt;data distribution changing&lt;/em&gt;. ("&lt;strong&gt;covariant shift&lt;/strong&gt;")  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;motivating example:&lt;br&gt;
cat-classification, &lt;em&gt;trained all with black cats, but applied to colored cats&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image016.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;For NN, consider the 3rd layer's units:&lt;br&gt;
input features: a[2],&lt;br&gt;
if cover the first 2 layers, this is a NN to map from a[2] to y_hat&lt;br&gt;
⇒ but when weights w[2],b[2] are updated in GD, &lt;em&gt;a[2]'s distribution is always changing&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image017.png"&gt;&lt;br&gt;
&lt;strong&gt;intuition&lt;/strong&gt;: With BN, a[2] are ensured to &lt;em&gt;always have the same mean/variance&lt;/em&gt;&lt;br&gt;
  → "data distribution" is unchanged → later layers can learn more easily, independent of previous layer's weights' change.  &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;intuition 2&lt;/strong&gt;: BN as &lt;em&gt;regularization&lt;/em&gt;&lt;br&gt;
  each minibatch is scaled by mean/var of just that minibatch&lt;br&gt;
  → &lt;em&gt;add noise&lt;/em&gt; to the transformation from z[l] to z_tilde[l].&lt;br&gt;
  ⇒ similar to dropout, add noise to each layer's activations.&lt;br&gt;
  therefore BN have (&lt;em&gt;slight&lt;/em&gt;) regularization effect (thie regularization effect gets smaller as minibatch size grows).&lt;br&gt;
  (This is an unintended side effect.)  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="batch-norm-at-test-time"&gt;Batch Norm at test time&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;At training time&lt;/em&gt;, z[l] is standarlized &lt;em&gt;over each minibatch&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image018.png"&gt;&lt;br&gt;
⇒ But at test time needs to treat examples one at a time.&lt;br&gt;
→ estimate the value of meu/sigma2&lt;br&gt;
⇒ using&lt;em&gt; exp-weighted-avg&lt;/em&gt; estimator across minibatchs (with beta close to 1 → ~running average).&lt;br&gt;
at test time, just use the latest value of this exp-weighted-avg estimation as meu/sigma2.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image019.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="multiclass-classification_1"&gt;Multiclass classification&lt;/h2&gt;
&lt;h3 id="softmax-regression"&gt;Softmax Regression&lt;/h3&gt;
&lt;p&gt;So far: only binary classification&lt;br&gt;
generalize logistic regression to &amp;gt;2 classes ⇒ &lt;em&gt;softmax regression&lt;/em&gt;.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;C&lt;/code&gt; = #classes, = #units in output layer&lt;br&gt;
each component in y_hat is probability of one class, y_hat is normalized.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image021.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;softmax layer&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;z[L] = W[L] * a[l-1] + b[L] — same as before  &lt;/li&gt;
&lt;li&gt;a[L] = y_hat = g(z[L])  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;activation function: &lt;em&gt;softmax&lt;/em&gt;&lt;br&gt;
take exp(z[L]) --element-wise, and then normalize:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image023.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;The softmax activation function is unusual because it takes a &lt;em&gt;vector&lt;/em&gt; instead of scalar.&lt;br&gt;
Softmax is generalization of logistic regression: decision boundary of a &lt;em&gt;single-layer (no hidden layer)&lt;/em&gt; softmax is also &lt;em&gt;linear&lt;/em&gt;:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image025.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="training-a-softmax-classifier"&gt;Training a softmax classifier&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;understanding softmax&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"softmax" is in contrast to "&lt;em&gt;hardmax&lt;/em&gt;":  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;hardmax[i]= 1 if z_i=max else 0.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image026.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When C = 2, softmax reduces to logistic regression.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;softmax(C=2) = [logistic-reg(), 1-logistic-reg]  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;loss function&lt;/strong&gt;&lt;br&gt;
recall: loss function in logistic regression&lt;br&gt;
L(y, y_hat) = -1 * sum( y_i * log yhat_i + (1-y_i) * log(yhat_i) )&lt;br&gt;
→ want y_hat big when y_i=1, small when y_i=0  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training label y: one-hot encoding.  &lt;/li&gt;
&lt;li&gt;prediciton y_hat: probability vector  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;loss function:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if y_k=1, want to make yhat_k big  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image027.png"&gt;&lt;br&gt;
→ max-likelihood estimation.  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GD with softmax&lt;/strong&gt;&lt;br&gt;
fwdprop:&lt;br&gt;
Z[L] --(softmax)--&amp;gt; a[L]=y_hat → L(y_hat, y)&lt;br&gt;
backprop:&lt;br&gt;
&lt;em&gt;dZ[L] = y_hat - y&lt;/em&gt; &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="introduction-to-programming-frameworks_1"&gt;Introduction to programming frameworks&lt;/h2&gt;
&lt;h3 id="deep-learning-frameworks"&gt;Deep learning frameworks&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image029.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h3 id="tensorflow"&gt;TensorFlow&lt;/h3&gt;
&lt;p&gt;motivating problem: minimize cost function &lt;code&gt;J(w) = (w-5)^2&lt;/code&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;import tensorflow as tf&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;define &lt;em&gt;parameter&lt;/em&gt; to optimize:&lt;br&gt;
&lt;code&gt;w = tf.Variable(0, dtype=tf.float32)&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;define cost function:&lt;br&gt;
&lt;code&gt;cost = tf.add(tf.add(w**2), tf.multiply(-10., w)), 25)  # w^2 - 10w + 25  
  # also possible to use tf-reloaded operators:  
  cost = w**2 - 10 * w + 25&lt;/code&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tells tf to minimize the cost with GD optimizer:  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)&lt;/code&gt;&lt;br&gt;
till now the &lt;em&gt;computation graph&lt;/em&gt; is defined → backward derivatives are auto-computed.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk3/pasted_image030.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start the training  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quite idiomatic process:&lt;br&gt;
initialize vars → create session → run operations in session  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;init = tf.global_variables_initializer()  &lt;/span&gt;
&lt;span class="code-line"&gt;session = tf.Session()  &lt;/span&gt;
&lt;span class="code-line"&gt;session.run(init)  &lt;/span&gt;
&lt;span class="code-line"&gt;session.run(train) # run 1 iteration of training&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;alternative format:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;with tf.Session() as session:  &lt;/span&gt;
&lt;span class="code-line"&gt;  session.run(init)  &lt;/span&gt;
&lt;span class="code-line"&gt;  session.run(train)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To inspect the value of a parameter: &lt;code&gt;print(session.run(w))&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run 1000 iters of GD:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;for i in range(1000):  
  session.run(train)&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Let loss function depends on training data:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;define training data as &lt;em&gt;placer holder&lt;/em&gt;.&lt;br&gt;
  a placerholder is a variable whose value will be assigned later.&lt;br&gt;
&lt;code&gt;x = tf.placeholder(tf.float32, [3,1])  
    cost = x[0][0] * w**2 + x[1][0] * w + x[2][0]&lt;/code&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;feed actual data value to placerholder: use &lt;em&gt;feed_dict&lt;/em&gt; in session.run()  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;data = np.array([1., -10., 25.]).reshape((3,1)  
session.run(train, feed_dict={x: data})&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="deep learning"></category></entry><entry><title>[Improving Deep Neural Networks] week1. Practical aspects of Deep Learning</title><link href="http://x-wei.github.io/Ng_DLMooc_c2wk1.html" rel="alternate"></link><published>2017-10-21T00:00:00+02:00</published><updated>2017-10-21T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-10-21:/Ng_DLMooc_c2wk1.html</id><summary type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h1 id="setting-up-your-maching-learning-application"&gt;Setting up your Maching Learning Application&lt;/h1&gt;
&lt;h2 id="train-dev-test-sets"&gt;Train / Dev / Test sets&lt;/h2&gt;
&lt;p&gt;Applied ML: highly iterative process. &lt;em&gt;idea-code-exp loop&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;splitting data&lt;/strong&gt;&lt;br&gt;
splitting data in order to speed up the idea-code-exp loop:  &lt;br&gt;
*training set / dev(hold-out/cross-validataion) set / test set *  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;split ratio&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with 100~10000 examples: 70/30 or 60/20/20 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]  &lt;/p&gt;
&lt;h1 id="setting-up-your-maching-learning-application"&gt;Setting up your Maching Learning Application&lt;/h1&gt;
&lt;h2 id="train-dev-test-sets"&gt;Train / Dev / Test sets&lt;/h2&gt;
&lt;p&gt;Applied ML: highly iterative process. &lt;em&gt;idea-code-exp loop&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;splitting data&lt;/strong&gt;&lt;br&gt;
splitting data in order to speed up the idea-code-exp loop:  &lt;br&gt;
*training set / dev(hold-out/cross-validataion) set / test set *  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;split ratio&lt;/strong&gt;:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with 100~10000 examples: 70/30 or 60/20/20  &lt;/li&gt;
&lt;li&gt;with ~1M examples: dev/test set can have much smaller ratio, e.g. 98/1/1  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;mismatched train/test distribution&lt;/strong&gt;&lt;br&gt;
training and test set don't come from the same dist.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rule of thumb: &lt;strong&gt;make sure&lt;/strong&gt; &lt;strong&gt;dev and test set come from the same distribution.&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;might be OK to only have dev set. — thought  in this case no longer have unbiased estimate of performance.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="bias-variance"&gt;Bias / Variance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;high variance: &lt;em&gt;overfitting&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;high bias: &lt;em&gt;underfitting&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image001.png"&gt;&lt;br&gt;
high base and high variance (worse case): high bias in some region and high variance elsewhere&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image004.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;how to estimate bias&amp;amp;variance&lt;/strong&gt;&lt;br&gt;
→ &lt;em&gt;look at train and dev set error&lt;/em&gt; &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high variance: Err_train &amp;lt;&amp;lt; Err_dev — not generalize well  &lt;/li&gt;
&lt;li&gt;high bias: Err_train ~= Err_dev, and Err_train &amp;gt;&amp;gt; Err_human — not learning well even on training set  &lt;/li&gt;
&lt;li&gt;high bias &lt;em&gt;and&lt;/em&gt; high variance (worse): Err_train &amp;gt;&amp;gt; Err_human, Err_train &amp;gt;&amp;gt; Err_dev  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image003.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="basic-recipe-for-machine-learning"&gt;Basic Recipe for Machine Learning&lt;/h2&gt;
&lt;p&gt;basic recipe:  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;does algo have high bias ? (look at Err_train)  &lt;ul&gt;
&lt;li&gt;if yes → try bigger nn / other architecture  &lt;/li&gt;
&lt;li&gt;until having low bias (fit well training set)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;high variance ? (look at Err_dev)  &lt;ul&gt;
&lt;li&gt;if yes → get more data / regularization / other architecture  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image005.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bias-variance tradeoff&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in pre-DL era, bias and variance are tradeoff (decrease one → increase the other)  &lt;/li&gt;
&lt;li&gt;in DL era: &lt;em&gt;if getting bigger nn and more data always possible&lt;/em&gt;, &lt;em&gt;both can be reduced&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(when well regularized,) &lt;em&gt;"training a bigger NN almost never hurts."&lt;/em&gt; &lt;/p&gt;
&lt;h1 id="regularizing-your-neural-network_1"&gt;Regularizing your neural network&lt;/h1&gt;
&lt;p&gt;2 ways to reduce variance: regularize, or get more data.  &lt;/p&gt;
&lt;h2 id="regularization"&gt;Regularization&lt;/h2&gt;
&lt;h3 id="example-logistic-regression"&gt;example: logistic regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;params: &lt;code&gt;w&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;cost function &lt;code&gt;J(w,b) = 1/m * L(yhat_i, yi)&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ add one more term to cost &lt;code&gt;J&lt;/code&gt;: adding L2 norm of &lt;code&gt;w&lt;/code&gt;(&lt;em&gt;L2 regularization&lt;/em&gt;)&lt;br&gt;
(lambda: regularization param)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image006.png"&gt;&lt;br&gt;
&lt;em&gt;just omit regularizing b&lt;/em&gt;: &lt;code&gt;w&lt;/code&gt; is high dim, &lt;code&gt;b&lt;/code&gt; is single number.  &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;L1 regularization: L1 norm of &lt;code&gt;w&lt;/code&gt; → &lt;em&gt;w will be sparse → &lt;/em&gt;compressing the model (just a little bit)&lt;br&gt;
⇒ &lt;em&gt;L2-reg is much often used&lt;/em&gt; &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="example-nn"&gt;example: NN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;params: &lt;code&gt;w[l]&lt;/code&gt;, &lt;code&gt;b[l]&lt;/code&gt; for l = 1..N  &lt;/li&gt;
&lt;li&gt;sum of the norms of each &lt;code&gt;w[l]&lt;/code&gt; matrix.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ &lt;em&gt;"Frobenius norm"&lt;/em&gt; of a matrix: sum (each element squared)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image007.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;: adding one more term from backprop&lt;br&gt;
d(1/2m * ||w||) = lambda / m &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;L2-reg also called "&lt;strong&gt;weight decay&lt;/strong&gt;": &lt;br&gt;
with L2-reg, looks as if doing the backprop updating, with w being w' = (1-alpha*lambda/m) * w (decayed w)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image009.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="why-regularization-reduces-overfitting_1"&gt;Why regularization reduces overfitting?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;why imposing small params prevents overfitting?&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;intuition 1&lt;/strong&gt;&lt;br&gt;
→ heavy regularization &lt;br&gt;
→ weight ~= 0 &lt;br&gt;
→ many hidden units' impact are "&lt;em&gt;zeroed-out"&lt;/em&gt;&lt;br&gt;
→ simpler NN&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image010.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;intuition 2&lt;/strong&gt;&lt;br&gt;
e.g. activation g(z) = tanh(z)&lt;br&gt;
small z → g(z) ~= linear, &lt;br&gt;
large z → g(z) flattend&lt;br&gt;
⇒ large lambda → small w &lt;br&gt;
→ z small &lt;br&gt;
→ every layer ~linear&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image011.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="dropout-regularization"&gt;Dropout Regularization&lt;/h2&gt;
&lt;p&gt;another powerful method of regularization&lt;br&gt;
&lt;strong&gt;dropout&lt;/strong&gt;: &lt;em&gt;For each training example&lt;/em&gt;, in each layer, &lt;em&gt;eliminate randomly some of its output values.&lt;/em&gt; &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image012.png"&gt; &lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image013.png"&gt; &lt;/img&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="dropout-implementation-inverted-dropout"&gt;dropout implementation: "inverted dropout"&lt;/h3&gt;
&lt;p&gt;example: dropout of &lt;em&gt;layer 3&lt;/em&gt;, keep_prob = 0.8 (prob of keeping hidden unit)&lt;br&gt;
→ generate a rand matrix of shape the same shape as activation &lt;code&gt;a[3]&lt;/code&gt; &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;d3 = np.random.rand(a3.shape[0], a3.shape[1]) &amp;lt; keep_prob  # d3 is bool matrix  &lt;/span&gt;
&lt;span class="code-line"&gt;a3 = np.multiply(a3, d3)  # element-wise multiply  &lt;/span&gt;
&lt;span class="code-line"&gt;a3 /= keep_prob  # ****"inverted dropout"****&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;"inverted dropout": why a3 /= keep_prob (i.e. make a3 larger)?&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;let's say layer 3 has 50 units, keep_prob = 0.8   &lt;/li&gt;
&lt;li&gt;→ ~10 units shut off  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;z[4] = w[4] * a[3] + b[4]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ a[3] have random 20% units shut off &lt;br&gt;
→ &lt;em&gt;w[4]&lt;/em&gt;a[3] will be reduced by 20% in expection*  &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inverted dropout: a3 /= keep_prob, to &lt;em&gt;keep expected value a3 remains unchanged&lt;/em&gt;.  &lt;/li&gt;
&lt;li&gt;(No dropout at test time) → inverted dropout &lt;em&gt;avoids scaling problem at test time&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;making predictions at test time&lt;/strong&gt;&lt;br&gt;
NOT use dropout at test time ⇒ don't want output to be random at test time...  &lt;/br&gt;&lt;/p&gt;
&lt;h2 id="understanding-dropout_1"&gt;Understanding Dropout&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;why randomly shut units prevents overfitting ?&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition: can't rely on any one input feature → have to spread out weight&lt;/strong&gt;&lt;br&gt;
spread weights ~→ smaller L2 norm (shrink weights)&lt;br&gt;
Can be formally proven: dropout is equal to &lt;em&gt;adaptive&lt;/em&gt; L2-reg, with penalty of different weight being different.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;For one hidden unit: any of it input features (from prev layer) can go out at random&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image014.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vary keep_prob for different layer   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ &lt;em&gt;smaller keep_prob for larger layer&lt;/em&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually no dropout (or very small dropout) for input layer...  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image016.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Downside of dropout&lt;/strong&gt;&lt;br&gt;
cost function J &lt;em&gt;no longer well-defined &lt;/em&gt;(because output yhat is random)&lt;br&gt;
→ can no longer plot cost-iter curve&lt;br&gt;
→ turn off dropout before plotting the curve  &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="other-regularization-methods"&gt;Other regularization methods&lt;/h2&gt;
&lt;h3 id="data-augmentation"&gt;data augmentation&lt;/h3&gt;
&lt;p&gt;adding more training example is expensive &lt;br&gt;
→ vary existing training data (e.g. flipping/rand-distortions of training image for cats)  &lt;/br&gt;&lt;/p&gt;
&lt;h3 id="early-stopping"&gt;early stopping&lt;/h3&gt;
&lt;p&gt;plot Err or J to #iterations &lt;em&gt;for both train and dev set.&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image017.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Downside of early-stopping&lt;/strong&gt;: &lt;br&gt;
&lt;em&gt;optimization cost J&lt;/em&gt; and &lt;em&gt;not overfitting&lt;/em&gt; should be separated task ("Orthogonalization")&lt;br&gt;
→ early-stopping couples the two jobs.  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;upside of early stopping: no need to try different values of regularization param (lambda) → finds "mid-size w" at once.  &lt;/p&gt;
&lt;h1 id="setting-up-your-optimization-problem_2"&gt;Setting up your optimization problem&lt;/h1&gt;
&lt;p&gt;How to speed up training (i.e. optimize J)  &lt;/p&gt;
&lt;h2 id="normalizing-inputs"&gt;Normalizing inputs&lt;/h2&gt;
&lt;p&gt;normalize input:  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;substract mean   &lt;/li&gt;
&lt;li&gt;normalize variance  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image018.png"&gt;&lt;br&gt;
detail: in data splitting, &lt;em&gt;use the same meu/sigma to normalize test set !&lt;/em&gt; &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;why normalizing input ?&lt;/strong&gt;&lt;br&gt;
if features x1 x2 are on different scales → w1 and w2 not same scale&lt;br&gt;
J is more symmetric, easier to optimize&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image019.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id="vanishing-exploding-gradients"&gt;Vanishing / Exploding gradients&lt;/h3&gt;
&lt;p&gt;One problem in training very deep NN: vanishing/exploding gradients.  &lt;/p&gt;
&lt;p&gt;example: a very deep NN, each layer 2 units, linear activation g(z)=z, ignore bias b[l] = 0.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image021.png"&gt;&lt;br&gt;
linear activations → y is just a linear transformation of x&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image022.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assuming each w[l] = 1.5 * Identity_matrix ⇒ activations increase exponentially  &lt;/li&gt;
&lt;li&gt;assuming each w[l] = 0.5 * Id ⇒ activations decrease exponentially  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image023.png"&gt;&lt;br&gt;
yhat too large or too small → hard to train  &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;h2 id="weight-initialization-for-deep-networks_1"&gt;Weight Initialization for Deep Networks&lt;/h2&gt;
&lt;p&gt;A partial solution of vanishing/exploding gradient problem: &lt;em&gt;carefully initialize weights&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;single neuron example:&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image024.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;y = g(w*x), g = relu  &lt;/li&gt;
&lt;li&gt;n = # inputs for   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;z = w1&lt;em&gt;x1 + ... + wn&lt;/em&gt;xn, &lt;br&gt;
if wi are initzed randomly&lt;br&gt;
→ large ns ⇒ z will be large ! &lt;br&gt;
⇒ &lt;strong&gt;set var(wi) = 1/n&lt;/strong&gt; (2/n in practice) to keep z in similar scale for diffent #inputs&lt;br&gt;
initialization code: &lt;br&gt;
&lt;code&gt;w[l] = np.random.randn(shape[l]) * np.sqrt( 2 / n[l-1] )  # n[l-1] = #inputs for layer-l&lt;/code&gt; &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;other variants&lt;/strong&gt;&lt;br&gt;
when activation function g = tanh &lt;br&gt;
⇒ use var(wi) = 1/n ("&lt;strong&gt;Xavier initialization&lt;/strong&gt;")&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image025.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="numerical-approximation-of-gradients"&gt;Numerical approximation of gradients&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;checking the derivative computation&lt;/strong&gt;&lt;br&gt;
example: f(x) = x ^ 3&lt;br&gt;
→ &lt;em&gt;vary x by epsilon&lt;/em&gt; to approximate f'(x), &lt;em&gt;use 2-sided difference&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image026.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image027.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;error order = O(epsilon^2) for 2-sided difference&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image028.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="gradient-checking"&gt;Gradient checking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Verify&lt;/strong&gt; that your implementation is correct. — help finding out bugs in implementation early.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;concat all params into a big vector &lt;code&gt;theta&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;concat all dW[l] db[l] into big vector &lt;code&gt;d_theta&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;to &lt;strong&gt;check if d_theta is correct&lt;/strong&gt;: construct a &lt;code&gt;d_theta_approx&lt;/code&gt; vector  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image029.png"&gt;&lt;br&gt;
⇒ &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image030.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;How to check "approximate":&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c2wk1//pasted_image031.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="gradient-checking-implementation-notes"&gt;Gradient Checking Implementation Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dont' use checking in training: constructing d_theta_approx is slow  &lt;/li&gt;
&lt;li&gt;When check fails: look at components to try to find bug  &lt;/li&gt;
&lt;li&gt;Remember regularization: J contains reg term as well  &lt;/li&gt;
&lt;li&gt;Doesn't work with dropout: J not well defined (random variable), turn dropout off before checking.  &lt;/li&gt;
&lt;li&gt;Run check at random initialization (w,b~=0), then again after some training(w,b~&amp;gt;0)  &lt;/li&gt;
&lt;/ul&gt;</content><category term="deep learning"></category></entry><entry><title>[Neural Networks and Deep Learning] week4. Deep Neural Network</title><link href="http://x-wei.github.io/Ng_DLMooc_c1wk4.html" rel="alternate"></link><published>2017-09-28T00:00:00+02:00</published><updated>2017-09-28T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-09-28:/Ng_DLMooc_c1wk4.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="deep-l-layer-neural-network"&gt;Deep L-layer neural network&lt;/h2&gt;
&lt;p&gt;Layer counting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input layer is not counted as a layer, "layer 0"&lt;/li&gt;
&lt;li&gt;last layer (layer L, output layer) is counted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;p&gt;notation:
layer 0 = input layer
&lt;code&gt;L&lt;/code&gt; = number of layers
&lt;code&gt;n^[l]&lt;/code&gt; = size of layer l
&lt;code&gt;a^[l]&lt;/code&gt; = activation of layer l = &lt;code&gt;g[l]( z[l …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="deep-l-layer-neural-network"&gt;Deep L-layer neural network&lt;/h2&gt;
&lt;p&gt;Layer counting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input layer is not counted as a layer, "layer 0"&lt;/li&gt;
&lt;li&gt;last layer (layer L, output layer) is counted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;p&gt;notation:
layer 0 = input layer
&lt;code&gt;L&lt;/code&gt; = number of layers
&lt;code&gt;n^[l]&lt;/code&gt; = size of layer l
&lt;code&gt;a^[l]&lt;/code&gt; = activation of layer l = &lt;code&gt;g[l]( z[l] )&lt;/code&gt; → a[L] = yhat, a[0] = x&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image002.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="forward-propagation-in-a-deep-network"&gt;Forward Propagation in a Deep Network&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image003.png"/&gt;&lt;/p&gt;
&lt;p&gt;⇒ general rule:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image004.png"&gt;&lt;br&gt;
vectorization over all training examples: 
Z = [z(1),...,z(m)] one column per example ⇒ &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;A[0] = X&lt;/span&gt;
&lt;span class="code-line"&gt;for l = 1..L:&lt;/span&gt;
&lt;span class="code-line"&gt;  Z[l] = W[l]A[l-1] + b[l]&lt;/span&gt;
&lt;span class="code-line"&gt;  A[l] = g[l]( Z[l] )&lt;/span&gt;
&lt;span class="code-line"&gt;Yhat = A[L]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="getting-your-matrix-dimensions-right"&gt;Getting your matrix dimensions right&lt;/h2&gt;
&lt;p&gt;Debug: walk through matrix dimensions of NN, &lt;code&gt;W[l]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Single training example dimension:&lt;br&gt;
&lt;code&gt;a[l-1].shape = (n[l-1], 1)&lt;/code&gt;&lt;br&gt;
&lt;code&gt;z[l].shape = (n[l], 1)&lt;/code&gt;&lt;br&gt;
⇒ &lt;code&gt;z[l] = W[l] * a[l-1] + b[l], shape = (n[l],1)&lt;/code&gt;&lt;br&gt;
⇒ &lt;strong&gt;W[l].shape = (n[l], n[l-1]), b[l].shape = (n[l],1)&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image006.png"/&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Vectorized (m examples) dimension:&lt;br&gt;
Z = [z(1),...,z(m)] &lt;em&gt;stacking columns&lt;/em&gt;.&lt;br&gt;
&lt;code&gt;Z[l].shape = (n[l], m)&lt;/code&gt;&lt;br&gt;
Z[l] = W[l] * A[l-1] + b[l]&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image007.png"&gt;&lt;br&gt;
&lt;strong&gt;Z[l].shape = A[l].shape = (n[l], m)&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="why-deep-representations"&gt;Why deep representations?&lt;/h2&gt;
&lt;p&gt;intuition: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image010.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;as layers grow: simple to complex representation / low to high level of abstraction.&lt;/p&gt;
&lt;p&gt;Circuit theory: small deep NN is better than big shallow NN.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image011.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: representation of a XOR.join(x1..xn) function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using deep NN ⇒ build an XOR binary tree&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image012.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using shallow NN: one single layer → enumerate all 2^n configurations of inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image013.png"/&gt;&lt;/p&gt;
&lt;h2 id="building-blocks-of-deep-neural-networks"&gt;Building blocks of deep neural networks&lt;/h2&gt;
&lt;p&gt;Fwdprop and backprop, for layer l.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fwdprop: &lt;/strong&gt;from a[l-1] to a[l]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;note: &lt;em&gt;cache z[l] for backprop.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Backprop: &lt;/strong&gt;from da[l] to da[l-1], dw[l] and db[l]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image014.png"/&gt;&lt;/p&gt;
&lt;p&gt;Once the fwd and back functions are implemented, put layers together:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image015.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="forward-and-backward-propagation"&gt;Forward and Backward Propagation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Fwd prop&lt;/strong&gt;&lt;br&gt;
input = a[l-1], output = a[l], cache = z[l]  &lt;/br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;Z[l] = W[l] * A[l-1] + b[l]&lt;/span&gt;
&lt;span class="code-line"&gt;Z[l] = g[l]( Z[l] )&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Back prop&lt;/strong&gt;
input = da[l], output = da[l-1], dW[1], db[l]&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image016.png"&gt;&lt;br&gt;
note: &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;remember &lt;/em&gt;&lt;code&gt;da = dL/da&lt;/code&gt;&lt;em&gt;, so here &lt;/em&gt;&lt;code&gt;da&lt;/code&gt;&lt;em&gt;~='1/da' mathematically.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;derivate of matrix multiplication = transposed matrix derivative: (A*B)' = B^T' * A^T'&lt;/li&gt;
&lt;li&gt;&lt;em&gt;initial paule&lt;/em&gt; of backprop: da[L] = dL/dyhat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image018.png"&gt;&lt;br&gt;
Vectorized version:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image017.png"/&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;h2 id="parameters-vs-hyperparameters"&gt;Parameters vs Hyperparameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;parameters: W[l] and b[l] → trained from data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hyperparams&lt;/strong&gt;: &lt;ul&gt;
&lt;li&gt;alpha (learning_rate), number of iterations, L, n[l] size of each layer, g[l] at each layer...&lt;/li&gt;
&lt;li&gt;momentum, minibatch, regularization...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ finally decides what params will be.&lt;/p&gt;
&lt;p&gt;empirical: try out different hyperparams.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image019.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="what-does-this-have-to-do-with-the-brain"&gt;What does this have to do with the brain?&lt;/h2&gt;
&lt;p&gt;logistic regression unit ~~~&amp;gt; neuron in brain&lt;/p&gt;
&lt;h2 id="assignment-implementing-a-l-layer-nn"&gt;assignment: implementing a L-layer NN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;params initialization:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;note: different signature for &lt;code&gt;np.random.randn&lt;/code&gt; and &lt;code&gt;np.zeros&lt;/code&gt;:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;W = np.random.randn(d0, d1) * 0.01&lt;/span&gt;
&lt;span class="code-line"&gt;b = np.zeros((d0, d1)) # Needs putting dims in a tuple!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;function activation:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;np.maximum&lt;/code&gt; is element-wise comparison, whereas &lt;code&gt;np.max&lt;/code&gt; will apply on certain axis.
so &lt;code&gt;ReLU(x) = np.maximum(0, x)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fwd prop:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image023.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cost:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image022.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;backprop formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image020.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initial paulse of backprop dA[L]: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk4/pasted_image021.png"&gt;&lt;br&gt;
&lt;code&gt;dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))&lt;/code&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Neural Networks and Deep Learning] week3. Shallow Neural Network</title><link href="http://x-wei.github.io/Ng_DLMooc_c1wk3.html" rel="alternate"></link><published>2017-09-19T00:00:00+02:00</published><updated>2017-09-19T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-09-19:/Ng_DLMooc_c1wk3.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="neural-networks-overview"&gt;Neural Networks Overview&lt;/h2&gt;
&lt;p&gt;new notation: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superscript &lt;code&gt;[i]&lt;/code&gt; for quantities in layer i. (compared to superscript &lt;code&gt;(i)&lt;/code&gt; for ith training example).&lt;/li&gt;
&lt;li&gt;subscript &lt;code&gt;i&lt;/code&gt; for ith unit in a layer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="neural-network-representation"&gt;Neural Network Representation&lt;/h2&gt;
&lt;p&gt;notation: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a^[i]&lt;/code&gt;: activation at layer i.&lt;/li&gt;
&lt;li&gt;input layer: x, layer 0.&lt;/li&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;output layer: prediction (yhat …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="neural-networks-overview"&gt;Neural Networks Overview&lt;/h2&gt;
&lt;p&gt;new notation: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superscript &lt;code&gt;[i]&lt;/code&gt; for quantities in layer i. (compared to superscript &lt;code&gt;(i)&lt;/code&gt; for ith training example).&lt;/li&gt;
&lt;li&gt;subscript &lt;code&gt;i&lt;/code&gt; for ith unit in a layer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="neural-network-representation"&gt;Neural Network Representation&lt;/h2&gt;
&lt;p&gt;notation: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a^[i]&lt;/code&gt;: activation at layer i.&lt;/li&gt;
&lt;li&gt;input layer: x, layer 0.&lt;/li&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;output layer: prediction (yhat)&lt;/li&gt;
&lt;li&gt;don't count input layer as a layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;a 2 layer NN:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="computing-a-neural-networks-output"&gt;Computing a Neural Network's Output&lt;/h2&gt;
&lt;p&gt;each node in NN: 2 step computation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;z = wx + b&lt;/li&gt;
&lt;li&gt;a = sigmoid(z)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image002.png"&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image003.png"/&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;z^[1]&lt;/code&gt; = stacking &lt;code&gt;z[1]_i&lt;/code&gt;s vertically
&lt;code&gt;a^[1]&lt;/code&gt; = sigmoid(&lt;code&gt;z^[1]&lt;/code&gt;)
vectorize computing &lt;code&gt;z^[1]&lt;/code&gt;: W = &lt;em&gt;stacking rows of wi.T&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image005.png"&gt;&lt;br&gt;
W.shape = (4,3)
b.shape = (4,1)&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input at layer i = &lt;code&gt;a^[i-1]&lt;/code&gt; (&lt;code&gt;x = a[0]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;output of each layer: &lt;code&gt;a[i] = sigmoid(W[i] a^[i-1] + b[i])&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image006.png"/&gt;&lt;/p&gt;
&lt;h2 id="vectorizing-across-multiple-examples"&gt;Vectorizing across multiple examples&lt;/h2&gt;
&lt;p&gt;vectorize the computation acrosse m examples.
training examples: x^(1)...x^(m)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image007.png"&gt;&lt;br&gt;
computing all yhat(i) using forloop:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image008.png"/&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;X = &lt;em&gt;stacking columns of x(i)&lt;/em&gt;, &lt;code&gt;X = [x(1)...x(m)]&lt;/code&gt;
Z[1] = stacking columns of z&lt;a href="i"&gt;1&lt;/a&gt; = [z&lt;a href="1"&gt;1&lt;/a&gt;...z&lt;a href="m"&gt;1&lt;/a&gt;]
A = stacking columns of a(i)&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image010.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image009.png"&gt;&lt;br&gt;
&lt;strong&gt;horizontal index = training example&lt;/strong&gt; &lt;code&gt;^(i)&lt;/code&gt;&lt;br&gt;
&lt;strong&gt;vertical index = nodes in layer &lt;/strong&gt;&lt;code&gt;_i&lt;/code&gt;&lt;strong&gt;/ input feature&lt;/strong&gt;&lt;code&gt;x_i&lt;/code&gt;
⇒ &lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Z[1] = W[1] * X + b[1]&lt;/li&gt;
&lt;li&gt;A[1] = sigmoid(Z[1])&lt;/li&gt;
&lt;li&gt;Z[2] = W[2] * A[1] + b[2]&lt;/li&gt;
&lt;li&gt;A[2] = sigmoid(Z[2]) = Yhat&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="explanation-for-vectorized-implementation"&gt;Explanation for Vectorized Implementation&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image012.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recap&lt;/strong&gt;:
stacking columns of training examples &lt;code&gt;x(i)&lt;/code&gt; and activations &lt;code&gt;a[l](i)&lt;/code&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image013.png"&gt;&lt;br&gt;
⇒ &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image014.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="activation-functions"&gt;Activation functions&lt;/h2&gt;
&lt;p&gt;general case: &lt;code&gt;a = g(z)&lt;/code&gt;, where g() is a nonlinear function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sigmoid: &lt;code&gt;a = 1 / (1 + exp(-z))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image018.png"&gt;&lt;br&gt;
  a ∈ [0,1]&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tanh: &lt;code&gt;a = (exp(z) - exp(-z)) / (exp(z) + exp(-z))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image019.png"&gt;&lt;br&gt;
  a ∈ [-1, 1] — shifted sigmoid function 
  ⇒ data is &lt;em&gt;centered, learning for next layer easier&lt;/em&gt;
&lt;em&gt;almost always better than sigmoid&lt;/em&gt;, except for output layer (yhat = probability ∈[0,1])&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;downside of sigmoid and tanh: &lt;em&gt;slope very small when |z| is large&lt;/em&gt; — GD slow.
⇒ ReLU&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReLU &lt;code&gt;a = max(0, z)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;da/dz = 1 or 0
  NN learns faster because slope is constant when |z| large
  disadvantage: da/dz = 0 when z&amp;lt;0
  → leaky ReLU: small slope when z&amp;lt;0&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image020.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image021.png"/&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rules of thumb&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;output layer: sigmoid for binary classification (output probability), &lt;em&gt;otherwise never use sigmoid&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;hidden layer: use ReLU activation by default&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="why-do-you-need-non-linear-activation-functions"&gt;Why do you need non-linear activation functions?&lt;/h2&gt;
&lt;p&gt;use a linear activation function g(z) = z ?
⇒ &lt;code&gt;yhat&lt;/code&gt; will just be a &lt;em&gt;linear function&lt;/em&gt; of &lt;code&gt;x&lt;/code&gt;. &lt;code&gt;yhat = Wx+b&lt;/code&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image022.png"&gt;&lt;br&gt;
one single place when using linear activation: in output layer ( y∈R )when doing regression&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="derivatives-of-activation-functions"&gt;Derivatives of activation functions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;formulas for g'(z)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="g-sigmoid"&gt;g = sigmoid&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image023.png"&gt;&lt;br&gt;
⇒ &lt;code&gt;g'(z) = g(z) * (1 - g(z)) = a * (1-a)&lt;/code&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when z = +inf, g(z) = 1, g'(z) = 1*(1-1) = 0&lt;/li&gt;
&lt;li&gt;when z = -inf, g(z) = 0, g'(z) = 0&lt;/li&gt;
&lt;li&gt;when z = 0, g(z) = 0.5, g'(z) = 0.25&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="g-tanh"&gt;g = tanh&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image024.png"&gt;&lt;br&gt;
⇒ &lt;code&gt;g'(z) = 1 - tanh(z)^2 = 1 - a^2&lt;/code&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when z = +inf, tanh(z) = 1, g' = 0&lt;/li&gt;
&lt;li&gt;when z = -inf, tanh(z) = -1, g' = 0&lt;/li&gt;
&lt;li&gt;when z = 0, tanh(z) = 0, g' = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="g-relu-leaky-relu"&gt;g = ReLU / Leaky ReLU&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt;:
g(z) = max(0, z)
g' is &lt;em&gt;subgradient:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;g' = 0 when z&amp;lt;0&lt;/li&gt;
&lt;li&gt;g' = 1 when z&amp;gt;=0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Leaky ReLU&lt;/strong&gt;:
g(z) = max(0.01z, z)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;g' = 0.01 when z&amp;lt;0&lt;/li&gt;
&lt;li&gt;g' = 1 when z&amp;gt;=0&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="gradient-descent-for-neural-networks_1"&gt;Gradient descent for Neural Networks&lt;/h2&gt;
&lt;p&gt;NN with single hidden layer: n[0] = nx, n[1] = hidden layer size, n[2] = 1
params: w[1], b[1], w[2], b[2]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;w[1].shape=(n[1], n[0]), b[1].shape=(n[1], 1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w[2].shape=(n[2], n[1]) , b[2].shape=(n[2],1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;output: yhat = a[2]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;cost function J(w[1],b[1],w[2],b[2]) = 1/m * sum(L(yhat, y))&lt;/p&gt;
&lt;p&gt;Gradient descent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;random initialization&lt;/li&gt;
&lt;li&gt;repeat:&lt;ul&gt;
&lt;li&gt;compute dw[1], db[1], dw[2], db[2]&lt;/li&gt;
&lt;li&gt;w[1] := w[1] - alpha*dw[1], ...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fwd prop:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image026.png"&gt;&lt;br&gt;
general formular for &lt;code&gt;l&lt;/code&gt;th layer:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image034.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Bck prop: 
computing derivatives &lt;code&gt;dw&lt;/code&gt;, &lt;code&gt;db&lt;/code&gt;
note: use &lt;code&gt;keepdims = True&lt;/code&gt; or  &lt;code&gt;.rehape()&lt;/code&gt; to avoid rank-1 arraies.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image027.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="backpropagation-intuition-optional"&gt;Backpropagation intuition (optional)&lt;/h2&gt;
&lt;p&gt;Derive the formulas using computation graph + chain rule.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image028.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;gradient for a single example &lt;code&gt;x=x(i), y=y(i)&lt;/code&gt;:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image029.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;vectorized implementation for i=1,..,m:
&lt;strong&gt;stacking columns&lt;/strong&gt;:&lt;code&gt;X = [x(1),..,x(m)]&lt;/code&gt;, &lt;code&gt;Z = [z(1)...z(m)]&lt;/code&gt;, &lt;code&gt;Y = [y(1)..y(m)]&lt;/code&gt;, 
→ &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image031.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="random-initialization"&gt;Random Initialization&lt;/h2&gt;
&lt;p&gt;Unlike logistic regression, needs init params randomly.&lt;/p&gt;
&lt;p&gt;If we init all &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; to zeros: all activations &lt;code&gt;a_i&lt;/code&gt; and &lt;code&gt;a_j&lt;/code&gt; will be equal → &lt;code&gt;dz_i = dz_j&lt;/code&gt; → &lt;em&gt;all hidden units completely identical&lt;/em&gt;
⇒ needs to init all params &lt;em&gt;random, small&lt;/em&gt; number (small because we want have larger derivatives for sigmoid, which is at small values, to speed up gd).
when w is init to small rand, b don't need random init.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk3/pasted_image033.png"/&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Neural Networks and Deep Learning] week2. Neural Networks Basics</title><link href="http://x-wei.github.io/Ng_DLMooc_c1wk2.html" rel="alternate"></link><published>2017-09-13T00:00:00+02:00</published><updated>2017-09-13T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-09-13:/Ng_DLMooc_c1wk2.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;This week: &lt;strong&gt;logistic regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id="binary-classification-notation"&gt;Binary Classification &amp;amp; notation&lt;/h2&gt;
&lt;p&gt;ex. cat classifier from image
image pixels: 64x64x3 
⇒ unroll(flatten) to a feature vector &lt;code&gt;x&lt;/code&gt; dim=64x64x3=12288:=&lt;code&gt;n&lt;/code&gt; (input dimension)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;notation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superscript &lt;code&gt;(i)&lt;/code&gt; for ith example, e.g. &lt;code&gt;x^(i)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;superscript &lt;code&gt;[l]&lt;/code&gt; for lth layer, e.g. &lt;code&gt;w^[l]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt;: number …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;This week: &lt;strong&gt;logistic regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id="binary-classification-notation"&gt;Binary Classification &amp;amp; notation&lt;/h2&gt;
&lt;p&gt;ex. cat classifier from image
image pixels: 64x64x3 
⇒ unroll(flatten) to a feature vector &lt;code&gt;x&lt;/code&gt; dim=64x64x3=12288:=&lt;code&gt;n&lt;/code&gt; (input dimension)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;notation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superscript &lt;code&gt;(i)&lt;/code&gt; for ith example, e.g. &lt;code&gt;x^(i)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;superscript &lt;code&gt;[l]&lt;/code&gt; for lth layer, e.g. &lt;code&gt;w^[l]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt;: number of data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_x&lt;/code&gt;: input dimension, &lt;code&gt;n_y&lt;/code&gt;: output dimension.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_h^[l]&lt;/code&gt;: number of hidden units for layer l.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;L&lt;/code&gt;: number of layers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: dim=(&lt;code&gt;n_x&lt;/code&gt;,&lt;code&gt;m&lt;/code&gt;), each &lt;em&gt;column&lt;/em&gt; is a training example x^(i).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: dim=(&lt;code&gt;1&lt;/code&gt;,&lt;code&gt;m&lt;/code&gt;), one single &lt;code&gt;row&lt;/code&gt; matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image.png"/&gt;&lt;/p&gt;
&lt;h1 id="logistic-regression-as-a-nueral-network_1"&gt;Logistic Regression as a Nueral Network&lt;/h1&gt;
&lt;h2 id="logistic-regression"&gt;Logistic Regression&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image009.png"&gt;&lt;br&gt;
dim(x) = n_x
parameters: w (dim=n_x) , b (dim=1)
(alternative notation: adding b to w → add x_0 = 1 to feature x. → will NOT use this notation here
keeping w and b separate make implementation easier )&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;linear regression: &lt;code&gt;y_hat = w^T*x + b&lt;/code&gt;
logistic regssion: &lt;code&gt;y_hat = sigmoid(w^T*x + b)&lt;/code&gt;
sigmoid function: S-shaped function 
&lt;code&gt;sigmoid(z) = 1 / ( 1 + e^-z)&lt;/code&gt;
z large → sigmoid(z) ~= 1
z small → sigmoid(z) ~= 0&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image001.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image025.png"/&gt;&lt;/p&gt;
&lt;h2 id="logistic-regression-cost-function"&gt;Logistic Regression Cost Function&lt;/h2&gt;
&lt;p&gt;To train model for best parameters (w, b), need to define loss function.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image010.png"&gt; &lt;br&gt;
y_hat: between (0,1)
training set: {(x^(i), y^(i)))), i = 1..m}
want: y_hat(i) ~= y(i)&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt; &lt;code&gt;L(y_hat, y)&lt;/code&gt;: on a &lt;em&gt;single&lt;/em&gt; training example (x, y)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;square error: &lt;code&gt;L(y_hat, y) = (y_hat - y)^2/2&lt;/code&gt; &lt;ul&gt;
&lt;li&gt;⇒ &lt;em&gt;not convex&lt;/em&gt;, GD not work well, uneasy to optimize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;loss function used in logistic regression: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;L(y_hat, y) = -[ylog(y_hat) + (1-y)log(1-y_hat)]&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convex w.r.t. w and b&lt;/li&gt;
&lt;li&gt;when y = 1, loss = -log(y_hat)  → want y_hat large → y_hat ~=1&lt;/li&gt;
&lt;li&gt;when y = 0, loss = -log(1-y_hat) → want y_hat small → y_hat ~=0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cost function&lt;/strong&gt; &lt;code&gt;J(w,b)&lt;/code&gt;: average on all training sets, only depends on parameters w, b&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient Descent&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image005.png"&gt;&lt;br&gt;
⇒ minimize &lt;code&gt;J(w,b)&lt;/code&gt; wrt. w and b&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;J(w,b)&lt;/code&gt; is convex ⇒ gradient descent&lt;/li&gt;
&lt;li&gt;Initialization: for logistic regression, any init works because of convexity of J, usually init as 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient descent: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt; = learning rate&lt;/li&gt;
&lt;li&gt;derivative &lt;code&gt;dJ(w)/dw&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;~= slope of function &lt;code&gt;J&lt;/code&gt; at point &lt;code&gt;w&lt;/code&gt; 
~= direction where &lt;code&gt;J&lt;/code&gt; &lt;em&gt;grows&lt;/em&gt; fastest at point &lt;code&gt;w&lt;/code&gt;
&lt;em&gt;denote this as '&lt;/em&gt;&lt;code&gt;dw&lt;/code&gt;&lt;em&gt;' in code&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;algo: 'take steepest descent'&lt;ul&gt;
&lt;li&gt;from an init value of w_0&lt;/li&gt;
&lt;li&gt;repeatedly update w until converge &lt;code&gt;w := w - alpha*dw&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image006.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image007.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;In the case of logistic regression, &amp;gt;1 params (&lt;code&gt;w&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;) to update:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Intuitions about derivatives: &lt;code&gt;f'(a)&lt;/code&gt; = slope of function &lt;code&gt;f&lt;/code&gt; at &lt;code&gt;a&lt;/code&gt; .&lt;/p&gt;
&lt;h2 id="computation-graph"&gt;Computation Graph&lt;/h2&gt;
&lt;p&gt;example: function &lt;code&gt;J(a,b,c) = 3(a+b*c)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Forward propagation&lt;/strong&gt;: compute J(a,b,c) value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;internal u := b*c&lt;/li&gt;
&lt;li&gt;internal v := a+u&lt;/li&gt;
&lt;li&gt;J = 3 * v&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image011.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backward propagation&lt;/strong&gt;: compute derivatives dJ/da, dJ/db, dJ/dc:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J = 3*v → compute dJ/dv&lt;/li&gt;
&lt;li&gt;v = a + u → compute dv/da, dv/du&lt;/li&gt;
&lt;li&gt;u = bc → compute du/db, du/dc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ chain rule&lt;em&gt;: dJ/da is multiplying the derivatives along the path from J back to a&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dJ/da = dJ/dv * dv/da&lt;/li&gt;
&lt;li&gt;dJ/db = dJ/dv * dv/du * du/db&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;dJ/dc = dJ/dv * dv/du * du/dc&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In code: &lt;em&gt;denote '&lt;/em&gt;&lt;code&gt;dvar&lt;/code&gt;&lt;em&gt;' as d(FinalOutput)/d(var) for simplicity. i.e. da = dJ/da, dv = dJ/dv, etc.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image014.png"/&gt;&lt;/p&gt;
&lt;h2 id="logistic-regression-gradient-descent-computation-graph"&gt;Logistic Regression Gradient Descent (&amp;amp;computation graph)&lt;/h2&gt;
&lt;p&gt;logistic regression loss(on a single training example x,y) L.
as computation graph:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;z = wx + b&lt;/li&gt;
&lt;li&gt;a := sigmoid(z) (=y_hat, 'logit'?)&lt;/li&gt;
&lt;li&gt;loss function L(a,y) = - [y(loga) + (1-y)log(1-a)]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image015.png"/&gt;&lt;/p&gt;
&lt;h2 id="gradient-descent-on-m-examples"&gt;Gradient Descent on m Examples&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;cost function&lt;/em&gt;, i.e. on all training sets.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image016.png"&gt;&lt;br&gt;
J(w,b) = avg{L(x,y), for all m examples}
→ by linearity  of derivative: dJ/dw = avg(dL/dw), just average dw^(i) over all indices i.&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;In implementation: use &lt;strong&gt;vectorization&lt;/strong&gt; as much as possible, get rid of for loops.&lt;/p&gt;
&lt;h1 id="python-and-vectorization_1"&gt;Python and Vectorization&lt;/h1&gt;
&lt;h2 id="vectorization"&gt;Vectorization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;avoid explicit for-loops whenever possible&lt;/em&gt;
e.g. z = w^T * x + b
in numpy:
&lt;code&gt;z = np.dot(w, x) + b&lt;/code&gt;
&lt;em&gt;~300 times faster than explicit for loop&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;more examples:
u = A*v matrix multiplication
→ &lt;code&gt;u =&lt;/code&gt; &lt;code&gt;np.dot(A, v)&lt;/code&gt;
note: &lt;code&gt;A * v&lt;/code&gt; would element-wise multiply
u = exp(v) element-wise operation: exponential/log/abs/...
→ &lt;code&gt;u = np.exp(v)&lt;/code&gt; &lt;code&gt;/ np.log(v) / np.abs(v) / v**2 / 1/v&lt;/code&gt;&lt;/p&gt;
&lt;h2 id="vectorizing-logistic-regression"&gt;Vectorizing Logistic Regression&lt;/h2&gt;
&lt;p&gt;implementation before: two for-loops( 1 for each training set, 1 for each feature vector).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training input &lt;code&gt;X = [x(1), ... , x(m)]&lt;/code&gt;, X.dim = (n_x, m)&lt;/li&gt;
&lt;li&gt;weight &lt;code&gt;w^T = [w_1, ... , w_nx]&lt;/code&gt;, w.dim = (n_x, 1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Fwd propagation&lt;/strong&gt;&lt;br&gt;
z(i) = w^T * x(i) + b, i = 1..m,
→ Z := [z(1)...z(m)] = w^T * X + [b...b], Z.dim = (1, m), stack horizentally
→ &lt;code&gt;Z = np.dot(w.T, X) + b&lt;/code&gt; (scalar b &lt;em&gt;auto broadcasted&lt;/em&gt; to a row vector)
a(i) = sigmoid( z(i) ) = y_hat(i)
→ A := [a(1)...a(m)] = sigmoid(Z), sigmoid is vectorized&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bkwd propagation: gradient computation&lt;/strong&gt;&lt;br&gt;
&lt;code&gt;dz(i)  = a(i) - y(i)&lt;/code&gt;
→ stack horizentally:
Y = [y(1)...y(m)]
dZ := [dz(1)...dz(m)] = A - Y
graidents:
&lt;code&gt;dw = sum( x(i) * dz(i) ) / m&lt;/code&gt;, dw.dim = (nx, 1)
&lt;code&gt;db = sum( dz(i) ) / m&lt;/code&gt;
→ 
db = 1/m * np.sum(dZ)
dw = 1/m * X*dz^T&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image017.png"/&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;efficient back-prop implementation:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image018.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="broadcasting-in-python"&gt;Broadcasting in Python&lt;/h2&gt;
&lt;p&gt;example: calculate percentage of calories from carb/protein/fat for each food — without fooloop&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image019.png"&gt;&lt;br&gt;
two lines of numpy code:
    A = np.array([[...]..]) # A.dim = (3,4)
    cal = A.sum(axis=0) # total calories
    percentage = 100 * A / cal.reshape(1,4) # percentage.dim = (1,4)&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;axis=0&lt;/code&gt;→ sum &lt;em&gt;vertically, &lt;/em&gt;&lt;code&gt;axis=1&lt;/code&gt;&lt;em&gt; → sum horizentally&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image020.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;reshape(a,b)&lt;/code&gt; → redundant here, just to make sure shape correct, &lt;em&gt;reshape call is cheap&lt;/em&gt;. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;A / cal&lt;/code&gt; → (3&lt;em&gt;4 matrix) / (1&lt;/em&gt;4 matrix) → &lt;strong&gt;broadcasting&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;more broadcasting examples:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image021.png"&gt;&lt;br&gt;
General principle: computing (m,n) matrix with (1,n) matrix 
⇒ the (1,n) matrix is &lt;em&gt;auto expanded to a (m,n) matrix&lt;/em&gt; by copying the row m times, to match the shape, calculate element-wise&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image022.png"/&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="a-note-on-python-numpy-vectors"&gt;A note on python numpy vectors&lt;/h2&gt;
&lt;p&gt;flexibility of broadcasting: both advantage and &lt;em&gt;weakness&lt;/em&gt;.
example: adding column vec and a row vec → get a matrix instead of throwing exceptions.
    &amp;gt;&amp;gt;&amp;gt; a
    array([1, 2, 3])
    &amp;gt;&amp;gt;&amp;gt; b
    array([[1],
           [2]])
    &amp;gt;&amp;gt;&amp;gt; a + b
    array([[2, 3, 4],
           [3, 4, 5]])&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips and trick to eliminate bugs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;avoid&lt;/strong&gt; &lt;strong&gt;rank-1 array&lt;/strong&gt;: &lt;br&gt;
&lt;code&gt;a.shape = (x,)&lt;/code&gt;
this is &lt;em&gt;neither row nor column vector&lt;/em&gt;, have non-intuitive effects.
    &amp;gt;&amp;gt;&amp;gt; a = np.array([1,2,3])
    &amp;gt;&amp;gt;&amp;gt; a.shape
    (3,)  # NOT (3,1)
    &amp;gt;&amp;gt;&amp;gt; a.T
    array([1, 2, 3])
    &amp;gt;&amp;gt;&amp;gt; np.dot(a, a.T)  # Mathematically would expact a matrix, if a is column vec
    14
    &amp;gt;&amp;gt;&amp;gt; a.T.shape
    (3,)&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;⇒ &lt;em&gt;do &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; use rank-1 arraies, use column/row vectors&lt;/em&gt;
    &amp;gt;&amp;gt;&amp;gt; a2 = a.reshape((-1, 1))  # A column vector -- (5,1) matrix.
    &amp;gt;&amp;gt;&amp;gt; a2
    array([[1],
           [2],
           [3]])
    &amp;gt;&amp;gt;&amp;gt; a2.T
    array([[1, 2, 3]])  # Note: two brackets!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;add assertions&lt;/strong&gt;&lt;br&gt;
&lt;code&gt;assert(a.shape == (3,1))&lt;/code&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="explanation-of-logistic-regression-cost-function-optional"&gt;Explanation of logistic regression cost function (optional)&lt;/h2&gt;
&lt;p&gt;Justisfy why we use this form of cost function:
y_hat ~= chance of y==1 given x
want to express P(y|x) using y_hat and y
P(y|x) as func(y, y_hat) at different values of y:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if y = 1: P(y|x) = P(y=1|x) = y_hat&lt;/li&gt;
&lt;li&gt;if y = 0: P(y|x) = P(y=0|x) = 1 - y_hat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ wrap the two cases &lt;em&gt;in one single formula&lt;/em&gt;: using exponent of y and (1-y)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image023.png"&gt;&lt;br&gt;
⇒ take log of P(y|x) ⇒ loss function (for a single training example)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image024.png"&gt;&lt;br&gt;
⇒ aggregate over all training examples i = 1..m:
(assume: data are iid)
P(labels in training set) = multiply( P(y(i)|x(i) )
take log → log(P(labels in training set)) = sum( log P(y(i)|x(i) ) = - J
&lt;strong&gt;maximizing likelihood = minimizing cost function&lt;/strong&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h1 id="assignments_1"&gt;Assignments&lt;/h1&gt;
&lt;h2 id="python-numpy-basics"&gt;python / numpy basics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;np.reshape() / np.shape&lt;/li&gt;
&lt;li&gt;calculate norm: &lt;code&gt;np.linalg.norm()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image026.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;keepdims=True&lt;/code&gt;: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;axes that are reduced will be &lt;em&gt;kept&lt;/em&gt; (with size=1)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;  &amp;gt;&amp;gt;&amp;gt; a&lt;/span&gt;
&lt;span class="code-line"&gt;  array([[ 0.01014617,  0.08222027, -0.59608242],&lt;/span&gt;
&lt;span class="code-line"&gt;        [-0.18495204, -1.50409531, -1.03853663],&lt;/span&gt;
&lt;span class="code-line"&gt;        [ 0.03995499, -0.67679544,  0.11513247]])&lt;/span&gt;
&lt;span class="code-line"&gt;  &amp;gt;&amp;gt;&amp;gt; a.sum(keepdims=1)&lt;/span&gt;
&lt;span class="code-line"&gt;  array([[-3.75300795]])&lt;/span&gt;
&lt;span class="code-line"&gt;  &amp;gt;&amp;gt;&amp;gt; a.sum()&lt;/span&gt;
&lt;span class="code-line"&gt;  -3.7530079538833663&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"&gt;broadcasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;softmax: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;softmax for row vec:
  x.shape = (1,n), x = [x1,...xn]
  y = softmax(x), y.shape = (1,n), &lt;code&gt;yi = exp(xi) / sum( exp(xi) )&lt;/code&gt;
  softmax for matrix
  X.shape = (m,n)
  Y = softmax(X) = [softmax(row-i of X)], Y.shape = (m, 1)&lt;/p&gt;
&lt;h2 id="logistic-regression-with-a-neural-network-mindset"&gt;Logistic Regression with a Neural Network mindset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;input preprocessing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;input dataset shape = (m, num_px, num_px, 3)
  → &lt;em&gt;reshape&lt;/em&gt; to one column per example, shape = (num_px&lt;em&gt;num_px&lt;/em&gt;3, ~~m~~)
  → &lt;em&gt;center &amp;amp; standardize&lt;/em&gt; data: &lt;code&gt;x' = (xi - x_mean) / std(x)&lt;/code&gt;, 
  but &lt;em&gt;for images:&lt;/em&gt; just divide by 255.0 (max pixel value), convenient and works almost as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;params initialization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For logistic regression (cost function convex), just init to zeros is OK. 
    w = np.zeros((dim,1))
    b = 0.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fwd prop: compute cost function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image027.png"&gt;&lt;br&gt;
  input &lt;code&gt;X&lt;/code&gt; (shape = nx*m, one column per example)→ logits &lt;code&gt;Z&lt;/code&gt; → activations &lt;code&gt;A=sigmoid(Z)&lt;/code&gt;→ cost &lt;code&gt;J&lt;/code&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bkwd prop&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk2//pasted_image028.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;gradient descent: w := w - alpha*dw&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predict: using learned params&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yhat = A = sigmoid(wT * X + b)&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>[Neural Networks and Deep Learning] week1. Introduction to deep learning</title><link href="http://x-wei.github.io/Ng_DLMooc_c1wk1.html" rel="alternate"></link><published>2017-09-11T00:00:00+02:00</published><updated>2017-09-11T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2017-09-11:/Ng_DLMooc_c1wk1.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="what-is-a-neural-network"&gt;What is a neural network?&lt;/h2&gt;
&lt;p&gt;Example: housing price prediciton. &lt;/p&gt;
&lt;p&gt;Each neuron: ReLU function&lt;/p&gt;
&lt;p&gt;Stacking multiple layers of neurons: hidden layers are concepts more general than input layer — found automatically by NN.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="supervised-learning-with-neural-networks"&gt;Supervised Learning with Neural Networks&lt;/h2&gt;
&lt;p&gt;supervised learning: during training, always have output corresponding to input.&lt;/p&gt;
&lt;p&gt;Different NN types …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="what-is-a-neural-network"&gt;What is a neural network?&lt;/h2&gt;
&lt;p&gt;Example: housing price prediciton. &lt;/p&gt;
&lt;p&gt;Each neuron: ReLU function&lt;/p&gt;
&lt;p&gt;Stacking multiple layers of neurons: hidden layers are concepts more general than input layer — found automatically by NN.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="supervised-learning-with-neural-networks"&gt;Supervised Learning with Neural Networks&lt;/h2&gt;
&lt;p&gt;supervised learning: during training, always have output corresponding to input.&lt;/p&gt;
&lt;p&gt;Different NN types are used for different problems:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image002.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image003.png"/&gt;&lt;/p&gt;
&lt;p&gt;structured data: database, each feature/column has a well-defined meaning.
unstructured data: audio/image/text, no well-defined meaning for pixels/tokens&lt;/p&gt;
&lt;h2 id="why-is-deep-learning-taking-off"&gt;Why is Deep Learning taking off?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt; drives deep learning progress.
(scale: both of NN and of data)&lt;/p&gt;
&lt;p&gt;trandition methods: pleateaus as amount of data grows further. 
NN: grows with data.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image005.png"/&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data scales up &lt;/li&gt;
&lt;li&gt;computation faster&lt;/li&gt;
&lt;li&gt;new algorithms, e.g. from sigmoid to ReLU, which in turn speeds up computation too. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="about-this-course"&gt;About this course&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image007.png"/&gt;&lt;/p&gt;
&lt;p&gt;This course: &lt;strong&gt;implementing&lt;/strong&gt; NN.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/Ng_DLMooc_c1wk1/pasted_image006.png"/&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 4: Deep Models for Text and Sequences</title><link href="http://x-wei.github.io/dlMOOC_L4.html" rel="alternate"></link><published>2016-06-07T00:00:00+02:00</published><updated>2016-06-07T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-07:/dlMOOC_L4.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;problems with text:   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;often very rare word is important, e.g. &lt;em&gt;retinopathy&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;ambiguity: e.g. &lt;em&gt;cat&lt;/em&gt; and &lt;em&gt;kitty&lt;/em&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;→ need a lot of labeled data ⇒ not realistic. &lt;br&gt;
⇒ &lt;strong&gt;unsupervised learning&lt;/strong&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;similar words appear in similar context. &lt;br&gt;
embedding: map words to small vectors&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image.png"&gt;&lt;br&gt;
measure the closeness by cosine distance: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="word2vec"&gt;word2vec&lt;/h2&gt;
&lt;p&gt;initial: random …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;problems with text:   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;often very rare word is important, e.g. &lt;em&gt;retinopathy&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;ambiguity: e.g. &lt;em&gt;cat&lt;/em&gt; and &lt;em&gt;kitty&lt;/em&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;→ need a lot of labeled data ⇒ not realistic. &lt;br&gt;
⇒ &lt;strong&gt;unsupervised learning&lt;/strong&gt; &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;similar words appear in similar context. &lt;br&gt;
embedding: map words to small vectors&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image.png"&gt;&lt;br&gt;
measure the closeness by cosine distance: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="word2vec"&gt;word2vec&lt;/h2&gt;
&lt;p&gt;initial: random vector&lt;br&gt;
→ train model to predict nearby word. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image001.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image004.png"&gt;&lt;br&gt;
pb: too many words in dictionary → softmax too slow&lt;br&gt;
⇒ random sample the non-target words &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image006.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="tsne"&gt;tSNE&lt;/h2&gt;
&lt;p&gt;dimension reduction (not PCA) that preserves the neighborhood structure (close vector → close in 2d as well). &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image002.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="rnn"&gt;RNN&lt;/h2&gt;
&lt;p&gt;treat varaible length sequences of words. &lt;br&gt;
use the current word (Xi) and the last prediction as input. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image007.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="backprop-for-rnn"&gt;backprop for RNN&lt;/h2&gt;
&lt;p&gt;apply highly correlated derivatives to W → not good for SGD. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;pb if we use highly correlated updates: grad either explod or it disappear quickly.   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image009.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;fix grad-exploding: &lt;em&gt;clip&lt;/em&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image010.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;grad-vanishing: memory loss in RNN&lt;br&gt;
⇒ LSTM  &lt;/br&gt;&lt;/p&gt;
&lt;h2 id="lstm"&gt;LSTM&lt;/h2&gt;
&lt;p&gt;in RNN: replace the NN by a LSTM cell&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image011.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image013.png"&gt;&lt;br&gt;
represent the system with memory by a diagram with logical gates:   &lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image014.png"&gt;&lt;br&gt;
change the decision variables to continous:&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image012.png"&gt;&lt;br&gt;
a logistic regression in each gate: controls when to remember and when to forget things. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image015.png"&gt;&lt;br&gt;
&lt;a href="http://blog.csdn.net/dark_scope/article/details/47056361"&gt;http://blog.csdn.net/dark_scope/article/details/47056361&lt;/a&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image024.png"&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image023.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;regularization for LSTM:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2 regularization: OK  &lt;/li&gt;
&lt;li&gt;dropout: OK when used for input/output (X and Y), but NOT use to the recurrent in/out.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="beam-search"&gt;beam search&lt;/h2&gt;
&lt;p&gt;beam search is for &lt;em&gt;generating&lt;/em&gt; sequences by RNN.   &lt;/p&gt;
&lt;p&gt;Greedy approach: at each step, &lt;em&gt;sample&lt;/em&gt; from the predicted distribution of the RNN. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image017.png"&gt;&lt;br&gt;
smarter approach: &lt;br&gt;
predict more steps and pick the seq with largest proba. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image018.png"&gt;&lt;br&gt;
pb with this: the number of possible seq grows exponentially &lt;br&gt;
⇒ just keep the few most promising seqs → "&lt;strong&gt;Beam search"&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image016.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="seq-to-seq"&gt;seq to seq&lt;/h2&gt;
&lt;p&gt;RNN: model to map vaiable length seq to fix-length vectors. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image021.png"&gt;&lt;br&gt;
Beam search: sequence generation (map fix-length vectors to seq)&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image019.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;concat them together: seq to seq system&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L4/pasted_image022.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;e.g. &lt;br&gt;
translation, speech recognation, image captionning  &lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 3: Convolutional Neural Networks</title><link href="http://x-wei.github.io/dlMOOC_L3.html" rel="alternate"></link><published>2016-06-06T00:00:00+02:00</published><updated>2016-06-06T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-06:/dlMOOC_L3.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;statistical invariance → &lt;strong&gt;weight sharing&lt;/strong&gt;&lt;br&gt;
e.g. image colors, translation invariance...   &lt;/br&gt;&lt;/p&gt;
&lt;h2 id="convnet"&gt;convnet&lt;/h2&gt;
&lt;p&gt;is NNs that share their weights across space.   &lt;/p&gt;
&lt;p&gt;convolution: slide a small patch of NN over the image to produce a new "image"&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;convnet forms a pyramid, each "stack of pincake" get larger depth and smaller area. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="convolutional-lingo"&gt;convolutional …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;statistical invariance → &lt;strong&gt;weight sharing&lt;/strong&gt;&lt;br&gt;
e.g. image colors, translation invariance...   &lt;/br&gt;&lt;/p&gt;
&lt;h2 id="convnet"&gt;convnet&lt;/h2&gt;
&lt;p&gt;is NNs that share their weights across space.   &lt;/p&gt;
&lt;p&gt;convolution: slide a small patch of NN over the image to produce a new "image"&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;convnet forms a pyramid, each "stack of pincake" get larger depth and smaller area. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="convolutional-lingo"&gt;convolutional lingo&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image002.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;patch (kernel)&lt;/strong&gt;&lt;br&gt;
small NN that slides over the image.   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;depth&lt;/strong&gt;&lt;br&gt;
number of pincakes in stack.   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;feature map&lt;/strong&gt;&lt;br&gt;
each "pincake" in stack.   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;stride&lt;/strong&gt;&lt;br&gt;
nb of pixels that you shift each time you move your filter. &lt;br&gt;
e.g. stride=1 → output almost the same size as the input; stride=2 → output about half size  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;padding&lt;/strong&gt;&lt;br&gt;
the way you treat the edge of image.   &lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;valid padding&lt;/em&gt;: don't go pass the edge  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;same padding&lt;/em&gt;: go off the image and pad with 0s (output size=input size)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image003.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image004.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;once got "deep and narrow" representation by convolution, connect to a normal (regular) fully-conncected NN. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="pooling"&gt;pooling&lt;/h2&gt;
&lt;p&gt;better way to reduce the spatial extend (i.e. size) of the feature map. &lt;br&gt;
simple convnet: use large stride to reduce the feature map size. ⇒ &lt;em&gt;aggressive&lt;/em&gt;&lt;br&gt;
&lt;strong&gt;pooling&lt;/strong&gt;: use small stride (ex. stride=1), then &lt;em&gt;take convolutions in neighbourhood and combine them&lt;/em&gt;.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image006.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;max pooling&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image007.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;average pooling&lt;/strong&gt;&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="1x1-convolution"&gt;1x1 convolution&lt;/h2&gt;
&lt;p&gt;classic convolution = &lt;em&gt;linear&lt;/em&gt; classifier over a small patch of image&lt;br&gt;
&lt;strong&gt;add a 1x1 convolution in the middle&lt;/strong&gt; ⇒ a mini-dnn over the patch. &lt;br&gt;
cheap: not convolution, just matrix multiplication. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image009.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="inception-module"&gt;inception module&lt;/h2&gt;
&lt;p&gt;between each layers, just do both pooling and 1x1 conv, and 3x3 and 5x5.. conv, and concatenate them together. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L3/pasted_image010.png"&gt;&lt;br&gt;
benefit: total number of parameters is small, yet performance better.   &lt;/br&gt;&lt;/img&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 2: Deep Neural Networks</title><link href="http://x-wei.github.io/dlMOOC_L2.html" rel="alternate"></link><published>2016-06-05T18:00:00+02:00</published><updated>2016-06-05T18:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-05:/dlMOOC_L2.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="linear-models"&gt;Linear models&lt;/h2&gt;
&lt;p&gt;matrix multiplication: fast with GPU&lt;br&gt;
numerically stable&lt;br&gt;
cannot cocatenate linear units → equivalent to one big matrix...  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;⇒ add non-linear units in between   &lt;/p&gt;
&lt;h2 id="rectified-linear-units-relu"&gt;rectified linear units (RELU)&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image002.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;chain rule: efficient computationally&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image004.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="back-propagation"&gt;back propagation&lt;/h2&gt;
&lt;p&gt;easy to compute the gradient as long as the function Y(X) is made of simple …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id="linear-models"&gt;Linear models&lt;/h2&gt;
&lt;p&gt;matrix multiplication: fast with GPU&lt;br&gt;
numerically stable&lt;br&gt;
cannot cocatenate linear units → equivalent to one big matrix...  &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;⇒ add non-linear units in between   &lt;/p&gt;
&lt;h2 id="rectified-linear-units-relu"&gt;rectified linear units (RELU)&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image002.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;chain rule: efficient computationally&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image003.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image004.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="back-propagation"&gt;back propagation&lt;/h2&gt;
&lt;p&gt;easy to compute the gradient as long as the function Y(X) is made of simple blocks with simple deritivates. &lt;br&gt;
most deep-learning framework can do it automatically for you.   &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;N.B. The backprop block takes 2x memory/compute wrt the forward prop blocks. &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image005.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;first neural network: RELU units between linear classifiers: &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image001.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2 id="tensor-flow"&gt;Tensor flow&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tensors&lt;/code&gt; define computations, and they are nodes in a computation &lt;code&gt;graph&lt;/code&gt;. &lt;br&gt;
To actually run the optimization, use &lt;code&gt;sessions&lt;/code&gt;...  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;define a computation graph:   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;batch_size = 128  &lt;/span&gt;
&lt;span class="code-line"&gt;num_hidden = 1024&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;graph = tf.Graph()  &lt;/span&gt;
&lt;span class="code-line"&gt;with graph.as_default():&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Input data. For the training data, we use a placeholder that will be fed  &lt;/span&gt;
&lt;span class="code-line"&gt;  # at run time with a training minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_train_dataset = tf.placeholder(tf.float32,  &lt;/span&gt;
&lt;span class="code-line"&gt;                                    shape=(batch_size, image_size * image_size))  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_valid_dataset = tf.constant(valid_dataset)  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_test_dataset = tf.constant(test_dataset)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Variables for linear layer 1  &lt;/span&gt;
&lt;span class="code-line"&gt;  W1 = tf.Variable(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.truncated_normal([image_size * image_size, num_hidden]))  &lt;/span&gt;
&lt;span class="code-line"&gt;  b1 = tf.Variable(tf.zeros([num_hidden]))&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Hidden RELU input computation  &lt;/span&gt;
&lt;span class="code-line"&gt;  y1 = tf.matmul(tf_train_dataset, W1) + b1  &lt;/span&gt;
&lt;span class="code-line"&gt;  # Hidden RELU output computation  &lt;/span&gt;
&lt;span class="code-line"&gt;  X1 = tf.nn.relu(y1)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Variables for linear layer 2  &lt;/span&gt;
&lt;span class="code-line"&gt;  W2 = tf.Variable(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.truncated_normal([num_hidden, num_labels]))#W2  &lt;/span&gt;
&lt;span class="code-line"&gt;  b2 = tf.Variable(tf.zeros([num_labels])) #b2  &lt;/span&gt;
&lt;span class="code-line"&gt;  # logit (y2) output  &lt;/span&gt;
&lt;span class="code-line"&gt;  logits = tf.matmul(X1, W2) + b2  &lt;/span&gt;
&lt;span class="code-line"&gt;  loss = tf.reduce_mean(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  def getlogits(X):  &lt;/span&gt;
&lt;span class="code-line"&gt;    y1 = tf.matmul(X, W1) + b1  &lt;/span&gt;
&lt;span class="code-line"&gt;    X1 = tf.nn.relu(y1)  &lt;/span&gt;
&lt;span class="code-line"&gt;    return tf.matmul(X1, W2) + b2&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Optimizer.  &lt;/span&gt;
&lt;span class="code-line"&gt;  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Predictions for the training, validation, and test data.  &lt;/span&gt;
&lt;span class="code-line"&gt;  train_prediction = tf.nn.softmax(logits)  &lt;/span&gt;
&lt;span class="code-line"&gt;  valid_prediction = tf.nn.softmax( getlogits(tf_valid_dataset) )  &lt;/span&gt;
&lt;span class="code-line"&gt;  test_prediction = tf.nn.softmax( getlogits(tf_test_dataset))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;run sgd optimization:   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;num_steps = 3001&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;with tf.Session(graph=graph) as session:  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf.initialize_all_variables().run()  &lt;/span&gt;
&lt;span class="code-line"&gt;  print("Initialized")  &lt;/span&gt;
&lt;span class="code-line"&gt;  for step in range(num_steps):  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Pick an offset within the training data, which has been randomized.  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Note: we could use better randomization across epochs.  &lt;/span&gt;
&lt;span class="code-line"&gt;    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Generate a minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;    batch_data = train_dataset[offset:(offset + batch_size), :]  &lt;/span&gt;
&lt;span class="code-line"&gt;    batch_labels = train_labels[offset:(offset + batch_size), :]  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Prepare a dictionary telling the session where to feed the minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;    # The key of the dictionary is the placeholder node of the graph to be fed,  &lt;/span&gt;
&lt;span class="code-line"&gt;    # and the value is the numpy array to feed to it.  &lt;/span&gt;
&lt;span class="code-line"&gt;    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}  &lt;/span&gt;
&lt;span class="code-line"&gt;    _, l, predictions = session.run(  &lt;/span&gt;
&lt;span class="code-line"&gt;      [optimizer, loss, train_prediction], feed_dict=feed_dict)  &lt;/span&gt;
&lt;span class="code-line"&gt;    if (step % 500 == 0):  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Minibatch loss at step %d: %f" % (step, l))  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Validation accuracy: %.1f%%" % accuracy(  &lt;/span&gt;
&lt;span class="code-line"&gt;        valid_prediction.eval(), valid_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;  print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="regularization"&gt;Regularization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;early termination: stop when cannot improve in validation performance.   &lt;/li&gt;
&lt;li&gt;L2 regularization: adding L2 norm of   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image006.png"&gt; &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="dropout"&gt;Dropout&lt;/h2&gt;
&lt;p&gt;def. &lt;strong&gt;activation&lt;/strong&gt; is the output of last layer that flows into the next layer. &lt;br&gt;
dropout: &lt;em&gt;randomly set half of activations to 0&lt;/em&gt;.  &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;rational: forcing your model to learn reduadant representations (consus over an ensemble of nns...)... &lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image007.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;N.B.&lt;br&gt;
for evaluation no longer dropout, &lt;code&gt;ye&lt;/code&gt; = average of activations, trick to let &lt;code&gt;ye=E(yt)&lt;/code&gt;, in training, multiply the remaining activations by 2.&lt;br&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L2/pasted_image008.png"&gt; &lt;/img&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 1: From Machine Learning to Deep Learning</title><link href="http://x-wei.github.io/dlMOOC_L1.html" rel="alternate"></link><published>2016-06-05T00:00:00+02:00</published><updated>2016-06-05T00:00:00+02:00</updated><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-05:/dlMOOC_L1.html</id><summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;这是udacity上deeplearning的笔记, 做得非常粗糙, 而且这门课也只是介绍性质的... 
&lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;https://www.udacity.com/course/deep-learning--ud730&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="softmax-function"&gt;Softmax function&lt;/h2&gt;
&lt;p&gt;socres &lt;code&gt;yi&lt;/code&gt; ⇒ probabilities &lt;code&gt;pi&lt;/code&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;property: &lt;strong&gt;smaller scores ⇒ less certain about result&lt;/strong&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;h2 id="onehot-encoding"&gt;Onehot encoding&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image002.png"/&gt;&lt;/p&gt;
&lt;h2 id="cross-entropy"&gt;Cross entropy&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;measure how well the probability vector &lt;/em&gt;&lt;code&gt;S&lt;/code&gt;&lt;em&gt; corresponds to the label vector &lt;/em&gt;&lt;code&gt;L&lt;/code&gt;&lt;em&gt;.&lt;/em&gt; 
⇒ cross entropy &lt;code&gt;D(S,L)&lt;/code&gt;&lt;em&gt;( D&amp;gt;=0, the smaller the …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;这是udacity上deeplearning的笔记, 做得非常粗糙, 而且这门课也只是介绍性质的... 
&lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;https://www.udacity.com/course/deep-learning--ud730&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="softmax-function"&gt;Softmax function&lt;/h2&gt;
&lt;p&gt;socres &lt;code&gt;yi&lt;/code&gt; ⇒ probabilities &lt;code&gt;pi&lt;/code&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;property: &lt;strong&gt;smaller scores ⇒ less certain about result&lt;/strong&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;h2 id="onehot-encoding"&gt;Onehot encoding&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image002.png"/&gt;&lt;/p&gt;
&lt;h2 id="cross-entropy"&gt;Cross entropy&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;measure how well the probability vector &lt;/em&gt;&lt;code&gt;S&lt;/code&gt;&lt;em&gt; corresponds to the label vector &lt;/em&gt;&lt;code&gt;L&lt;/code&gt;&lt;em&gt;.&lt;/em&gt; 
⇒ cross entropy &lt;code&gt;D(S,L)&lt;/code&gt;&lt;em&gt;( D&amp;gt;=0, the smaller the better)&lt;/em&gt;
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image003.png"/&gt;&lt;/p&gt;
&lt;p&gt;N.B. &lt;code&gt;D(S,L)&lt;/code&gt; is not symmetric (never log 0 ) &lt;/p&gt;
&lt;p&gt;recap ("multinominal logistic classificaton"): 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image004.png"/&gt;&lt;/p&gt;
&lt;h2 id="minimizing-cross-entropy"&gt;Minimizing cross entropy&lt;/h2&gt;
&lt;p&gt;take avg D as loss function: 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image008.png"&gt;
⇒ optimization, for example, by grad-desc: 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image007.png"/&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;for the moment, take the optimizer as black box. &lt;/p&gt;
&lt;p&gt;two practical problems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how to feed img pixels to classifiers &lt;/li&gt;
&lt;li&gt;how to initialize the optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="numerical-stability"&gt;numerical stability&lt;/h2&gt;
&lt;p&gt;adding very small values to very large values will introduce a lot of errors ! 
ex. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; a = 1e9&lt;/span&gt;
&lt;span class="code-line"&gt;&amp;gt;&amp;gt;&amp;gt; for _ in xrange(1000000):&lt;/span&gt;
&lt;span class="code-line"&gt;...     a += 1e-6&lt;/span&gt;
&lt;span class="code-line"&gt;&amp;gt;&amp;gt;&amp;gt; a - 1e9&lt;/span&gt;
&lt;span class="code-line"&gt;0.95367431640625&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;⇒ the result is not 1... &lt;/p&gt;
&lt;p&gt;⇒ normalize input ! ⇒ &lt;strong&gt;0 mean, 1 variance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;this make optimizers easier to find optimum. 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image009.png"/&gt;&lt;/p&gt;
&lt;p&gt;normalization for images: 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image010.png"/&gt;&lt;/p&gt;
&lt;h2 id="weight-initialization"&gt;weight initialization&lt;/h2&gt;
&lt;p&gt;draw init w/b from a &lt;code&gt;Gaussian(0, sigma)&lt;/code&gt;, sigma → magtitude of initial output. 
small sigma means small outputs → uncertain about result. 
⇒ take small sigma for initialization 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image011.png"/&gt;&lt;/p&gt;
&lt;p&gt;recap: &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image012.png"&gt;
⇒ feed this loss fcn to the optimizer &lt;/img&gt;&lt;/p&gt;
&lt;h2 id="training-validation-and-test-dataset"&gt;training, validation and test dataset&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;rule of thumb (30)&lt;/strong&gt;: 
a change that affects 30 examples in the validation set is statically significant. 
⇒ in most cases use &amp;gt;30000 samples in validation set → changes in 0.1% is significant. &lt;/p&gt;
&lt;h2 id="sgd"&gt;SGD&lt;/h2&gt;
&lt;p&gt;rule of thumb: computing &lt;code&gt;grad(L)&lt;/code&gt; takes 3x time than computing loss fcn &lt;code&gt;L&lt;/code&gt;. → pb for scaling.. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image014.png"&gt;
SGD is the only fast enough model in practice. &lt;/img&gt;&lt;/p&gt;
&lt;p&gt;tricks to help SGD: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;normalize data (0 mean, uni-var)&lt;/li&gt;
&lt;li&gt;randomly initialize weights&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;momentum&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;learning rate decay&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="momentum"&gt;Momentum&lt;/h2&gt;
&lt;p&gt;SGD: many small steps in random directions → general direction is more accurate. 
⇒ keep a running average of the gradients&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image015.png"/&gt;&lt;/p&gt;
&lt;h2 id="learning-rate-decay"&gt;Learning rate decay&lt;/h2&gt;
&lt;p&gt;take smaller and smaller steps (alpha decays)
e.g. alpha decays exponentially...&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image016.png"/&gt;&lt;/p&gt;
&lt;h2 id="parameter-tuning"&gt;parameter tuning&lt;/h2&gt;
&lt;p&gt;how quickly you learning != how well you train.. 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image017.png"&gt;
balck magics in deep learning: 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image018.png"/&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adagrad&lt;/strong&gt;
variant of SGD, implicitly decays momentum and learning rate. &lt;/p&gt;
&lt;p&gt;recap: 
&lt;img alt="" class="img-responsive" src="../images/dlMOOC_L1/pasted_image019.png"/&gt;&lt;/p&gt;</content><category term="deep learning"></category></entry></feed>