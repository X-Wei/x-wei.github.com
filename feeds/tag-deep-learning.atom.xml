<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's blog</title><link href="http://x-wei.github.io/" rel="alternate"></link><link href="http://x-wei.github.io/feeds/tag-deep-learning.atom.xml" rel="self"></link><id>http://x-wei.github.io/</id><updated>2016-06-07T00:00:00+02:00</updated><entry><title>(DeepLearning MOOC) Lesson 4: Deep Models for Text and Sequences</title><link href="http://x-wei.github.io/dlMOOC_L4.html" rel="alternate"></link><published>2016-06-07T00:00:00+02:00</published><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-07:dlMOOC_L4.html</id><summary type="html">&lt;p&gt;problems with text:   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;often very rare word is important, e.g. &lt;em&gt;retinopathy&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;ambiguity: e.g. &lt;em&gt;cat&lt;/em&gt; and &lt;em&gt;kitty&lt;/em&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;→ need a lot of labeled data ⇒ not realistic. &lt;br/&gt;
⇒ &lt;strong&gt;unsupervised learning&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;similar words appear in similar context. &lt;br/&gt;
embedding: map words to small vectors&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image.png"/&gt;&lt;br/&gt;
measure the closeness by cosine distance: &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image003.png"/&gt; &lt;/p&gt;
&lt;h2 id="word2vec"&gt;word2vec&lt;/h2&gt;
&lt;p&gt;initial: random vector&lt;br/&gt;
→ train model to predict nearby word. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image001.png"/&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image004.png"/&gt;&lt;br/&gt;
pb: too many words in dictionary → softmax too slow&lt;br/&gt;
⇒ random sample the non-target words &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image005.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image006.png"/&gt; &lt;/p&gt;
&lt;h2 id="tsne"&gt;tSNE&lt;/h2&gt;
&lt;p&gt;dimension reduction (not PCA) that preserves the neighborhood structure (close vector → close in 2d as well). &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image002.png"/&gt; &lt;/p&gt;
&lt;h2 id="rnn"&gt;RNN&lt;/h2&gt;
&lt;p&gt;treat varaible length sequences of words. &lt;br/&gt;
use the current word (Xi) and the last prediction as input. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image007.png"/&gt; &lt;/p&gt;
&lt;h2 id="backprop-for-rnn"&gt;backprop for RNN&lt;/h2&gt;
&lt;p&gt;apply highly correlated derivatives to W → not good for SGD. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image008.png"/&gt; &lt;/p&gt;
&lt;p&gt;pb if we use highly correlated updates: grad either explod or it disappear quickly.   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image009.png"/&gt; &lt;/p&gt;
&lt;p&gt;fix grad-exploding: &lt;em&gt;clip&lt;/em&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image010.png"/&gt; &lt;/p&gt;
&lt;p&gt;grad-vanishing: memory loss in RNN&lt;br/&gt;
⇒ LSTM  &lt;/p&gt;
&lt;h2 id="lstm"&gt;LSTM&lt;/h2&gt;
&lt;p&gt;in RNN: replace the NN by a LSTM cell&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image011.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image013.png"/&gt;&lt;br/&gt;
represent the system with memory by a diagram with logical gates:   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image014.png"/&gt;&lt;br/&gt;
change the decision variables to continous:&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image012.png"/&gt;&lt;br/&gt;
a logistic regression in each gate: controls when to remember and when to forget things. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image015.png"/&gt;&lt;br/&gt;
&lt;a href="http://blog.csdn.net/dark_scope/article/details/47056361"&gt;http://blog.csdn.net/dark_scope/article/details/47056361&lt;/a&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image024.png"/&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image023.png"/&gt; &lt;/p&gt;
&lt;p&gt;regularization for LSTM:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2 regularization: OK  &lt;/li&gt;
&lt;li&gt;dropout: OK when used for input/output (X and Y), but NOT use to the recurrent in/out.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="beam-search"&gt;beam search&lt;/h2&gt;
&lt;p&gt;beam search is for &lt;em&gt;generating&lt;/em&gt; sequences by RNN.   &lt;/p&gt;
&lt;p&gt;Greedy approach: at each step, &lt;em&gt;sample&lt;/em&gt; from the predicted distribution of the RNN. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image017.png"/&gt;&lt;br/&gt;
smarter approach: &lt;br/&gt;
predict more steps and pick the seq with largest proba. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image018.png"/&gt;&lt;br/&gt;
pb with this: the number of possible seq grows exponentially &lt;br/&gt;
⇒ just keep the few most promising seqs → "&lt;strong&gt;Beam search"&lt;/strong&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image016.png"/&gt; &lt;/p&gt;
&lt;h2 id="seq-to-seq"&gt;seq to seq&lt;/h2&gt;
&lt;p&gt;RNN: model to map vaiable length seq to fix-length vectors. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image021.png"/&gt;&lt;br/&gt;
Beam search: sequence generation (map fix-length vectors to seq)&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image019.png"/&gt; &lt;/p&gt;
&lt;p&gt;concat them together: seq to seq system&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L4/pasted_image022.png"/&gt; &lt;/p&gt;
&lt;p&gt;e.g. &lt;br/&gt;
translation, speech recognation, image captionning  &lt;/p&gt;</summary><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 3: Convolutional Neural Networks</title><link href="http://x-wei.github.io/dlMOOC_L3.html" rel="alternate"></link><published>2016-06-06T00:00:00+02:00</published><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-06:dlMOOC_L3.html</id><summary type="html">&lt;p&gt;statistical invariance → &lt;strong&gt;weight sharing&lt;/strong&gt;&lt;br/&gt;
e.g. image colors, translation invariance...   &lt;/p&gt;
&lt;h2 id="convnet"&gt;convnet&lt;/h2&gt;
&lt;p&gt;is NNs that share their weights across space.   &lt;/p&gt;
&lt;p&gt;convolution: slide a small patch of NN over the image to produce a new "image"&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image.png"/&gt; &lt;/p&gt;
&lt;p&gt;convnet forms a pyramid, each "stack of pincake" get larger depth and smaller area. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image001.png"/&gt; &lt;/p&gt;
&lt;h2 id="convolutional-lingo"&gt;convolutional lingo&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image002.png"/&gt; &lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;patch (kernel)&lt;/strong&gt;&lt;br/&gt;
small NN that slides over the image.   &lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;depth&lt;/strong&gt;&lt;br/&gt;
number of pincakes in stack.   &lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;feature map&lt;/strong&gt;&lt;br/&gt;
each "pincake" in stack.   &lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;stride&lt;/strong&gt;&lt;br/&gt;
nb of pixels that you shift each time you move your filter. &lt;br/&gt;
e.g. stride=1 → output almost the same size as the input; stride=2 → output about half size  &lt;/p&gt;
&lt;p&gt;def. &lt;strong&gt;padding&lt;/strong&gt;&lt;br/&gt;
the way you treat the edge of image.   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;valid padding&lt;/em&gt;: don't go pass the edge  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;same padding&lt;/em&gt;: go off the image and pad with 0s (output size=input size)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image003.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image004.png"/&gt; &lt;/p&gt;
&lt;p&gt;once got "deep and narrow" representation by convolution, connect to a normal (regular) fully-conncected NN. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image005.png"/&gt; &lt;/p&gt;
&lt;h2 id="pooling"&gt;pooling&lt;/h2&gt;
&lt;p&gt;better way to reduce the spatial extend (i.e. size) of the feature map. &lt;br/&gt;
simple convnet: use large stride to reduce the feature map size. ⇒ &lt;em&gt;aggressive&lt;/em&gt;&lt;br/&gt;
&lt;strong&gt;pooling&lt;/strong&gt;: use small stride (ex. stride=1), then &lt;em&gt;take convolutions in neighbourhood and combine them&lt;/em&gt;.&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image006.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;max pooling&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image007.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;average pooling&lt;/strong&gt;&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image008.png"/&gt; &lt;/p&gt;
&lt;h2 id="1x1-convolution"&gt;1x1 convolution&lt;/h2&gt;
&lt;p&gt;classic convolution = &lt;em&gt;linear&lt;/em&gt; classifier over a small patch of image&lt;br/&gt;
&lt;strong&gt;add a 1x1 convolution in the middle&lt;/strong&gt; ⇒ a mini-dnn over the patch. &lt;br/&gt;
cheap: not convolution, just matrix multiplication. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image009.png"/&gt; &lt;/p&gt;
&lt;h2 id="inception-module"&gt;inception module&lt;/h2&gt;
&lt;p&gt;between each layers, just do both pooling and 1x1 conv, and 3x3 and 5x5.. conv, and concatenate them together. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L3/pasted_image010.png"/&gt;&lt;br/&gt;
benefit: total number of parameters is small, yet performance better.   &lt;/p&gt;</summary><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 2: Deep Neural Networks</title><link href="http://x-wei.github.io/dlMOOC_L2.html" rel="alternate"></link><published>2016-06-05T18:00:00+02:00</published><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-05:dlMOOC_L2.html</id><summary type="html">&lt;h2 id="linear-models"&gt;Linear models&lt;/h2&gt;
&lt;p&gt;matrix multiplication: fast with GPU&lt;br/&gt;
numerically stable&lt;br/&gt;
cannot cocatenate linear units → equivalent to one big matrix...  &lt;/p&gt;
&lt;p&gt;⇒ add non-linear units in between   &lt;/p&gt;
&lt;h2 id="rectified-linear-units-relu"&gt;rectified linear units (RELU)&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image002.png"/&gt; &lt;/p&gt;
&lt;p&gt;chain rule: efficient computationally&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image003.png"/&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image004.png"/&gt; &lt;/p&gt;
&lt;h2 id="back-propagation"&gt;back propagation&lt;/h2&gt;
&lt;p&gt;easy to compute the gradient as long as the function Y(X) is made of simple blocks with simple deritivates. &lt;br/&gt;
most deep-learning framework can do it automatically for you.   &lt;/p&gt;
&lt;p&gt;N.B. The backprop block takes 2x memory/compute wrt the forward prop blocks. &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image005.png"/&gt; &lt;/p&gt;
&lt;p&gt;first neural network: RELU units between linear classifiers: &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image001.png"/&gt; &lt;/p&gt;
&lt;h2 id="tensor-flow"&gt;Tensor flow&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tensors&lt;/code&gt; define computations, and they are nodes in a computation &lt;code&gt;graph&lt;/code&gt;. &lt;br/&gt;
To actually run the optimization, use &lt;code&gt;sessions&lt;/code&gt;...  &lt;/p&gt;
&lt;p&gt;define a computation graph:   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;batch_size = 128  &lt;/span&gt;
&lt;span class="code-line"&gt;num_hidden = 1024&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;graph = tf.Graph()  &lt;/span&gt;
&lt;span class="code-line"&gt;with graph.as_default():&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Input data. For the training data, we use a placeholder that will be fed  &lt;/span&gt;
&lt;span class="code-line"&gt;  # at run time with a training minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_train_dataset = tf.placeholder(tf.float32,  &lt;/span&gt;
&lt;span class="code-line"&gt;                                    shape=(batch_size, image_size * image_size))  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_valid_dataset = tf.constant(valid_dataset)  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf_test_dataset = tf.constant(test_dataset)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Variables for linear layer 1  &lt;/span&gt;
&lt;span class="code-line"&gt;  W1 = tf.Variable(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.truncated_normal([image_size * image_size, num_hidden]))  &lt;/span&gt;
&lt;span class="code-line"&gt;  b1 = tf.Variable(tf.zeros([num_hidden]))&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Hidden RELU input computation  &lt;/span&gt;
&lt;span class="code-line"&gt;  y1 = tf.matmul(tf_train_dataset, W1) + b1  &lt;/span&gt;
&lt;span class="code-line"&gt;  # Hidden RELU output computation  &lt;/span&gt;
&lt;span class="code-line"&gt;  X1 = tf.nn.relu(y1)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Variables for linear layer 2  &lt;/span&gt;
&lt;span class="code-line"&gt;  W2 = tf.Variable(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.truncated_normal([num_hidden, num_labels]))#W2  &lt;/span&gt;
&lt;span class="code-line"&gt;  b2 = tf.Variable(tf.zeros([num_labels])) #b2  &lt;/span&gt;
&lt;span class="code-line"&gt;  # logit (y2) output  &lt;/span&gt;
&lt;span class="code-line"&gt;  logits = tf.matmul(X1, W2) + b2  &lt;/span&gt;
&lt;span class="code-line"&gt;  loss = tf.reduce_mean(  &lt;/span&gt;
&lt;span class="code-line"&gt;    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  def getlogits(X):  &lt;/span&gt;
&lt;span class="code-line"&gt;    y1 = tf.matmul(X, W1) + b1  &lt;/span&gt;
&lt;span class="code-line"&gt;    X1 = tf.nn.relu(y1)  &lt;/span&gt;
&lt;span class="code-line"&gt;    return tf.matmul(X1, W2) + b2&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Optimizer.  &lt;/span&gt;
&lt;span class="code-line"&gt;  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;  # Predictions for the training, validation, and test data.  &lt;/span&gt;
&lt;span class="code-line"&gt;  train_prediction = tf.nn.softmax(logits)  &lt;/span&gt;
&lt;span class="code-line"&gt;  valid_prediction = tf.nn.softmax( getlogits(tf_valid_dataset) )  &lt;/span&gt;
&lt;span class="code-line"&gt;  test_prediction = tf.nn.softmax( getlogits(tf_test_dataset))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;run sgd optimization:   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;num_steps = 3001&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;with tf.Session(graph=graph) as session:  &lt;/span&gt;
&lt;span class="code-line"&gt;  tf.initialize_all_variables().run()  &lt;/span&gt;
&lt;span class="code-line"&gt;  print("Initialized")  &lt;/span&gt;
&lt;span class="code-line"&gt;  for step in range(num_steps):  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Pick an offset within the training data, which has been randomized.  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Note: we could use better randomization across epochs.  &lt;/span&gt;
&lt;span class="code-line"&gt;    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Generate a minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;    batch_data = train_dataset[offset:(offset + batch_size), :]  &lt;/span&gt;
&lt;span class="code-line"&gt;    batch_labels = train_labels[offset:(offset + batch_size), :]  &lt;/span&gt;
&lt;span class="code-line"&gt;    # Prepare a dictionary telling the session where to feed the minibatch.  &lt;/span&gt;
&lt;span class="code-line"&gt;    # The key of the dictionary is the placeholder node of the graph to be fed,  &lt;/span&gt;
&lt;span class="code-line"&gt;    # and the value is the numpy array to feed to it.  &lt;/span&gt;
&lt;span class="code-line"&gt;    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}  &lt;/span&gt;
&lt;span class="code-line"&gt;    _, l, predictions = session.run(  &lt;/span&gt;
&lt;span class="code-line"&gt;      [optimizer, loss, train_prediction], feed_dict=feed_dict)  &lt;/span&gt;
&lt;span class="code-line"&gt;    if (step % 500 == 0):  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Minibatch loss at step %d: %f" % (step, l))  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;      print("Validation accuracy: %.1f%%" % accuracy(  &lt;/span&gt;
&lt;span class="code-line"&gt;        valid_prediction.eval(), valid_labels))  &lt;/span&gt;
&lt;span class="code-line"&gt;  print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="regularization"&gt;Regularization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;early termination: stop when cannot improve in validation performance.   &lt;/li&gt;
&lt;li&gt;L2 regularization: adding L2 norm of   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image006.png"/&gt; &lt;/p&gt;
&lt;h2 id="dropout"&gt;Dropout&lt;/h2&gt;
&lt;p&gt;def. &lt;strong&gt;activation&lt;/strong&gt; is the output of last layer that flows into the next layer. &lt;br/&gt;
dropout: &lt;em&gt;randomly set half of activations to 0&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;rational: forcing your model to learn reduadant representations (consus over an ensemble of nns...)... &lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image007.png"/&gt; &lt;/p&gt;
&lt;p&gt;N.B.&lt;br/&gt;
for evaluation no longer dropout, &lt;code&gt;ye&lt;/code&gt; = average of activations, trick to let &lt;code&gt;ye=E(yt)&lt;/code&gt;, in training, multiply the remaining activations by 2.&lt;br/&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L2/pasted_image008.png"/&gt; &lt;/p&gt;</summary><category term="deep learning"></category></entry><entry><title>(DeepLearning MOOC) Lesson 1: From Machine Learning to Deep Learning</title><link href="http://x-wei.github.io/dlMOOC_L1.html" rel="alternate"></link><published>2016-06-05T00:00:00+02:00</published><author><name>mx</name></author><id>tag:x-wei.github.io,2016-06-05:dlMOOC_L1.html</id><summary type="html">&lt;p&gt;这是udacity上deeplearning的笔记, 做得非常粗糙, 而且这门课也只是介绍性质的... 
&lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;https://www.udacity.com/course/deep-learning--ud730&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="softmax-function"&gt;Softmax function&lt;/h2&gt;
&lt;p&gt;socres &lt;code&gt;yi&lt;/code&gt; ⇒ probabilities &lt;code&gt;pi&lt;/code&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;property: &lt;strong&gt;smaller scores ⇒ less certain about result&lt;/strong&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image001.png"/&gt;&lt;/p&gt;
&lt;h2 id="onehot-encoding"&gt;Onehot encoding&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image002.png"/&gt;&lt;/p&gt;
&lt;h2 id="cross-entropy"&gt;Cross entropy&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;measure how well the probability vector &lt;/em&gt;&lt;code&gt;S&lt;/code&gt;&lt;em&gt; corresponds to the label vector &lt;/em&gt;&lt;code&gt;L&lt;/code&gt;&lt;em&gt;.&lt;/em&gt; 
⇒ cross entropy &lt;code&gt;D(S,L)&lt;/code&gt;&lt;em&gt;( D&amp;gt;=0, the smaller the better)&lt;/em&gt;
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image003.png"/&gt;&lt;/p&gt;
&lt;p&gt;N.B. &lt;code&gt;D(S,L)&lt;/code&gt; is not symmetric (never log 0 ) &lt;/p&gt;
&lt;p&gt;recap ("multinominal logistic classificaton"): 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image004.png"/&gt;&lt;/p&gt;
&lt;h2 id="minimizing-cross-entropy"&gt;Minimizing cross entropy&lt;/h2&gt;
&lt;p&gt;take avg D as loss function: 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image008.png"/&gt;
⇒ optimization, for example, by grad-desc: 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image007.png"/&gt;&lt;/p&gt;
&lt;p&gt;for the moment, take the optimizer as black box. &lt;/p&gt;
&lt;p&gt;two practical problems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how to feed img pixels to classifiers &lt;/li&gt;
&lt;li&gt;how to initialize the optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="numerical-stability"&gt;numerical stability&lt;/h2&gt;
&lt;p&gt;adding very small values to very large values will introduce a lot of errors ! 
ex. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; a = 1e9&lt;/span&gt;
&lt;span class="code-line"&gt;&amp;gt;&amp;gt;&amp;gt; for _ in xrange(1000000):&lt;/span&gt;
&lt;span class="code-line"&gt;...     a += 1e-6&lt;/span&gt;
&lt;span class="code-line"&gt;&amp;gt;&amp;gt;&amp;gt; a - 1e9&lt;/span&gt;
&lt;span class="code-line"&gt;0.95367431640625&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;⇒ the result is not 1... &lt;/p&gt;
&lt;p&gt;⇒ normalize input ! ⇒ &lt;strong&gt;0 mean, 1 variance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;this make optimizers easier to find optimum. 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image009.png"/&gt;&lt;/p&gt;
&lt;p&gt;normalization for images: 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image010.png"/&gt;&lt;/p&gt;
&lt;h2 id="weight-initialization"&gt;weight initialization&lt;/h2&gt;
&lt;p&gt;draw init w/b from a &lt;code&gt;Gaussian(0, sigma)&lt;/code&gt;, sigma → magtitude of initial output. 
small sigma means small outputs → uncertain about result. 
⇒ take small sigma for initialization 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image011.png"/&gt;&lt;/p&gt;
&lt;p&gt;recap: &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image012.png"/&gt;
⇒ feed this loss fcn to the optimizer &lt;/p&gt;
&lt;h2 id="training-validation-and-test-dataset"&gt;training, validation and test dataset&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;rule of thumb (30)&lt;/strong&gt;: 
a change that affects 30 examples in the validation set is statically significant. 
⇒ in most cases use &amp;gt;30000 samples in validation set → changes in 0.1% is significant. &lt;/p&gt;
&lt;h2 id="sgd"&gt;SGD&lt;/h2&gt;
&lt;p&gt;rule of thumb: computing &lt;code&gt;grad(L)&lt;/code&gt; takes 3x time than computing loss fcn &lt;code&gt;L&lt;/code&gt;. → pb for scaling.. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image014.png"/&gt;
SGD is the only fast enough model in practice. &lt;/p&gt;
&lt;p&gt;tricks to help SGD: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;normalize data (0 mean, uni-var)&lt;/li&gt;
&lt;li&gt;randomly initialize weights&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;momentum&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;learning rate decay&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="momentum"&gt;Momentum&lt;/h2&gt;
&lt;p&gt;SGD: many small steps in random directions → general direction is more accurate. 
⇒ keep a running average of the gradients&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image015.png"/&gt;&lt;/p&gt;
&lt;h2 id="learning-rate-decay"&gt;Learning rate decay&lt;/h2&gt;
&lt;p&gt;take smaller and smaller steps (alpha decays)
e.g. alpha decays exponentially...&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image016.png"/&gt;&lt;/p&gt;
&lt;h2 id="parameter-tuning"&gt;parameter tuning&lt;/h2&gt;
&lt;p&gt;how quickly you learning != how well you train.. 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image017.png"/&gt;
balck magics in deep learning: 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image018.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adagrad&lt;/strong&gt;
variant of SGD, implicitly decays momentum and learning rate. &lt;/p&gt;
&lt;p&gt;recap: 
&lt;img alt="" class="img-responsive" src="images/dlMOOC_L1/pasted_image019.png"/&gt;&lt;/p&gt;</summary><category term="deep learning"></category></entry></feed>